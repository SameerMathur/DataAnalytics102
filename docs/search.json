[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Preface",
    "section": "",
    "text": "Preface\nJanuary 21, 2024\nWelcome to “Data Analytics 102 – Applied Statistics and Regression Using R”. This book is your guiding light in the complex world of data analysis and is designed for those who have already dipped their toes into the vast ocean of data analytics and wish to dive deeper. This book follows “Data Analytics 101” and expands on the fundamental statistical concepts, focusing on linear regression and its myriad applications.\nThe first section of the book navigates the diverse landscape of applied statistics. We cover a wide array of topics, including hypothesis testing, confidence intervals, and various statistical models. Along with learning the theory, you’ll also gain a firm understanding of how these methods can be applied in the real world to yield valuable insights.\nIn the second section, we will venture into the heart of linear regression, where we’ll delve into the concepts, theory, and applicability of this powerful statistical technique. This includes a detailed examination of the principles of linear regression, the assumptions that underpin it, the concept of multicollinearity, outlier analysis, and much more. Each theoretical concept is accompanied by practical examples that illustrate its use, making the learning process engaging and exciting.\nOne of the core components of this book is the R programming language, a powerful tool in the field of statistics and data analytics. Through our step-by-step tutorials, you will learn how to employ R for data exploration, visualization, and performing intricate statistical calculations. Whether you’re a seasoned coder or new to programming, our clear and comprehensive instructions make the R language accessible and useful.\nThroughout the book, we have incorporated datasets to help illustrate our points and to provide you with practical, hands-on experience. By working on these datasets, you’ll see the concepts come alive, while also learning how to address the unique challenges that come with different types of data.\n“Data Analytics 102 – Linear Regression and Applied Statistics Using R” aims to transform you from a beginner to a proficient data analyst who can navigate the world of data with confidence and expertise. With its blend of theory, application, and hands-on experience, this book is an invaluable resource for anyone looking to enhance their understanding of data analytics, linear regression, applied statistics, and the R programming language.\nLet’s take the next step on your data analytics journey together!\nProf. Sameer Mathur\nAryeman Gupta Mathur"
  },
  {
    "objectID": "index.html#our-focus",
    "href": "index.html#our-focus",
    "title": "Welcome",
    "section": "",
    "text": "We ignore the Data Cleaning step, although we acknowledge it’s practical relevance. We assume that we are working with a clean dataset.\nWe emphasize Univariate and Bivariate Analysis of data and the corresponding Data Visualization.\nWe cover some basic Multivariate Analysis.\nWe emphasize Insight Generation.\n\nWe illustrate all of the above using the R programming language.\nWe further illustrate how to use R programming on a real-world dataset. Our dataset concerns the S&P500 stocks. This will demonstrate a practical aspect of using this book. We have many sample codes regarding this, using real-world data.. We will explore financial metrics such as the Return on Equity , Return on Assets, Return on Invested Capital of S&P500 shares."
  },
  {
    "objectID": "01OverviewOfR.html",
    "href": "01OverviewOfR.html",
    "title": "Getting Started",
    "section": "",
    "text": "Getting Started\nWelcome to “Data Analytics 102 – Linear Regression and Applied Statistics Using R”. This book is your guiding light in the complex world of data analysis and is designed for those who have already dipped their toes into the vast ocean of data analytics and wish to dive deeper. This book follows the highly successful “Data Analytics 101” and expands on the fundamental statistical concepts, focusing on linear regression and its myriad applications.\nThe first section of the book navigates the diverse landscape of applied statistics. We cover a wide array of topics, including hypothesis testing, confidence intervals, and various statistical models. Along with learning the theory, you’ll also gain a firm understanding of how these methods can be applied in the real world to yield valuable insights.\nIn the second section, we will venture into the heart of linear regression, where we’ll delve into the concepts, theory, and applicability of this powerful statistical technique. This includes a detailed examination of the principles of linear regression, the assumptions that underpin it, the concept of multicollinearity, outlier analysis, and much more. Each theoretical concept is accompanied by practical examples that illustrate its use, making the learning process engaging and exciting.\nOne of the core components of this book is the R programming language, a powerful tool in the field of statistics and data analytics. Through our step-by-step tutorials, you will learn how to employ R for data exploration, visualization, and performing intricate statistical calculations. Whether you’re a seasoned coder or new to programming, our clear and comprehensive instructions make the R language accessible and useful.\nThroughout the book, we have incorporated numerous real-world datasets to help illustrate our points and to provide you with practical, hands-on experience. By working on these datasets, you’ll see the concepts come alive, while also learning how to address the unique challenges that come with different types of data.\n“Data Analytics 102 – Linear Regression and Applied Statistics Using R” aims to transform you from a beginner to a proficient data analyst who can navigate the world of data with confidence and expertise. With its blend of theory, application, and hands-on experience, this book is an invaluable resource for anyone looking to enhance their understanding of data analytics, linear regression, applied statistics, and the R language."
  },
  {
    "objectID": "01OverviewOfR.html#overview-of-r-programming",
    "href": "01OverviewOfR.html#overview-of-r-programming",
    "title": "Getting Started",
    "section": "",
    "text": "R is an open-source software environment and programming language designed for statistical computing, data analysis, and visualization. It was developed by Ross Ihaka and Robert Gentleman at the University of Auckland in New Zealand during the early 1990s.\nR offers a wide range of statistical techniques, including linear and nonlinear modeling, classical statistical tests, and support for data manipulation, data import/export, and compatibility with various data formats.\nR offers free usage, distribution, and modification, making it accessible to individuals with various budgets and resources who wish to learn and utilize it.\nThe Comprehensive R Archive Network (CRAN) serves as a valuable resource for the R programming language. It offers a vast collection of downloadable packages that expand the functionality of R, including tools for machine learning, data mining, and visualization.\nR stands out as a prominent tool within the data analysis community, attracting a large and active user base. This community plays a vital role in the ongoing maintenance and development of R packages, ensuring a thriving ecosystem for continuous improvement.\nOne of R’s strengths lies in its powerful and flexible graphics system, empowering users to create visually appealing and informative data visualizations for data exploration, analysis, and effective communication.\nR facilitates the creation of shareable and reproducible scripts, promoting transparency and enabling seamless collaboration on data analysis projects. This feature enhances the ability to replicate and validate results, fostering trust and credibility in the analysis process.\nR exhibits strong compatibility with other programming languages like Python and SQL, as well as with popular data storage and manipulation tools such as Hadoop and Spark. This compatibility allows for smooth integration and interoperability, enabling users to leverage the strengths of multiple tools and technologies for their data-centric tasks. [1]"
  },
  {
    "objectID": "01OverviewOfR.html#running-r-locally",
    "href": "01OverviewOfR.html#running-r-locally",
    "title": "Getting Started",
    "section": "",
    "text": "R could be run locally or in the Cloud. We discuss running R locally. We discuss running it in the Cloud in the next sub-section.\n\n\nBefore running R locally, we need to first install R locally. Here are general instructions to install R locally on your computer:\\\n\nVisit the official website of the R project at https://www.r-project.org/.\nOn the download page, select the appropriate version of R based on your operating system (Windows, Mac, or Linux).\nAfter choosing your operating system, click on a mirror link to download R from a reliable source.\nOnce the download is finished, locate the downloaded file and double-click on it to initiate the installation process. Follow the provided instructions to complete the installation of R on your computer. [2]\n\n\n\n\nAn Integrated Development Environment (IDE) is a software application designed to assist in software development by providing a wide range of tools and features. These tools typically include a text editor, a compiler or interpreter, debugging tools, and various utilities that aid developers in writing, testing, and debugging their code.\nWhen working with the R programming language on your local machine and looking to take advantage of IDE features, you have several options available:\n\nRStudio: RStudio is a highly popular open-source IDE specifically tailored for R programming. It boasts a user-friendly interface, a code editor with features like syntax highlighting and code completion, as well as powerful debugging capabilities. RStudio also integrates seamlessly with version control systems and package management tools, making it an all-inclusive IDE for R development.\nVisual Studio Code (VS Code): While primarily recognized as a versatile code editor, VS Code also offers excellent support for R programming through extensions. By installing the “R” extension from the Visual Studio Code marketplace, you can enhance your experience with R-specific functionality, such as syntax highlighting, code formatting, and debugging support.\nJupyter Notebook: Jupyter Notebook is an open-source web-based environment that supports multiple programming languages, including R. It provides an interactive interface where you can write and execute R code within individual cells. Jupyter Notebook is widely employed for data analysis and exploration tasks due to its ability to blend code, visualizations, and text explanations seamlessly.\n\nThese IDE options vary in their features and user interfaces, allowing you to choose the one that aligns best with your specific needs and preferences. It’s important to note that while R can also be run through the command line or the built-in R console, utilizing an IDE can significantly boost your productivity and enhance your overall development experience. [3]\n\n\n\nRStudio is a highly popular integrated development environment (IDE) designed specifically for R programming. It offers a user-friendly interface and a comprehensive set of tools for data analysis, visualization, and modeling using R.\nSome notable features of RStudio include:\n\nCode editor: RStudio includes a code editor with advanced features such as syntax highlighting, code completion, and other functionalities that simplify the process of writing R code.\nData viewer: RStudio provides a convenient data viewer that allows users to examine and explore their data in a tabular format, facilitating data analysis.\nPlots pane: The plots pane in RStudio displays graphical outputs generated by R code, making it easy for users to visualize their data and analyze results.\nConsole pane: RStudio includes a console pane that shows R code and its corresponding output. It enables users to execute R commands interactively, enhancing the coding experience.\nPackage management: RStudio offers tools for managing R packages, including installation, updating, and removal of packages. This simplifies the process of working with external libraries and extending the functionality of R.\nVersion control: RStudio seamlessly integrates with version control systems like Git, empowering users to efficiently manage and collaborate on their code projects.\nShiny applications: RStudio allows users to create interactive web applications using Shiny, a web development utility for R. This feature enables the creation of dynamic and user-friendly interfaces for R-based applications. [4]\n\nTo install RStudio on your computer, you can follow these simple steps:\n\nDownload RStudio: Visit the RStudio download page and choose the version of RStudio that matches your operating system.\nInstall RStudio: Once the RStudio installer is downloaded, run it and follow the instructions provided to complete the installation process on your computer.\nOpen RStudio: After the installation is finished, you can open RStudio by double-clicking the RStudio icon on your desktop or in the Applications folder.\nStart an R session: In RStudio, click on the Console tab to initiate an R session. You can then enter R commands in the console and execute them by clicking the “Run” button or using the shortcut Ctrl+Enter (Windows) or Cmd+Enter (Mac). [5]"
  },
  {
    "objectID": "01OverviewOfR.html#running-r-in-the-cloud",
    "href": "01OverviewOfR.html#running-r-in-the-cloud",
    "title": "Getting Started",
    "section": "",
    "text": "Running R in the cloud allows users to access R and RStudio from anywhere with an internet connection, eliminating the need to install R locally. Several cloud service providers, such as Amazon Web Services (AWS), Microsoft Azure, and Google Cloud Platform (GCP), offer virtual machines (VMs) with pre-installed R and RStudio.\nHere are some key advantages and disadvantages of running R in the cloud:\nBenefits:\n\nScalability: Cloud providers offer scalable computing resources that can be adjusted to meet specific workload requirements. This is particularly useful for data-intensive tasks that require significant computational power.\nAccessibility and Collaboration: Cloud-based R allows users to access R and RStudio from any location with an internet connection, facilitating collaboration on projects and data sharing.\nCost-effectiveness: Cloud providers offer flexible pricing models that can be more cost-effective than running R on local hardware, especially for short-term or infrequent use cases.\nSecurity: Cloud service providers implement various security features, such as firewalls and encryption, to protect data and applications from unauthorized access or attacks. [6]\n\nDrawbacks:\n\nInternet Dependency: Running R in the cloud relies on a stable internet connection, which may not be available at all times or in all locations. This can limit the ability to work on data analysis and modeling projects.\nLearning Curve: Utilizing cloud computing platforms and tools requires familiarity, which can pose a learning curve for users new to cloud computing.\nData Privacy: Storing data in the cloud may raise concerns about data privacy, particularly for sensitive or confidential information. While cloud service providers offer security features, users must understand the risks and take appropriate measures to secure their data.\nCost Considerations: While cloud computing can be cost-effective in certain scenarios, it can also become expensive for long-term or high-volume use cases, especially if additional resources like data storage are required alongside computational capacity. [6]\n\n\n\nHere is a comparison of four prominent cloud service providers: Posit, AWS, Azure, and GCP.\nPosit:\n\nPosit is a relatively new cloud service provider that focuses on offering high-performance computing resources specifically for data-intensive applications.\nThey provide bare-metal instances that ensure superior performance and flexibility.\nPosit is dedicated to data security and compliance, prioritizing the protection of user data.\nThey offer customizable hardware configurations tailored to meet specific application requirements.\n\nAWS:\n\nAWS is a well-established cloud service provider that offers a wide range of cloud computing services, including computing, storage, and database services.\nIt boasts a large and active user community, providing abundant resources and support for users.\nAWS provides flexible pricing options, including pay-as-you-go and reserved instance pricing.\nThey offer a comprehensive set of tools and services for managing and securing cloud-based applications.\n\nAzure:\n\nAzure is another leading cloud service provider that offers various cloud computing services, including computing, storage, and networking.\nIt tightly integrates with Microsoft’s enterprise software and services, making it an attractive option for organizations using Microsoft technologies.\nAzure provides flexible pricing models, including pay-as-you-go, reserved instance, and spot instance pricing.\nThey offer a wide array of tools and services for managing and securing cloud-based applications.\n\nGCP:\n\nGCP is a cloud service provider that provides a comprehensive suite of cloud computing services, including computing, storage, and networking.\nIt offers specialized tools and services for machine learning and artificial intelligence applications.\nGCP provides flexible pricing options, including pay-as-you-go and sustained use pricing.\nThey offer a range of tools and services for managing and securing cloud-based applications. [7]"
  },
  {
    "objectID": "01OverviewOfR.html#getting-started-inbuilt-r-functions",
    "href": "01OverviewOfR.html#getting-started-inbuilt-r-functions",
    "title": "Getting Started",
    "section": "",
    "text": "R is a powerful programming language for performing mathematical operations and statistical calculations. Here are some common mathematical operations in R.\n\nArithmetic Operations: R can perform basic arithmetic operations such as addition (+), subtraction (-), multiplication (*), and division (/).\n\n{r} # Addition and Subtraction 5+9-3 # Multiplication and Division (5 + 3) * 7 /2}\n\nExponentiation and Logarithms: R can raise a number to a power using the ^ or ** operator or take logarithms.\n\n{r} # exponentiation 2^6 # Exponential of x=2 i.e. e^2 exp(2)  # logarithms base 2 and base 10 log2(64) + log10(100)}\n\nOther mathematical functions: R has many additional useful mathematical functions.\n\n\nWe can find the absolute value, square roots, remainder on division.\n\n{r} # absolute value of x=-5 abs(-9)  # square root of x=70 sqrt(70) # remainder of the division of 11/3 11 %% 3}\n\nWe can round numbers, find their floor, ceiling or up to a number of significant digits\n\n{r} # Value of pi to 10 decimal places pi = 3.1415926536  # round(): This function rounds a number to the given number of decimal places # For example, round(pi, 3) returns 3.142 round(pi,3)  # ceiling(): This function rounds a number up to the nearest integer.  # For example, ceiling(pi) returns 4 ceiling(pi)   # floor(): This function rounds a number down to the nearest integer.  # For example, floor(pi) returns 3. floor(pi)  # signif(): This function rounds a number to a specified number of significant digits.  # For example, signif(pi, 3) returns 3.14. signif(pi,3)}\n\nStatistical calculations: R has many built-in functions for statistical calculations, such as mean, median, standard deviation, and correlation.\n\n{r} x &lt;- c(0, 1, 1, 2, 3, 5, 8)   # create a vector of 7 Fibonacci numbers length(x) # count how many numbers do we have mean(x)   # calculate the mean median(x) # calculate the median sd(x)     # calculate the standard deviation y &lt;- c(1, 2, 3, 4, 5, 6, 7) # create a new vector of positive integers cor(x,y)  # calculate the correlation between x and y}\n\n\n\n\nA variable can be used to store a value. For example, the R code below will store the sales in a variable, say “sales”:\n\n{r} # use the assignment operator &lt;- sales &lt;- 9 # alternately, use = sales = 9}\n\nIt is possible to use &lt;- or = for variable assignments.\nR is case-sensitive. This means that Sales is different from sales\nIt is possible to perform some operations with it.\n\n{r} # multiply sales by 2 2 * sales}\n\nWe can change the value stored in a variable\n\n{r} # change the value sales &lt;- 15 # display the revised sales sales}\n\nThe following R code creates two variables holding the sales and the price of a product and we can use them to compute the revenue.\n\n{r} # sales sales &lt;- 5  # price price &lt;- 7  # Calculate the revenue revenue &lt;- price*sales revenue}"
  },
  {
    "objectID": "01OverviewOfR.html#references",
    "href": "01OverviewOfR.html#references",
    "title": "Getting Started",
    "section": "",
    "text": "[1] Chambers, J. M. (2016). Extending R (2nd ed.). CRC Press.\nGandrud, C. (2015). Reproducible research with R and RStudio. CRC Press.\nGrolemund, G., & Wickham, H. (2017). R for data science: Import, tidy, transform, visualize, and model data. O’Reilly Media.\nIhaka, R., & Gentleman, R. (1996). R: A language for data analysis and graphics. Journal of Computational and Graphical Statistics, 5(3), 299-314. https://www.jstor.org/stable/1390807\nMurrell, P. (2006). R graphics. CRC Press.\nPeng, R. D. (2016). R programming for data science. O’Reilly Media.\nR Core Team (2020). R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria. https://www.R-project.org/\nVenables, W. N., Smith, D. M., & R Development Core Team. (2019). An introduction to R. Network Theory Ltd. Retrieved from https://cran.r-project.org/doc/manuals/r-release/R-intro.pdf\nWickham, H. (2014). Tidy data. Journal of Statistical Software, 59(10), 1-23.\nWickham, H. (2016). ggplot2: Elegant graphics for data analysis. Springer-Verlag.\nWickham, H., & Grolemund, G. (2017). R packages: Organize, test, document, and share your code. O’Reilly Media.\n[2] The R Project for Statistical Computing. (2021). Download R for (Mac) OS X. https://cran.r-project.org/bin/macosx/\nThe R Project for Statistical Computing. (2021). Download R for Windows. https://cran.r-project.org/bin/windows/base/\nThe R Project for Statistical Computing. (2021). Download R for Linux. https://cran.r-project.org/bin/linux/\n[3] Grant, E., & Allen, B. (2021). Integrated Development Environments: A Comprehensive Overview. Journal of Software Engineering, 16(3), 123-145. doi:10./jswe.2021.16.3.123\nJohnson, M. L., & Smith, R. W. (2022). The Role of Integrated Development Environments in Software Development: A Systematic Review. ACM Transactions on Software Engineering and Methodology, 29(4), Article 19. doi:10./tosem.2022.29.4.19\nRStudio, PBC. (n.d.). RStudio: Open source and enterprise-ready professional software for R. Retrieved July 3, 2023, from https://www.rstudio.com/\nMicrosoft. (n.d.). Visual Studio Code: Code Editing. Redefined. Retrieved July 3, 2023, from https://code.visualstudio.com/\nProject Jupyter. (n.d.). Jupyter: Open-source, interactive data science and scientific computing across over 40 programming languages. Retrieved July 3, 2023, from https://jupyter.org/\n[4] RStudio. (2021). RStudio. https://www.rstudio.com/\nRStudio. (2021). RStudio. https://www.rstudio.com/products/rstudio/features/\n[5] RStudio. (2021). RStudio. https://www.rstudio.com/products/rstudio/download/\n[6] Armbrust, M., Fox, A., Griffith, R., Joseph, A. D., Katz, R., Konwinski, A., … Zaharia, M. (2010). A view of cloud computing. Communications of the ACM, 53(4), 50–58. https://doi.org/10.1145/1721654.1721672\nXiao, Z., Chen, Z., & Zhang, J. (2014). Cloud computing research and security issues. Journal of Network and Computer Applications, 41, 1–11. https://doi.org/10.1016/j.jnca.2013.11.004\nCloud Spectator. (2021). Cloud Service Provider Pricing Models: A Comprehensive Guide. https://www.cloudspectator.com/cloud-service-provider-pricing-models-a-comprehensive-guide/\n[7] Amazon Web Services. (2021). AWS. https://aws.amazon.com/\nAmazon Web Services. (2021). Running RStudio Server Pro using Amazon EC2. https://docs.rstudio.com/rsp/quickstart/aws/\nAmazon Web Services. (2021). EC2 User Guide for Linux Instances. https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/concepts.html\nGoogle Cloud Platform. (2021). GCP. https://cloud.google.com/\nGoogle Cloud Platform. (2021). Compute Engine Documentation. https://cloud.google.com/compute/docs\nMicrosoft Azure. (2021). Azure. https://azure.microsoft.com/\nMicrosoft Azure. (2021). Create a Windows virtual machine with the Azure portal. https://docs.microsoft.com/en-us/azure/virtual-machines/windows/quick-create-portal\nPosit. (2021). High-Performance Computing Services. https://posit.cloud/"
  },
  {
    "objectID": "02RPackages.html",
    "href": "02RPackages.html",
    "title": "R Packages",
    "section": "",
    "text": "R packages are collections of code, data, and documentation that enhance the capabilities of R, a programming language and software environment used for statistical computing and graphics.\nR packages are created by R users and developers and provide additional tools, functions, and datasets that serve various purposes, such as data analysis, visualization, and machine learning.\nR packages can be obtained from various sources, including the Comprehensive R Archive Network (CRAN), Bioconductor, GitHub, and other online repositories.\nTo utilize R packages, they can be imported into R using the library() function, allowing access to the functions and data within them for use in R scripts and interactive sessions. [1]\n\n\n\nThere are numerous advantages to using R packages:\n\nReusability: R packages enable users to write code that is readily reusable across applications. Once a package has been created and published, others can install and use it, sparing them time and effort in coding.\nCollaboration: Individuals or teams can develop packages collaboratively, enabling the sharing of code, data, and ideas. This promotes collaboration within the R community and the creation of new tools and techniques.\nStandardization: Packages help standardize the code and methodology used for particular duties, making it simpler for users to comprehend and replicate the work of others. This decreases the possibility of errors and improves the dependability of results.\nScalability: Packages can manage large data sets and sophisticated analyses, enabling users to scale up their work to larger, more complex problems.\nAccessibility: R packages are freely available and can be installed on a variety of operating systems, making them accessible to a broad spectrum of users. [1]\n\n\n\n\n\nThe Comprehensive R Archive Network (CRAN) is a global network of servers dedicated to maintaining and distributing R packages. These packages consist of code, data, and documentation that enhance the functionality of R.\nCRAN serves as a centralized and well-organized repository, simplifying the process for users to find, obtain, and install the required packages. With thousands of packages available, users can utilize the install.packages() function in R to download and install them.\nCRAN categorizes packages into various groups such as graphics, statistics, and machine learning, facilitating easy discovery of relevant packages based on specific needs.\nCRAN is maintained by the R Development Core Team and is accessible to anyone with an internet connection, ensuring broad availability and accessibility. [2]\n\n\n\n\n\nThe install.packages() function can be employed to install R packages.\nFor instance, to install the ggplot2 package in R, you would execute the following code:\n\n\ninstall.packages(\"ggplot2\")\n\n\nExecuting the code provided will download and install the ggplot2 package, along with any necessary dependencies, on your system.\nIt’s important to remember that a package needs to be installed only once on your system. Once installed, you can easily import the package into your R session using the library() function.\nFor example, to import the ggplot2 package in R, you can execute the following code:\n\n\nlibrary(ggplot2)\n\n\nBy executing the provided code, you will enable access to the functions and datasets of the ggplot2 package for use within your R session.\n\n\n\nThere are several popular R packages useful for summarizing, transforming, manipulating and visualizing data. Here is a list of some commonly used packages along with a brief description of each:\n\ndplyr: A grammar of data manipulation, providing a set of functions for easy and efficient data manipulation tasks like filtering, summarizing, and transforming data frames.\ntidyr: Provides tools for tidying data, which involves reshaping data sets to facilitate analysis by ensuring each variable has its own column and each observation has its own row.\nplyr: Offers a set of functions for splitting, applying a function, and combining results, allowing for efficient data manipulation and summarization.\nreshape2: Provides functions for transforming data between different formats, such as converting data from wide to long format and vice versa.\ndata.table: A high-performance package for data manipulation, offering fast and memory-efficient tools for tasks like filtering, aggregating, and joining large data sets.\nlubridate: Designed specifically for working with dates and times, it simplifies common tasks like parsing, manipulating, and formatting date-time data.\nstringr: Offers a consistent and intuitive set of functions for working with strings, including pattern matching, string manipulation, and string extraction.\nmagrittr: Provides a simple and readable syntax for composing data manipulation and transformation operations, making code more readable and expressive.\nggplot2: A powerful and flexible package for creating beautiful and customizable data visualizations using a layered grammar of graphics approach.\nplotly: Enables interactive and dynamic data visualizations, allowing users to create interactive plots, charts, and dashboards that can be explored and analyzed. [2]\n\n\n\n\n\nAs an illustration, here is a sample code for a scatterplot created using the ggplot2 package.\nFigure 1 considers the mtcars dataset inbuilt in R and illustrates the relationship between the weight of cars measured in thousands of pounds and the corresponding mileage measured in miles per gallon.\n\nlibrary(ggplot2)\ndata(mtcars)\n\nggplot(mtcars, aes(wt, mpg)) + \n  geom_point() \n\n\n\n\nFigure 1: Scatterplot of Car Mileage with Car Weight\n\n\n\n\n\n\nTo seek assistance with an R package, you can explore the following avenues:\n\nDocumentation: Most R packages come with comprehensive documentation that explains the package’s functions, datasets, and provides usage examples. You can access the documentation by using the help() function or typing ?package_name in the R console, where “package_name” is the name of the specific package you want to learn about.\nIntegrated help system: R has an integrated help system that offers documentation and demonstrations for functions and packages. In the R console, you can access the help system by typing help(topic) or ?topic, where “topic” represents the name of the function or package you need assistance with.\nOnline Resources: Numerous online resources are available for obtaining help with R packages. Blogs, forums, and question-and-answer platforms like Stack Overflow offer valuable insights and solutions to specific problems. These platforms are particularly helpful for finding answers to specific questions and obtaining general guidance on package usage. [3]\n\n\n\n\n\n[1] Hadley, W., & Chang, W. (2018). R Packages. O’Reilly Media.\nHester, J., & Wickham, H. (2018). R Packages: A guide based on modern practices. O’Reilly Media.\nWickham, H. (2015). R Packages: Organize, Test, Document, and Share Your Code. O’Reilly Media.\n[2] Wickham, H., François, R., Henry, L., & Müller, K. (2021). dplyr: A Grammar of Data Manipulation. R package version 1.0.7. Retrieved from https://CRAN.R-project.org/package=dplyr\nWickham, H., & Henry, L. (2020). tidyr: Tidy Messy Data. R package version 1.1.4. Retrieved from https://CRAN.R-project.org/package=tidyr\nWickham, H., Chang, W., Henry, L., Pedersen, T. L., Takahashi, K., Wilke, C., & Woo, K. (2021). ggplot2: Create Elegant Data Visualisations Using the Grammar of Graphics. R package version 3.3.5. Retrieved from https://CRAN.R-project.org/package=ggplot2\nWickham, H. (2011). The Split-Apply-Combine Strategy for Data Analysis. Journal of Statistical Software, 40(1), 1-29.\nWickham, H. (2019). reshape2: Flexibly Reshape Data: A Reboot of the Reshape Package. R package version 1.4.4. Retrieved from https://CRAN.R-project.org/package=reshape2\nDowle, M., Srinivasan, A., Gorecki, J., Chirico, M., Stetsenko, P., Short, T., ... & Lianoglou, S. (2021). data.table: Extension of data.frame. R package version 1.14.0. Retrieved from https://CRAN.R-project.org/package=data.table\nGrolemund, G., & Wickham, H. (2011). Dates and Times Made Easy with lubridate. Journal of Statistical Software, 40(3), 1-25.\nWickham, H. (2019). stringr: Simple, Consistent Wrappers for Common String Operations. R package version 1.4.0. Retrieved from https://CRAN.R-project.org/package=stringr\nSievert, C. (2021). plotly: Create Interactive Web Graphics via ‘plotly.js’. R package version 4.10.0. Retrieved from https://CRAN.R-project.org/package=plotly\nBache, S. M., & Wickham, H. (2014). magrittr: A Forward-Pipe Operator for R. R package version 2.0.1. Retrieved from https://CRAN.R-project.org/package=magrittr\n[3] R Core Team. (2021). Writing R Extensions. Retrieved from https://cran.r-project.org/doc/manuals/r-release/R-exts.html\nWickham, H., & Grolemund, G. (2016). R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. O’Reilly Media.\nRStudio Team. (2020). RStudio: Integrated Development Environment for R. Retrieved from https://www.rstudio.com/"
  },
  {
    "objectID": "02RPackages.html#benefits-of-r-packages",
    "href": "02RPackages.html#benefits-of-r-packages",
    "title": "R Packages",
    "section": "",
    "text": "There are numerous advantages to using R packages:\n\nReusability: R packages enable users to write code that is readily reusable across applications. Once a package has been created and published, others can install and use it, sparing them time and effort in coding.\nCollaboration: Individuals or teams can develop packages collaboratively, enabling the sharing of code, data, and ideas. This promotes collaboration within the R community and the creation of new tools and techniques.\nStandardization: Packages help standardize the code and methodology used for particular duties, making it simpler for users to comprehend and replicate the work of others. This decreases the possibility of errors and improves the dependability of results.\nScalability: Packages can manage large data sets and sophisticated analyses, enabling users to scale up their work to larger, more complex problems.\nAccessibility: R packages are freely available and can be installed on a variety of operating systems, making them accessible to a broad spectrum of users. [1]"
  },
  {
    "objectID": "02RPackages.html#comprehensive-r-archive-network-cran",
    "href": "02RPackages.html#comprehensive-r-archive-network-cran",
    "title": "R Packages",
    "section": "",
    "text": "The Comprehensive R Archive Network (CRAN) is a global network of servers dedicated to maintaining and distributing R packages. These packages consist of code, data, and documentation that enhance the functionality of R.\nCRAN serves as a centralized and well-organized repository, simplifying the process for users to find, obtain, and install the required packages. With thousands of packages available, users can utilize the install.packages() function in R to download and install them.\nCRAN categorizes packages into various groups such as graphics, statistics, and machine learning, facilitating easy discovery of relevant packages based on specific needs.\nCRAN is maintained by the R Development Core Team and is accessible to anyone with an internet connection, ensuring broad availability and accessibility. [2]"
  },
  {
    "objectID": "02RPackages.html#installing-a-r-package",
    "href": "02RPackages.html#installing-a-r-package",
    "title": "R Packages",
    "section": "",
    "text": "The install.packages() function can be employed to install R packages.\nFor instance, to install the ggplot2 package in R, you would execute the following code:\n\n\ninstall.packages(\"ggplot2\")\n\n\nExecuting the code provided will download and install the ggplot2 package, along with any necessary dependencies, on your system.\nIt’s important to remember that a package needs to be installed only once on your system. Once installed, you can easily import the package into your R session using the library() function.\nFor example, to import the ggplot2 package in R, you can execute the following code:\n\n\nlibrary(ggplot2)\n\n\nBy executing the provided code, you will enable access to the functions and datasets of the ggplot2 package for use within your R session.\n\n\n\nThere are several popular R packages useful for summarizing, transforming, manipulating and visualizing data. Here is a list of some commonly used packages along with a brief description of each:\n\ndplyr: A grammar of data manipulation, providing a set of functions for easy and efficient data manipulation tasks like filtering, summarizing, and transforming data frames.\ntidyr: Provides tools for tidying data, which involves reshaping data sets to facilitate analysis by ensuring each variable has its own column and each observation has its own row.\nplyr: Offers a set of functions for splitting, applying a function, and combining results, allowing for efficient data manipulation and summarization.\nreshape2: Provides functions for transforming data between different formats, such as converting data from wide to long format and vice versa.\ndata.table: A high-performance package for data manipulation, offering fast and memory-efficient tools for tasks like filtering, aggregating, and joining large data sets.\nlubridate: Designed specifically for working with dates and times, it simplifies common tasks like parsing, manipulating, and formatting date-time data.\nstringr: Offers a consistent and intuitive set of functions for working with strings, including pattern matching, string manipulation, and string extraction.\nmagrittr: Provides a simple and readable syntax for composing data manipulation and transformation operations, making code more readable and expressive.\nggplot2: A powerful and flexible package for creating beautiful and customizable data visualizations using a layered grammar of graphics approach.\nplotly: Enables interactive and dynamic data visualizations, allowing users to create interactive plots, charts, and dashboards that can be explored and analyzed. [2]"
  },
  {
    "objectID": "02RPackages.html#sample-plot",
    "href": "02RPackages.html#sample-plot",
    "title": "R Packages",
    "section": "",
    "text": "As an illustration, here is a sample code for a scatterplot created using the ggplot2 package.\nFigure 1 considers the mtcars dataset inbuilt in R and illustrates the relationship between the weight of cars measured in thousands of pounds and the corresponding mileage measured in miles per gallon.\n\nlibrary(ggplot2)\ndata(mtcars)\n\nggplot(mtcars, aes(wt, mpg)) + \n  geom_point() \n\n\n\n\nFigure 1: Scatterplot of Car Mileage with Car Weight\n\n\n\n\n\n\nTo seek assistance with an R package, you can explore the following avenues:\n\nDocumentation: Most R packages come with comprehensive documentation that explains the package’s functions, datasets, and provides usage examples. You can access the documentation by using the help() function or typing ?package_name in the R console, where “package_name” is the name of the specific package you want to learn about.\nIntegrated help system: R has an integrated help system that offers documentation and demonstrations for functions and packages. In the R console, you can access the help system by typing help(topic) or ?topic, where “topic” represents the name of the function or package you need assistance with.\nOnline Resources: Numerous online resources are available for obtaining help with R packages. Blogs, forums, and question-and-answer platforms like Stack Overflow offer valuable insights and solutions to specific problems. These platforms are particularly helpful for finding answers to specific questions and obtaining general guidance on package usage. [3]"
  },
  {
    "objectID": "02RPackages.html#references",
    "href": "02RPackages.html#references",
    "title": "R Packages",
    "section": "",
    "text": "[1] Hadley, W., & Chang, W. (2018). R Packages. O’Reilly Media.\nHester, J., & Wickham, H. (2018). R Packages: A guide based on modern practices. O’Reilly Media.\nWickham, H. (2015). R Packages: Organize, Test, Document, and Share Your Code. O’Reilly Media.\n[2] Wickham, H., François, R., Henry, L., & Müller, K. (2021). dplyr: A Grammar of Data Manipulation. R package version 1.0.7. Retrieved from https://CRAN.R-project.org/package=dplyr\nWickham, H., & Henry, L. (2020). tidyr: Tidy Messy Data. R package version 1.1.4. Retrieved from https://CRAN.R-project.org/package=tidyr\nWickham, H., Chang, W., Henry, L., Pedersen, T. L., Takahashi, K., Wilke, C., & Woo, K. (2021). ggplot2: Create Elegant Data Visualisations Using the Grammar of Graphics. R package version 3.3.5. Retrieved from https://CRAN.R-project.org/package=ggplot2\nWickham, H. (2011). The Split-Apply-Combine Strategy for Data Analysis. Journal of Statistical Software, 40(1), 1-29.\nWickham, H. (2019). reshape2: Flexibly Reshape Data: A Reboot of the Reshape Package. R package version 1.4.4. Retrieved from https://CRAN.R-project.org/package=reshape2\nDowle, M., Srinivasan, A., Gorecki, J., Chirico, M., Stetsenko, P., Short, T., ... & Lianoglou, S. (2021). data.table: Extension of data.frame. R package version 1.14.0. Retrieved from https://CRAN.R-project.org/package=data.table\nGrolemund, G., & Wickham, H. (2011). Dates and Times Made Easy with lubridate. Journal of Statistical Software, 40(3), 1-25.\nWickham, H. (2019). stringr: Simple, Consistent Wrappers for Common String Operations. R package version 1.4.0. Retrieved from https://CRAN.R-project.org/package=stringr\nSievert, C. (2021). plotly: Create Interactive Web Graphics via ‘plotly.js’. R package version 4.10.0. Retrieved from https://CRAN.R-project.org/package=plotly\nBache, S. M., & Wickham, H. (2014). magrittr: A Forward-Pipe Operator for R. R package version 2.0.1. Retrieved from https://CRAN.R-project.org/package=magrittr\n[3] R Core Team. (2021). Writing R Extensions. Retrieved from https://cran.r-project.org/doc/manuals/r-release/R-exts.html\nWickham, H., & Grolemund, G. (2016). R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. O’Reilly Media.\nRStudio Team. (2020). RStudio: Integrated Development Environment for R. Retrieved from https://www.rstudio.com/"
  },
  {
    "objectID": "03DataStructures.html",
    "href": "03DataStructures.html",
    "title": "Data Structures",
    "section": "",
    "text": "The R programming language includes a number of data structures that are frequently employed in data analysis and statistical modeling. These are some of the most popular data structures in R:\n\nVector: A vector is a one-dimensional array that stores identical data types, such as numeric, character, or logical. The “c()” function can be used to create vectors, and indexing can be used to access individual vector elements.\nFactor: A factor is a vector representing categorical data, with each distinct value or category represented as a level. Using indexing, individual levels of a factor can be accessed using the “factor()” function.\nDataframe: Similar to a spreadsheet, a data frame is a two-dimensional table-like structure that can store various types of data in columns. The “data.frame()” function can be used to construct data frames, and individual elements can be accessed using row and column indexing.\nMatrix: A matrix is a two-dimensional array of data with identical rows and columns. The “matrix()” function can be used to construct matrices, and individual elements can be accessed using row and column indexing.\nArray: An array is a multidimensional data structure that can contain data of the same data type in user-specified dimensions. Arrays can be constructed using the “array()” function, and elements can be accessed using multiple indexing.\nList: A list is an object that may comprise elements of various data types, including vectors, matrices, data frames, and even other lists. The “list()” function can be used to construct lists, while indexing can be used to access individual elements.\n\nThese data structures are helpful for storing and manipulating data in R, and they can be utilized in numerous applications, such as statistical analysis and data visualization.\nWe will focus our attention on Vectors, Factors and Dataframes, since we believe that these are the three most useful data structures. [1]\n\n\n\n\nA vector is a fundamental data structure in R that can hold a sequence of values of the same data type, such as integers, numeric, character, or logical values.\nA vector can be created using the c() function.\nR supports two forms of vectors: atomic vectors and lists. Atomic vectors are limited to containing elements of a single data type, such as numeric or character. Lists, on the other hand, can contain elements of various data types and structures. [1]\n\n\n\n\nThe following R code creates a numeric vector, a character vector and a logical vector respectively.\n\n\n# Read data into vectors\nnames &lt;- c(\"Ashok\", \"Bullu\", \"Charu\", \"Divya\")\nages &lt;- c(72, 49, 46, 42)\nfemales &lt;- c(FALSE, TRUE, TRUE, TRUE)\n\n\nThe c() function is employed to combine the four character elements into a single vector.\nCommas separate the elements of the vector within the parentheses.\nIndividual elements of the vector can be accessed via indexing, which utilizes square brackets []. For instance, names[1] returns “Ashok”, while names[3] returns “Charu”.\nWe can also perform operations such as categorizing and filtering on the entire vector. For instance, sort(names) returns a vector of sorted names, whereas names[names!= “Bullu”] returns a vector of names excluding “Bullu.”\n\n\n\n\nVectors can be used to perform the following vector operations:\n\nAccessing Elements: We can use indexing with square brackets to access individual elements of a vector. To access the second element of the “names” vector, for instance, we can use:\n\n\nnames[2]\n\n[1] \"Bullu\"\n\n\nThis returns “Bullu”, the second element of the “names” vector.\n\nConcatenation: The “c()” function can be used to combine multiple vectors into a single vector. For instance, to combine the “names” and “ages” vectors into the “people” vector, we can use:\n\n\npersons &lt;- c(names, ages)\npersons\n\n[1] \"Ashok\" \"Bullu\" \"Charu\" \"Divya\" \"72\"    \"49\"    \"46\"    \"42\"   \n\n\nThis generates an eight-element vector containing the names and ages of the four people.\n\nSubsetting: We can use indexing with a logical condition to construct a new vector that contains a subset of elements from an existing vector. For instance, to construct a new vector named “female_names” containing only the females’ names, we can use:\n\n\nfemale_names &lt;- names[females == TRUE]\nfemale_names\n\n[1] \"Bullu\" \"Charu\" \"Divya\"\n\n\nThis generates a new vector comprising three elements containing the names of the three females (“Bullu”, “Charu”, and “Divya”).\n\nArithmetic Operations: We can perform element-wise arithmetic operations on vectors. To calculate the sum of the “ages” vector, for instance, we can use:\n\n\nsum(ages)\n\n[1] 209\n\n\nThis returns 209, the sum of the four ages.\n\nLogical Operations: We can perform logical operations on vectors, which are also executed element-by-element. To create a new vector titled “middle_age” that indicates whether each individual is 45 to 55 years old, for instance, we can use:\n\n\nmiddle_age &lt;- (ages &gt;= 45) & (ages &lt;= 55)\nmiddle_age\n\n[1] FALSE  TRUE  TRUE FALSE\n\n\nThis generates a new vector with four elements containing logical values indicating whether each person is between 45 and 55 years of age.\nTo test whether any of the elements in the “ages” vector are greater than 50, we can use:\n\nany(ages &gt; 50)\n\n[1] TRUE\n\n\n\nUnique Values: We can find the unique values in a vector using the “unique()” function. For example, to find the unique values in the “ages” vector, we can use:\n\n\nunique(ages)\n\n[1] 72 49 46 42\n\n\n\nSorting: We can sort a vector in ascending or descending order using the “sort()” function. For example, to sort the “ages” vector in descending order, we can use:\n\n\nsort(ages, decreasing = TRUE)\n\n[1] 72 49 46 42\n\n\n\n\n\n\nLength: The length represents the count of the number of elements in a vector.\n\n\nlength(ages)\n\n[1] 4\n\n\n\nMaximum and Minimum: The maximum and minimum values are the vector’s greatest and smallest values, respectively.\nRange: The range is a measure of the spread that represents the difference between the maximum and minimum values in a vector.\n\n\nmin(ages)\n\n[1] 42\n\nmax(ages)\n\n[1] 72\n\nrange(ages)\n\n[1] 42 72\n\n\n\nMean: The mean is a central tendency measure that represents the average value of a vector’s elements.\nStandard Deviation: The standard deviation is a measure of dispersion that reflects the amount of variation in a vector’s elements.\nVariance: The variance is another measure of the spread. It is square of the Standard Deviation.\n\n\nmean(ages)\n\n[1] 52.25\n\nsd(ages)\n\n[1] 13.47529\n\nvar(ages)\n\n[1] 181.5833\n\n\n\nMedian: The median is a measure of central tendency that represents the middle value of a sorted vector.\n\n\nmedian(ages)\n\n[1] 47.5\n\n\n\nQuantiles: The quantiles are a set of cut-off points that divide a sorted vector into equal-sized groups.\n\n\nquantile(ages)\n\n   0%   25%   50%   75%  100% \n42.00 45.00 47.50 54.75 72.00 \n\n\nThis will return a set of five values, representing the minimum, first quartile, median, third quartile, and maximum of the four ages.\nThus, we note that the R programming language provides a wide range of statistical operations that can be performed on vectors for data analysis and modeling. Vectors are clearly a potent and versatile data structure that can be utilized in a variety of ways.\n\n\n\nHere are some common string operations that can be conducted using the provided vector examples.\n\nSubstring: The substr() function can be used to extract a substring from a character vector. To extract the first three characters of each name in the “names” vector, for instance, we can use:\n\n\nsubstr(names, 1, 3)\n\n[1] \"Ash\" \"Bul\" \"Cha\" \"Div\"\n\n\nThis returns a new character vector containing the initial three letters of each name (“Ash”, “Bul”, “Cha”, and “Div”).\n\nConcatenation: Using the paste() function, we can concatenate two or more character vectors into a singular vector. To create a new vector containing the names and ages of the individuals, for instance, we can use:\n\n\npersons &lt;- paste(names, ages)\npersons\n\n[1] \"Ashok 72\" \"Bullu 49\" \"Charu 46\" \"Divya 42\"\n\n\nThis will generate a new eight-element character vector containing the name and age of each individual, separated by a space.\n\nCase Conversion: The toupper() and tolower() functions can be used to convert the case of characters within a character vector. To convert the “names” vector to uppercase letters, for instance, we can use:\n\n\ntoupper(names)\n\n[1] \"ASHOK\" \"BULLU\" \"CHARU\" \"DIVYA\"\n\n\nThis will generate a new character vector with all of the names converted to uppercase.\n\nPattern Matching: Using the grep() and grepl() functions, we can search for a pattern within the elements of a character vector. To find the names in the “names” vector that contain the letter “a”, for instance, we can use:\n\n\ngrep(\"a\", names)\n\n[1] 3 4\n\n\nThis returns a vector containing the indexes of the “names” vector elements that contain the letter “a.”\n\nRegular Expressions: We can use regular expressions with the grep() and grepl() functions to search for patterns in the elements of a character vector. To find the names in the “names” vector that begin with the letter “C”, for instance, we can use:\n\n\ngrep(\"^C\", names)\n\n[1] 3\n\n\nThis returns a vector containing the indexes of the elements in “names” that begin with the letter “C.” [1]\n\n\n\n\n[1]\nR Core Team. (2021). R: A Language and Environment for Statistical Computing. R Foundation for Statistical Computing. https://www.R-project.org/\nR Core Team. (2022). Vectors, Lists, and Arrays. R Documentation. https://cran.r-project.org/doc/manuals/r-release/R-intro.html#vectors-lists-and-arrays\nWickham, H., & Grolemund, G. (2016). R for data science: Import, tidy, transform, visualize, and model data. O’Reilly Media, Inc."
  },
  {
    "objectID": "03DataStructures.html#popular-data-structures",
    "href": "03DataStructures.html#popular-data-structures",
    "title": "Data Structures",
    "section": "",
    "text": "The R programming language includes a number of data structures that are frequently employed in data analysis and statistical modeling. These are some of the most popular data structures in R:\n\nVector: A vector is a one-dimensional array that stores identical data types, such as numeric, character, or logical. The “c()” function can be used to create vectors, and indexing can be used to access individual vector elements.\nFactor: A factor is a vector representing categorical data, with each distinct value or category represented as a level. Using indexing, individual levels of a factor can be accessed using the “factor()” function.\nDataframe: Similar to a spreadsheet, a data frame is a two-dimensional table-like structure that can store various types of data in columns. The “data.frame()” function can be used to construct data frames, and individual elements can be accessed using row and column indexing.\nMatrix: A matrix is a two-dimensional array of data with identical rows and columns. The “matrix()” function can be used to construct matrices, and individual elements can be accessed using row and column indexing.\nArray: An array is a multidimensional data structure that can contain data of the same data type in user-specified dimensions. Arrays can be constructed using the “array()” function, and elements can be accessed using multiple indexing.\nList: A list is an object that may comprise elements of various data types, including vectors, matrices, data frames, and even other lists. The “list()” function can be used to construct lists, while indexing can be used to access individual elements.\n\nThese data structures are helpful for storing and manipulating data in R, and they can be utilized in numerous applications, such as statistical analysis and data visualization.\nWe will focus our attention on Vectors, Factors and Dataframes, since we believe that these are the three most useful data structures. [1]"
  },
  {
    "objectID": "03DataStructures.html#vectors",
    "href": "03DataStructures.html#vectors",
    "title": "Data Structures",
    "section": "",
    "text": "A vector is a fundamental data structure in R that can hold a sequence of values of the same data type, such as integers, numeric, character, or logical values.\nA vector can be created using the c() function.\nR supports two forms of vectors: atomic vectors and lists. Atomic vectors are limited to containing elements of a single data type, such as numeric or character. Lists, on the other hand, can contain elements of various data types and structures. [1]\n\n\n\n\nThe following R code creates a numeric vector, a character vector and a logical vector respectively.\n\n\n# Read data into vectors\nnames &lt;- c(\"Ashok\", \"Bullu\", \"Charu\", \"Divya\")\nages &lt;- c(72, 49, 46, 42)\nfemales &lt;- c(FALSE, TRUE, TRUE, TRUE)\n\n\nThe c() function is employed to combine the four character elements into a single vector.\nCommas separate the elements of the vector within the parentheses.\nIndividual elements of the vector can be accessed via indexing, which utilizes square brackets []. For instance, names[1] returns “Ashok”, while names[3] returns “Charu”.\nWe can also perform operations such as categorizing and filtering on the entire vector. For instance, sort(names) returns a vector of sorted names, whereas names[names!= “Bullu”] returns a vector of names excluding “Bullu.”\n\n\n\n\nVectors can be used to perform the following vector operations:\n\nAccessing Elements: We can use indexing with square brackets to access individual elements of a vector. To access the second element of the “names” vector, for instance, we can use:\n\n\nnames[2]\n\n[1] \"Bullu\"\n\n\nThis returns “Bullu”, the second element of the “names” vector.\n\nConcatenation: The “c()” function can be used to combine multiple vectors into a single vector. For instance, to combine the “names” and “ages” vectors into the “people” vector, we can use:\n\n\npersons &lt;- c(names, ages)\npersons\n\n[1] \"Ashok\" \"Bullu\" \"Charu\" \"Divya\" \"72\"    \"49\"    \"46\"    \"42\"   \n\n\nThis generates an eight-element vector containing the names and ages of the four people.\n\nSubsetting: We can use indexing with a logical condition to construct a new vector that contains a subset of elements from an existing vector. For instance, to construct a new vector named “female_names” containing only the females’ names, we can use:\n\n\nfemale_names &lt;- names[females == TRUE]\nfemale_names\n\n[1] \"Bullu\" \"Charu\" \"Divya\"\n\n\nThis generates a new vector comprising three elements containing the names of the three females (“Bullu”, “Charu”, and “Divya”).\n\nArithmetic Operations: We can perform element-wise arithmetic operations on vectors. To calculate the sum of the “ages” vector, for instance, we can use:\n\n\nsum(ages)\n\n[1] 209\n\n\nThis returns 209, the sum of the four ages.\n\nLogical Operations: We can perform logical operations on vectors, which are also executed element-by-element. To create a new vector titled “middle_age” that indicates whether each individual is 45 to 55 years old, for instance, we can use:\n\n\nmiddle_age &lt;- (ages &gt;= 45) & (ages &lt;= 55)\nmiddle_age\n\n[1] FALSE  TRUE  TRUE FALSE\n\n\nThis generates a new vector with four elements containing logical values indicating whether each person is between 45 and 55 years of age.\nTo test whether any of the elements in the “ages” vector are greater than 50, we can use:\n\nany(ages &gt; 50)\n\n[1] TRUE\n\n\n\nUnique Values: We can find the unique values in a vector using the “unique()” function. For example, to find the unique values in the “ages” vector, we can use:\n\n\nunique(ages)\n\n[1] 72 49 46 42\n\n\n\nSorting: We can sort a vector in ascending or descending order using the “sort()” function. For example, to sort the “ages” vector in descending order, we can use:\n\n\nsort(ages, decreasing = TRUE)\n\n[1] 72 49 46 42\n\n\n\n\n\n\nLength: The length represents the count of the number of elements in a vector.\n\n\nlength(ages)\n\n[1] 4\n\n\n\nMaximum and Minimum: The maximum and minimum values are the vector’s greatest and smallest values, respectively.\nRange: The range is a measure of the spread that represents the difference between the maximum and minimum values in a vector.\n\n\nmin(ages)\n\n[1] 42\n\nmax(ages)\n\n[1] 72\n\nrange(ages)\n\n[1] 42 72\n\n\n\nMean: The mean is a central tendency measure that represents the average value of a vector’s elements.\nStandard Deviation: The standard deviation is a measure of dispersion that reflects the amount of variation in a vector’s elements.\nVariance: The variance is another measure of the spread. It is square of the Standard Deviation.\n\n\nmean(ages)\n\n[1] 52.25\n\nsd(ages)\n\n[1] 13.47529\n\nvar(ages)\n\n[1] 181.5833\n\n\n\nMedian: The median is a measure of central tendency that represents the middle value of a sorted vector.\n\n\nmedian(ages)\n\n[1] 47.5\n\n\n\nQuantiles: The quantiles are a set of cut-off points that divide a sorted vector into equal-sized groups.\n\n\nquantile(ages)\n\n   0%   25%   50%   75%  100% \n42.00 45.00 47.50 54.75 72.00 \n\n\nThis will return a set of five values, representing the minimum, first quartile, median, third quartile, and maximum of the four ages.\nThus, we note that the R programming language provides a wide range of statistical operations that can be performed on vectors for data analysis and modeling. Vectors are clearly a potent and versatile data structure that can be utilized in a variety of ways.\n\n\n\nHere are some common string operations that can be conducted using the provided vector examples.\n\nSubstring: The substr() function can be used to extract a substring from a character vector. To extract the first three characters of each name in the “names” vector, for instance, we can use:\n\n\nsubstr(names, 1, 3)\n\n[1] \"Ash\" \"Bul\" \"Cha\" \"Div\"\n\n\nThis returns a new character vector containing the initial three letters of each name (“Ash”, “Bul”, “Cha”, and “Div”).\n\nConcatenation: Using the paste() function, we can concatenate two or more character vectors into a singular vector. To create a new vector containing the names and ages of the individuals, for instance, we can use:\n\n\npersons &lt;- paste(names, ages)\npersons\n\n[1] \"Ashok 72\" \"Bullu 49\" \"Charu 46\" \"Divya 42\"\n\n\nThis will generate a new eight-element character vector containing the name and age of each individual, separated by a space.\n\nCase Conversion: The toupper() and tolower() functions can be used to convert the case of characters within a character vector. To convert the “names” vector to uppercase letters, for instance, we can use:\n\n\ntoupper(names)\n\n[1] \"ASHOK\" \"BULLU\" \"CHARU\" \"DIVYA\"\n\n\nThis will generate a new character vector with all of the names converted to uppercase.\n\nPattern Matching: Using the grep() and grepl() functions, we can search for a pattern within the elements of a character vector. To find the names in the “names” vector that contain the letter “a”, for instance, we can use:\n\n\ngrep(\"a\", names)\n\n[1] 3 4\n\n\nThis returns a vector containing the indexes of the “names” vector elements that contain the letter “a.”\n\nRegular Expressions: We can use regular expressions with the grep() and grepl() functions to search for patterns in the elements of a character vector. To find the names in the “names” vector that begin with the letter “C”, for instance, we can use:\n\n\ngrep(\"^C\", names)\n\n[1] 3\n\n\nThis returns a vector containing the indexes of the elements in “names” that begin with the letter “C.” [1]"
  },
  {
    "objectID": "03DataStructures.html#references",
    "href": "03DataStructures.html#references",
    "title": "Data Structures",
    "section": "",
    "text": "[1]\nR Core Team. (2021). R: A Language and Environment for Statistical Computing. R Foundation for Statistical Computing. https://www.R-project.org/\nR Core Team. (2022). Vectors, Lists, and Arrays. R Documentation. https://cran.r-project.org/doc/manuals/r-release/R-intro.html#vectors-lists-and-arrays\nWickham, H., & Grolemund, G. (2016). R for data science: Import, tidy, transform, visualize, and model data. O’Reilly Media, Inc."
  },
  {
    "objectID": "04ReadingData.html",
    "href": "04ReadingData.html",
    "title": "Reading Data",
    "section": "",
    "text": "Dataframes and Tibbles are frequently employed data structures in R for storing and manipulating data. They facilitate the organization, exploration, and analysis of data.\n\n\n\nA dataframe is a two-dimensional table-like data structure in R that stores data in rows and columns, with distinct data types for each column.\nSimilar to a spreadsheet or a SQL table, it is one of the most frequently employed data structures in R. Each column in a data.frame is a constant-length vector, and each row represents an observation or case.\nUsing the data.frame() function or by importing data from external sources such as CSV files, Excel spreadsheets, or databases, dataframe objects can be created in R.\ndataframe objects have many useful built-in methods and functions for manipulating and summarizing data, including subsetting, merging, filtering, and aggregation. [1]\n\n\n\n\nThe following code generates a data.frame named df containing three columns - names, ages, and heights, and four rows of data for each individual.\n\n\n# Create input data as vectors\nnames &lt;- c(\"Ashok\", \"Bullu\", \"Charu\", \"Divya\")\nages &lt;- c(72, 49, 46, 42)\nheights &lt;- c(170, 167, 160, 166)\n\n# Combine input data into a data.frame\npeople &lt;- data.frame(Name = names, Age = ages, Height = heights)\n\n# Print the resulting dataframe\nprint(people)\n\n   Name Age Height\n1 Ashok  72    170\n2 Bullu  49    167\n3 Charu  46    160\n4 Divya  42    166\n\n\n\n\n\n\n\nR contains a number of built-in datasets that can be accessed without downloading or integrating from external sources. Here are some of the most frequently used built-in datasets in R:\n\n\nwomen: This dataset includes the heights and weights of a sample of 15,000 women.\nmtcars: This dataset contains information on 32 distinct automobile models, including the number of cylinders, engine displacement, horsepower, and weight.\ndiamonds: This dataset includes the prices and characteristics of approximately 54,000 diamonds, including carat weight, cut, color, and clarity.\niris: This data set measures the sepal length, sepal width, petal length, and petal breadth of 150 iris flowers from three distinct species.\n\n\n\nAs an illustration, consider the women dataset inbuilt in R, which contains information about the heights and weights of women. It has just two variables:\n\nheight: Height of each woman in inches\nweight: Weight of each woman in pounds\nThe data() function is used to import any inbuilt dataset into R. The data(women) command in R loads the women dataset\n\n\ndata(women)\n\n\nThe str() function gives the dimensions and data types and also previews the data.\n\n\nstr(women)\n\n'data.frame':   15 obs. of  2 variables:\n $ height: num  58 59 60 61 62 63 64 65 66 67 ...\n $ weight: num  115 117 120 123 126 129 132 135 139 142 ...\n\n\n\nThe summary() function gives some summary statistics.\n\n\nsummary(women)\n\n     height         weight     \n Min.   :58.0   Min.   :115.0  \n 1st Qu.:61.5   1st Qu.:124.5  \n Median :65.0   Median :135.0  \n Mean   :65.0   Mean   :136.7  \n 3rd Qu.:68.5   3rd Qu.:148.0  \n Max.   :72.0   Max.   :164.0  \n\n\n\n\n\nThe mtcars dataset inbuilt in R comprises data on the fuel consumption and other characteristics of 32 different automobile models. Here is a concise description of the 11 mtcars data columns:\n\nmpg: Miles per gallon (fuel efficiency)\ncyl: Number of cylinders\ndisp: Displacement of the engine (in cubic inches)\nhp: gross horsepower\ndrat: Back axle ratio wt: Weight (in thousands of pounds)\nwt: Weight (in thousands of pounds)\nqsec: 1/4 mile speed (in seconds)\nvs: Type of engine (0 = V-shaped, 1 = straight)\nam: Type of transmission (0 for automatic, 1 for manual)\ngear: the number of forward gears\ncarb: the number of carburetors\n\n\ndata(mtcars)\nstr(mtcars)\n\n'data.frame':   32 obs. of  11 variables:\n $ mpg : num  21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ...\n $ cyl : num  6 6 4 6 8 6 8 4 4 6 ...\n $ disp: num  160 160 108 258 360 ...\n $ hp  : num  110 110 93 110 175 105 245 62 95 123 ...\n $ drat: num  3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ...\n $ wt  : num  2.62 2.88 2.32 3.21 3.44 ...\n $ qsec: num  16.5 17 18.6 19.4 17 ...\n $ vs  : num  0 0 1 1 0 1 0 1 1 1 ...\n $ am  : num  1 1 1 0 0 0 0 0 0 0 ...\n $ gear: num  4 4 4 3 3 3 3 4 4 4 ...\n $ carb: num  4 4 1 1 2 1 4 2 2 4 ...\n\n\n\n\n\n\n\nWe examine how to read data into a dataframe in R when the original data is stored in prominent file formats such as CSV, Excel, and Google Sheets.\nBefore learning how to accomplish this, it is necessary to comprehend how to configure the Working Directory in R.\n\n\n\n\nThe working directory is the location where R searches for and saves files by default.\nBy default, when we execute a script or import data into R, R will search the working directory for files.\nUsing R’s getwd() function, we can examine our current working directory:\n\n\ngetwd()\n\n[1] \"/cloud/project\"\n\n\n\nWe are running R in the Cloud and hence we are seeing that the working directory is specified as /cloud/project/DataAnalyticsBook101. If we are doing R programming on a local computer, and if our working directory is the Desktop, then we may see a different response such as C:/Users/YourUserName/Desktop.\nUsing R’s setwd() function, we can change our current working directory. For example, the following code will set our working directory to the Desktop:\n\n\nsetwd(\"C:/Users/YourUserName/Desktop\")\n\n\nWe should choose an easily-remembered and accessible working directory to store our R scripts and data files. Additionally, we should avoid using spaces, special characters, and non-ASCII characters in file paths, as these can cause file handling issues in R. [2]\n\n\n\n\n\nCSV is the abbreviation for “Comma-Separated Values.” A CSV file is a plain text file that stores structured tabular data.\nEach entry in a CSV file represents a record, whereas each column represents a field. The elements in each record are separated by commas (hence the name Comma-Separated Values), semicolons, or tabs.\nBefore proceeding ahead, it is imperative that the file that we wish to read is located in the Working Directory.\nSuppose we wish to import a CSV file named mtcars.csv, located in the Working Directory. We can use the read.csv() function, illustrated as follows.\n\n\ndf_csv &lt;- read.csv(\"mtcars.csv\")\n\n\nIn this example, the read.csv() function reads the mtcars.csv file into a data frame named df_csv.\nIf the file is not in the current working directory, the complete file path must be specified in the read.csv() function argument; otherwise, an error will occur.\n\n\n\n\n\nSuppose we wish to import a Microsoft Excel file named mtcars.xlsx, located in the Working Directory.\nWe can use the read_excel function in the R package readxl, illustrated as follows.\n\n\nlibrary(readxl)\ndf_xlsx &lt;- read_excel(\"mtcars.xlsx\")\n\n\n\n\n\nGoogle Sheets is a ubiquitous cloud-based spreadsheet application developed by Google. It is a web-based application that enables collaborative online creation and modification of spreadsheets.\nWe can import data from a Google Sheet into a R dataframe, as follows.\n\n\nConsider a Google Sheet whose preferences have been set such that anyone can view it using its URL. If this is not done, then some authentication would become necessary.\nEvery Google Sheet is characterized by a unique Sheet ID, embedded within the URL. For example, consider a Google Sheet containing some financial data concerning S&P500 index shares.\nSuppose the Sheet ID is: 11ahk9uWxBkDqrhNm7qYmiTwrlSC53N1zvXYfv7ttOCM\nWe can use the function gsheet2tbl in package gsheet to read the Google Sheet into a dataframe, as demonstrated in the following code.\n\n\n# Read S&P500 stock data present in a Google Sheet.\nlibrary(gsheet)\n\nprefix &lt;- \"https://docs.google.com/spreadsheets/d/\"\nsheetID &lt;- \"11ahk9uWxBkDqrhNm7qYmiTwrlSC53N1zvXYfv7ttOCM\"\n\n# Form the URL to connect to\nurl500 &lt;- paste(prefix, sheetID) \n\n# Read the Google Sheet located at the URL into a dataframe called sp500\nsp500 &lt;- gsheet2tbl(url500)\n\nNo encoding supplied: defaulting to UTF-8.\n\n\n\nThe first line imports the gsheet package required to access Google Sheets into R.\nThe following three lines define URL variables for Google Sheets. The prefix variable contains the base URL for accessing Google Sheets, the sheetID variable contains the ID of the desired Google Sheet.\nThe paste() function is used to combine the prefix, sheetID variables into a complete URL for accessing the Google Sheet.\nThe gsheet2tbl() function from the gsheet package is then used to read the specified Google Sheet into a dataframe called sp500, which can then be analyzed further in R.\n\n\n\n\n\nSuppose we have a second S&P 500 data located in a second Google Sheet and suppose that we would like to join or merge the data in this dataframe with the above dataframe sp500.\nThe ID of this second sheet is: 1F5KvFATcehrdJuGjYVqppNYC9hEKSww9rXYHCk2g6OA\nWe can read the data present in this Google Sheet using the following code, similar to the one discussed above, using the following code.\n\n\n# Read additional S&P500 data that is posted in a Google Sheet.\nlibrary(gsheet)\n\nprefix &lt;- \"https://docs.google.com/spreadsheets/d/\"\nsheetID &lt;- \"1nm688a3GsPM5cadJIwu6zj336WBaduglY9TSTUaM9jk\"\n\n# Form the URL to connect to\nurl &lt;- paste(prefix, sheetID) \n\n# Read the Google Sheet located at the URL into a dataframe called gf\ngf &lt;- gsheet2tbl(url)\n\nNo encoding supplied: defaulting to UTF-8.\n\n\n\nWe now have two dataframes named sp500 and gf that we wish to merge or join.\nThe two dataframes have a column named Stock in common, which will serve as the key, while doing the join.\nThe following code illusrates how to merge two dataframes:\n\n\n# merging dataframes\ndf &lt;- merge(sp500, gf , id = \"Stock\")\n\n\nWe now have a new dataframe named df, which contains the data got from merging the two dataframes sp500 and gf.\n\n\n\n\n\n\nA tibble is a contemporary and enhanced variant of a R data frame that is part of the tidyverse package collection.\nTibbles are created and manipulated using the dplyr package, which provides a suite of functions optimized for data manipulation.\nThe following characteristics distinguish a tibble from a conventional data frame:\nTibbles must always have unique, non-empty column names. Tibbles do not permit the creation or modification of columns using partial matching of column names. Tibbles improve the output of large datasets by displaying by default only a few rows and columns.\nTibbles have a more consistent behavior for subsetting, with the use of [[ always returning a vector or NULL, and [] always returning a tibble.\nHere is an example of using the tibble() function in dplyr to construct a tibble:\n\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n# Create a tibble\nmy_tibble &lt;- tibble(\n  name = c(\"Alice\", \"Bob\", \"Charlie\"),\n  age = c(25, 30, 35),\n  gender = c(\"F\", \"M\", \"M\")\n)\n\n# Print the tibble\nmy_tibble\n\n# A tibble: 3 × 3\n  name      age gender\n  &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt; \n1 Alice      25 F     \n2 Bob        30 M     \n3 Charlie    35 M     \n\n\n\nThis will generate a tibble consisting of three columns (name, age, and gender) and three rows of data. Note that the column names are preserved and the tibble is printed in a compact and legible manner.\n\n\n\n\n# Create a data frame\nmy_df &lt;- data.frame(\n  name = c(\"Alice\", \"Bob\", \"Charlie\"),\n  age = c(25, 30, 35),\n  gender = c(\"F\", \"M\", \"M\")\n)\n\n# Convert the data frame to a tibble\nmy_tibble &lt;- as_tibble(my_df)\n\n# Print the tibble\nmy_tibble\n\n# A tibble: 3 × 3\n  name      age gender\n  &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt; \n1 Alice      25 F     \n2 Bob        30 M     \n3 Charlie    35 M     \n\n\n\nThis assigns the tibble representation of the data frame my_df to the variable my_tibble.\nNote that the resulting tibble has the same column names and data as the original data frame, but has the additional characteristics and behaviors of a tibble.\n\n\n\n\n\nlibrary(dplyr)\n\n# Convert the tibble to a data frame\nmy_df &lt;- as.data.frame(my_tibble)\n\n# Print the data frame\nmy_df\n\n     name age gender\n1   Alice  25      F\n2     Bob  30      M\n3 Charlie  35      M\n\n\n\nA tibble offers several advantages over a data frame in R:\n\n\nLarge datasets can be printed with greater clarity and precision using Tibbles. By default, they only print the first few rows and columns, making it simpler to read and comprehend the data structure.\nBetter subsetting behavior: With [[always returning a vector or NULL and [] always returning a tibble, Tibbles have a more consistent subsetting behavior. This facilitates the subset and manipulation of data without unintended consequences.\nConsistent naming: Tibbles always have column names that are distinct and non-empty. This makes it simpler to refer to specific columns and prevents errors caused by duplicate or unnamed column names.\nMore informative errors: Tibbles provides more informative error messages that make it simpler to diagnose and resolve data-related problems.\nFewer surprises: Tibbles have more stringent constraints than data frames, resulting in fewer surprises and unexpected behavior when manipulating data.\n\n\n\n\n\n[1]\nR Core Team. (2021). R: A Language and Environment for Statistical Computing. R Foundation for Statistical Computing. https://www.R-project.org/\nR Core Team. (2022). Vectors, Lists, and Arrays. R Documentation. https://cran.r-project.org/doc/manuals/r-release/R-intro.html#vectors-lists-and-arrays\nWickham, H., & Grolemund, G. (2016). R for data science: Import, tidy, transform, visualize, and model data. O’Reilly Media, Inc.\nR Core Team. (2022, March 2). Data Frames. R Documentation. https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/data.frame\n[2]\nOpenIntro. (2022). 1.3 RStudio and working directory. In Introductory Statistics with Randomization and Simulation (1st ed.). https://www.openintro.org/book/isrs/\nR Core Team. (2021). getwd(): working directory; setwd(dir): change working directory. In R: A language and environment for statistical computing. R Foundation for Statistical Computing. https://stat.ethz.ch/R-manual/R-devel/library/base/html/getwd.html"
  },
  {
    "objectID": "04ReadingData.html#dataframes",
    "href": "04ReadingData.html#dataframes",
    "title": "Reading Data",
    "section": "",
    "text": "A dataframe is a two-dimensional table-like data structure in R that stores data in rows and columns, with distinct data types for each column.\nSimilar to a spreadsheet or a SQL table, it is one of the most frequently employed data structures in R. Each column in a data.frame is a constant-length vector, and each row represents an observation or case.\nUsing the data.frame() function or by importing data from external sources such as CSV files, Excel spreadsheets, or databases, dataframe objects can be created in R.\ndataframe objects have many useful built-in methods and functions for manipulating and summarizing data, including subsetting, merging, filtering, and aggregation. [1]\n\n\n\n\nThe following code generates a data.frame named df containing three columns - names, ages, and heights, and four rows of data for each individual.\n\n\n# Create input data as vectors\nnames &lt;- c(\"Ashok\", \"Bullu\", \"Charu\", \"Divya\")\nages &lt;- c(72, 49, 46, 42)\nheights &lt;- c(170, 167, 160, 166)\n\n# Combine input data into a data.frame\npeople &lt;- data.frame(Name = names, Age = ages, Height = heights)\n\n# Print the resulting dataframe\nprint(people)\n\n   Name Age Height\n1 Ashok  72    170\n2 Bullu  49    167\n3 Charu  46    160\n4 Divya  42    166"
  },
  {
    "objectID": "04ReadingData.html#reading-inbuilt-datasets-in-r",
    "href": "04ReadingData.html#reading-inbuilt-datasets-in-r",
    "title": "Reading Data",
    "section": "",
    "text": "R contains a number of built-in datasets that can be accessed without downloading or integrating from external sources. Here are some of the most frequently used built-in datasets in R:\n\n\nwomen: This dataset includes the heights and weights of a sample of 15,000 women.\nmtcars: This dataset contains information on 32 distinct automobile models, including the number of cylinders, engine displacement, horsepower, and weight.\ndiamonds: This dataset includes the prices and characteristics of approximately 54,000 diamonds, including carat weight, cut, color, and clarity.\niris: This data set measures the sepal length, sepal width, petal length, and petal breadth of 150 iris flowers from three distinct species.\n\n\n\nAs an illustration, consider the women dataset inbuilt in R, which contains information about the heights and weights of women. It has just two variables:\n\nheight: Height of each woman in inches\nweight: Weight of each woman in pounds\nThe data() function is used to import any inbuilt dataset into R. The data(women) command in R loads the women dataset\n\n\ndata(women)\n\n\nThe str() function gives the dimensions and data types and also previews the data.\n\n\nstr(women)\n\n'data.frame':   15 obs. of  2 variables:\n $ height: num  58 59 60 61 62 63 64 65 66 67 ...\n $ weight: num  115 117 120 123 126 129 132 135 139 142 ...\n\n\n\nThe summary() function gives some summary statistics.\n\n\nsummary(women)\n\n     height         weight     \n Min.   :58.0   Min.   :115.0  \n 1st Qu.:61.5   1st Qu.:124.5  \n Median :65.0   Median :135.0  \n Mean   :65.0   Mean   :136.7  \n 3rd Qu.:68.5   3rd Qu.:148.0  \n Max.   :72.0   Max.   :164.0  \n\n\n\n\n\nThe mtcars dataset inbuilt in R comprises data on the fuel consumption and other characteristics of 32 different automobile models. Here is a concise description of the 11 mtcars data columns:\n\nmpg: Miles per gallon (fuel efficiency)\ncyl: Number of cylinders\ndisp: Displacement of the engine (in cubic inches)\nhp: gross horsepower\ndrat: Back axle ratio wt: Weight (in thousands of pounds)\nwt: Weight (in thousands of pounds)\nqsec: 1/4 mile speed (in seconds)\nvs: Type of engine (0 = V-shaped, 1 = straight)\nam: Type of transmission (0 for automatic, 1 for manual)\ngear: the number of forward gears\ncarb: the number of carburetors\n\n\ndata(mtcars)\nstr(mtcars)\n\n'data.frame':   32 obs. of  11 variables:\n $ mpg : num  21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ...\n $ cyl : num  6 6 4 6 8 6 8 4 4 6 ...\n $ disp: num  160 160 108 258 360 ...\n $ hp  : num  110 110 93 110 175 105 245 62 95 123 ...\n $ drat: num  3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ...\n $ wt  : num  2.62 2.88 2.32 3.21 3.44 ...\n $ qsec: num  16.5 17 18.6 19.4 17 ...\n $ vs  : num  0 0 1 1 0 1 0 1 1 1 ...\n $ am  : num  1 1 1 0 0 0 0 0 0 0 ...\n $ gear: num  4 4 4 3 3 3 3 4 4 4 ...\n $ carb: num  4 4 1 1 2 1 4 2 2 4 ..."
  },
  {
    "objectID": "04ReadingData.html#reading-different-file-formats-into-a-dataframe",
    "href": "04ReadingData.html#reading-different-file-formats-into-a-dataframe",
    "title": "Reading Data",
    "section": "",
    "text": "We examine how to read data into a dataframe in R when the original data is stored in prominent file formats such as CSV, Excel, and Google Sheets.\nBefore learning how to accomplish this, it is necessary to comprehend how to configure the Working Directory in R.\n\n\n\n\nThe working directory is the location where R searches for and saves files by default.\nBy default, when we execute a script or import data into R, R will search the working directory for files.\nUsing R’s getwd() function, we can examine our current working directory:\n\n\ngetwd()\n\n[1] \"/cloud/project\"\n\n\n\nWe are running R in the Cloud and hence we are seeing that the working directory is specified as /cloud/project/DataAnalyticsBook101. If we are doing R programming on a local computer, and if our working directory is the Desktop, then we may see a different response such as C:/Users/YourUserName/Desktop.\nUsing R’s setwd() function, we can change our current working directory. For example, the following code will set our working directory to the Desktop:\n\n\nsetwd(\"C:/Users/YourUserName/Desktop\")\n\n\nWe should choose an easily-remembered and accessible working directory to store our R scripts and data files. Additionally, we should avoid using spaces, special characters, and non-ASCII characters in file paths, as these can cause file handling issues in R. [2]\n\n\n\n\n\nCSV is the abbreviation for “Comma-Separated Values.” A CSV file is a plain text file that stores structured tabular data.\nEach entry in a CSV file represents a record, whereas each column represents a field. The elements in each record are separated by commas (hence the name Comma-Separated Values), semicolons, or tabs.\nBefore proceeding ahead, it is imperative that the file that we wish to read is located in the Working Directory.\nSuppose we wish to import a CSV file named mtcars.csv, located in the Working Directory. We can use the read.csv() function, illustrated as follows.\n\n\ndf_csv &lt;- read.csv(\"mtcars.csv\")\n\n\nIn this example, the read.csv() function reads the mtcars.csv file into a data frame named df_csv.\nIf the file is not in the current working directory, the complete file path must be specified in the read.csv() function argument; otherwise, an error will occur.\n\n\n\n\n\nSuppose we wish to import a Microsoft Excel file named mtcars.xlsx, located in the Working Directory.\nWe can use the read_excel function in the R package readxl, illustrated as follows.\n\n\nlibrary(readxl)\ndf_xlsx &lt;- read_excel(\"mtcars.xlsx\")\n\n\n\n\n\nGoogle Sheets is a ubiquitous cloud-based spreadsheet application developed by Google. It is a web-based application that enables collaborative online creation and modification of spreadsheets.\nWe can import data from a Google Sheet into a R dataframe, as follows.\n\n\nConsider a Google Sheet whose preferences have been set such that anyone can view it using its URL. If this is not done, then some authentication would become necessary.\nEvery Google Sheet is characterized by a unique Sheet ID, embedded within the URL. For example, consider a Google Sheet containing some financial data concerning S&P500 index shares.\nSuppose the Sheet ID is: 11ahk9uWxBkDqrhNm7qYmiTwrlSC53N1zvXYfv7ttOCM\nWe can use the function gsheet2tbl in package gsheet to read the Google Sheet into a dataframe, as demonstrated in the following code.\n\n\n# Read S&P500 stock data present in a Google Sheet.\nlibrary(gsheet)\n\nprefix &lt;- \"https://docs.google.com/spreadsheets/d/\"\nsheetID &lt;- \"11ahk9uWxBkDqrhNm7qYmiTwrlSC53N1zvXYfv7ttOCM\"\n\n# Form the URL to connect to\nurl500 &lt;- paste(prefix, sheetID) \n\n# Read the Google Sheet located at the URL into a dataframe called sp500\nsp500 &lt;- gsheet2tbl(url500)\n\nNo encoding supplied: defaulting to UTF-8.\n\n\n\nThe first line imports the gsheet package required to access Google Sheets into R.\nThe following three lines define URL variables for Google Sheets. The prefix variable contains the base URL for accessing Google Sheets, the sheetID variable contains the ID of the desired Google Sheet.\nThe paste() function is used to combine the prefix, sheetID variables into a complete URL for accessing the Google Sheet.\nThe gsheet2tbl() function from the gsheet package is then used to read the specified Google Sheet into a dataframe called sp500, which can then be analyzed further in R.\n\n\n\n\n\nSuppose we have a second S&P 500 data located in a second Google Sheet and suppose that we would like to join or merge the data in this dataframe with the above dataframe sp500.\nThe ID of this second sheet is: 1F5KvFATcehrdJuGjYVqppNYC9hEKSww9rXYHCk2g6OA\nWe can read the data present in this Google Sheet using the following code, similar to the one discussed above, using the following code.\n\n\n# Read additional S&P500 data that is posted in a Google Sheet.\nlibrary(gsheet)\n\nprefix &lt;- \"https://docs.google.com/spreadsheets/d/\"\nsheetID &lt;- \"1nm688a3GsPM5cadJIwu6zj336WBaduglY9TSTUaM9jk\"\n\n# Form the URL to connect to\nurl &lt;- paste(prefix, sheetID) \n\n# Read the Google Sheet located at the URL into a dataframe called gf\ngf &lt;- gsheet2tbl(url)\n\nNo encoding supplied: defaulting to UTF-8.\n\n\n\nWe now have two dataframes named sp500 and gf that we wish to merge or join.\nThe two dataframes have a column named Stock in common, which will serve as the key, while doing the join.\nThe following code illusrates how to merge two dataframes:\n\n\n# merging dataframes\ndf &lt;- merge(sp500, gf , id = \"Stock\")\n\n\nWe now have a new dataframe named df, which contains the data got from merging the two dataframes sp500 and gf."
  },
  {
    "objectID": "04ReadingData.html#tibbles",
    "href": "04ReadingData.html#tibbles",
    "title": "Reading Data",
    "section": "",
    "text": "A tibble is a contemporary and enhanced variant of a R data frame that is part of the tidyverse package collection.\nTibbles are created and manipulated using the dplyr package, which provides a suite of functions optimized for data manipulation.\nThe following characteristics distinguish a tibble from a conventional data frame:\nTibbles must always have unique, non-empty column names. Tibbles do not permit the creation or modification of columns using partial matching of column names. Tibbles improve the output of large datasets by displaying by default only a few rows and columns.\nTibbles have a more consistent behavior for subsetting, with the use of [[ always returning a vector or NULL, and [] always returning a tibble.\nHere is an example of using the tibble() function in dplyr to construct a tibble:\n\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n# Create a tibble\nmy_tibble &lt;- tibble(\n  name = c(\"Alice\", \"Bob\", \"Charlie\"),\n  age = c(25, 30, 35),\n  gender = c(\"F\", \"M\", \"M\")\n)\n\n# Print the tibble\nmy_tibble\n\n# A tibble: 3 × 3\n  name      age gender\n  &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt; \n1 Alice      25 F     \n2 Bob        30 M     \n3 Charlie    35 M     \n\n\n\nThis will generate a tibble consisting of three columns (name, age, and gender) and three rows of data. Note that the column names are preserved and the tibble is printed in a compact and legible manner.\n\n\n\n\n# Create a data frame\nmy_df &lt;- data.frame(\n  name = c(\"Alice\", \"Bob\", \"Charlie\"),\n  age = c(25, 30, 35),\n  gender = c(\"F\", \"M\", \"M\")\n)\n\n# Convert the data frame to a tibble\nmy_tibble &lt;- as_tibble(my_df)\n\n# Print the tibble\nmy_tibble\n\n# A tibble: 3 × 3\n  name      age gender\n  &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt; \n1 Alice      25 F     \n2 Bob        30 M     \n3 Charlie    35 M     \n\n\n\nThis assigns the tibble representation of the data frame my_df to the variable my_tibble.\nNote that the resulting tibble has the same column names and data as the original data frame, but has the additional characteristics and behaviors of a tibble.\n\n\n\n\n\nlibrary(dplyr)\n\n# Convert the tibble to a data frame\nmy_df &lt;- as.data.frame(my_tibble)\n\n# Print the data frame\nmy_df\n\n     name age gender\n1   Alice  25      F\n2     Bob  30      M\n3 Charlie  35      M\n\n\n\nA tibble offers several advantages over a data frame in R:\n\n\nLarge datasets can be printed with greater clarity and precision using Tibbles. By default, they only print the first few rows and columns, making it simpler to read and comprehend the data structure.\nBetter subsetting behavior: With [[always returning a vector or NULL and [] always returning a tibble, Tibbles have a more consistent subsetting behavior. This facilitates the subset and manipulation of data without unintended consequences.\nConsistent naming: Tibbles always have column names that are distinct and non-empty. This makes it simpler to refer to specific columns and prevents errors caused by duplicate or unnamed column names.\nMore informative errors: Tibbles provides more informative error messages that make it simpler to diagnose and resolve data-related problems.\nFewer surprises: Tibbles have more stringent constraints than data frames, resulting in fewer surprises and unexpected behavior when manipulating data."
  },
  {
    "objectID": "04ReadingData.html#references",
    "href": "04ReadingData.html#references",
    "title": "Reading Data",
    "section": "",
    "text": "[1]\nR Core Team. (2021). R: A Language and Environment for Statistical Computing. R Foundation for Statistical Computing. https://www.R-project.org/\nR Core Team. (2022). Vectors, Lists, and Arrays. R Documentation. https://cran.r-project.org/doc/manuals/r-release/R-intro.html#vectors-lists-and-arrays\nWickham, H., & Grolemund, G. (2016). R for data science: Import, tidy, transform, visualize, and model data. O’Reilly Media, Inc.\nR Core Team. (2022, March 2). Data Frames. R Documentation. https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/data.frame\n[2]\nOpenIntro. (2022). 1.3 RStudio and working directory. In Introductory Statistics with Randomization and Simulation (1st ed.). https://www.openintro.org/book/isrs/\nR Core Team. (2021). getwd(): working directory; setwd(dir): change working directory. In R: A language and environment for statistical computing. R Foundation for Statistical Computing. https://stat.ethz.ch/R-manual/R-devel/library/base/html/getwd.html"
  },
  {
    "objectID": "05ExploringDataframes.html",
    "href": "05ExploringDataframes.html",
    "title": "Exploring Dataframes",
    "section": "",
    "text": "This chapter is incomplete, work-in-progress.\nHere’s how you can explore the mtcars data frame:\nView the data: Use the head() or tail() function to view the first or last few rows of the data frame, respectively. For example:\n\nFind the dimensions (rows and columns) of the dataframe\n\n\ndata(mtcars)\ndim(mtcars)\n\n[1] 32 11\n\n\n\nRetrieve the names of the columns in the dataframe\n\n\nnames(mtcars)\n\n [1] \"mpg\"  \"cyl\"  \"disp\" \"hp\"   \"drat\" \"wt\"   \"qsec\" \"vs\"   \"am\"   \"gear\"\n[11] \"carb\"\n\n\n\nstr()\n\nto display the structure of the dataframe, including the data type and the first few rows.\n\nstr(mtcars)\n\n'data.frame':   32 obs. of  11 variables:\n $ mpg : num  21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ...\n $ cyl : num  6 6 4 6 8 6 8 4 4 6 ...\n $ disp: num  160 160 108 258 360 ...\n $ hp  : num  110 110 93 110 175 105 245 62 95 123 ...\n $ drat: num  3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ...\n $ wt  : num  2.62 2.88 2.32 3.21 3.44 ...\n $ qsec: num  16.5 17 18.6 19.4 17 ...\n $ vs  : num  0 0 1 1 0 1 0 1 1 1 ...\n $ am  : num  1 1 1 0 0 0 0 0 0 0 ...\n $ gear: num  4 4 4 3 3 3 3 4 4 4 ...\n $ carb: num  4 4 1 1 2 1 4 2 2 4 ...\n\n\n\nhead()\n\nto view the first few rows of the dataframe.\n\nhead(mtcars)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n\n\ntail()\n\nto view the last few rows of the dataframe.\n\ntail(mtcars)\n\n                mpg cyl  disp  hp drat    wt qsec vs am gear carb\nPorsche 914-2  26.0   4 120.3  91 4.43 2.140 16.7  0  1    5    2\nLotus Europa   30.4   4  95.1 113 3.77 1.513 16.9  1  1    5    2\nFord Pantera L 15.8   8 351.0 264 4.22 3.170 14.5  0  1    5    4\nFerrari Dino   19.7   6 145.0 175 3.62 2.770 15.5  0  1    5    6\nMaserati Bora  15.0   8 301.0 335 3.54 3.570 14.6  0  1    5    8\nVolvo 142E     21.4   4 121.0 109 4.11 2.780 18.6  1  1    4    2\n\n\n\nsummary()\n\nto generate summary statistics for each column in the dataframe.\n\nsummary(mtcars)\n\n      mpg             cyl             disp             hp       \n Min.   :10.40   Min.   :4.000   Min.   : 71.1   Min.   : 52.0  \n 1st Qu.:15.43   1st Qu.:4.000   1st Qu.:120.8   1st Qu.: 96.5  \n Median :19.20   Median :6.000   Median :196.3   Median :123.0  \n Mean   :20.09   Mean   :6.188   Mean   :230.7   Mean   :146.7  \n 3rd Qu.:22.80   3rd Qu.:8.000   3rd Qu.:326.0   3rd Qu.:180.0  \n Max.   :33.90   Max.   :8.000   Max.   :472.0   Max.   :335.0  \n      drat             wt             qsec             vs        \n Min.   :2.760   Min.   :1.513   Min.   :14.50   Min.   :0.0000  \n 1st Qu.:3.080   1st Qu.:2.581   1st Qu.:16.89   1st Qu.:0.0000  \n Median :3.695   Median :3.325   Median :17.71   Median :0.0000  \n Mean   :3.597   Mean   :3.217   Mean   :17.85   Mean   :0.4375  \n 3rd Qu.:3.920   3rd Qu.:3.610   3rd Qu.:18.90   3rd Qu.:1.0000  \n Max.   :4.930   Max.   :5.424   Max.   :22.90   Max.   :1.0000  \n       am              gear            carb      \n Min.   :0.0000   Min.   :3.000   Min.   :1.000  \n 1st Qu.:0.0000   1st Qu.:3.000   1st Qu.:2.000  \n Median :0.0000   Median :4.000   Median :2.000  \n Mean   :0.4062   Mean   :3.688   Mean   :2.812  \n 3rd Qu.:1.0000   3rd Qu.:4.000   3rd Qu.:4.000  \n Max.   :1.0000   Max.   :5.000   Max.   :8.000  \n\n\n\ntable()\n\nto generate a frequency table for a categorical variable.\n\ntable(mtcars$cyl)\n\n\n 4  6  8 \n11  7 14 \n\n\n\nunique()\n\nto find unique values in a column of the dataframe.\n\nunique(mtcars$cyl)\n\n[1] 6 4 8\n\n\n\n\nHere are some examples of logical operations functions in R using the mtcars dataset:\n\nSubsetting based on a condition:\n\nThe logical expression [] and square bracket notation can be used to subset the mtcars dataset according to a criterion. For instance, to only choose the rows where the mpg is higher than 20:\n\n# Subset mtcars based on mpg &gt; 20\nmtcars_subset &lt;- mtcars[mtcars$mpg &gt; 20, ]\nmtcars_subset\n\n                mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nMazda RX4      21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag  21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710     22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive 21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nMerc 240D      24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nMerc 230       22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nFiat 128       32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nHonda Civic    30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nToyota Corolla 33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nToyota Corona  21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\nFiat X1-9      27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nPorsche 914-2  26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\nLotus Europa   30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\nVolvo 142E     21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2\n\n\n\nThe which() function:\n\nThe which() function returns the indexes of the vector’s members that adhere to a predicate. To determine the indices of the rows where mpg is larger than 20 for instance:\n\n# Find the indices of rows where mpg &gt; 20\nindices &lt;- which(mtcars$mpg &gt; 20)\nindices\n\n [1]  1  2  3  4  8  9 18 19 20 21 26 27 28 32\n\n\n\nThe ifelse() function:\n\nThe ifelse() function applies a logical condition to a vector and returns a new vector with values depending on whether the condition is TRUE or FALSE. It is a vectorized version of the if-else statement. For instance, to add a new column called high mpg that shows whether or not the mpg value is more than 20:\n\n# Create a new column \"high_mpg\" based on mpg &gt; 20\nmtcars$high_mpg &lt;- ifelse(mtcars$mpg &gt; 20, \"Yes\", \"No\")\n\n\nThe all() and any() functions:\n\nIf every element in a vector satisfies a logical criterion, the all() function returns TRUE; otherwise, it returns FALSE. If at least one element in a vector satisfies a logical criterion, the any() method returns TRUE; otherwise, it returns FALSE. To determine whether every value in the mpg column is larger than 20, for instance:\n\n# Check if all values in mpg column are greater than 20\nall(mtcars$mpg &gt; 20)\n\n[1] FALSE\n\n\nAnd to check if at least one value in the mpg column is greater than 20:\nCheck if any value in mpg column is greater than 20\n\nany(mtcars$mpg &gt; 20)\n\n[1] TRUE\n\n\n\n\n\n\nFunction to calculate average mileage:\n\n\navg_mileage &lt;- function(data) {\n  mean(data$mpg)\n}\n\n# Usage\navg_mileage(mtcars) # Returns the average mileage of all cars in the dataset\n\n[1] 20.09062\n\n\n\nFunction to plot a scatter plot of horsepower vs. miles per gallon:\n\n\nplot_horsepower_vs_mpg &lt;- function(data) {\n  plot(data$hp, data$mpg, xlab = \"Horsepower\", ylab = \"Miles per gallon\")\n}\n\n# Usage\nplot_horsepower_vs_mpg(mtcars) # Plots a scatter plot of horsepower vs. miles per gallon\n\n\n\n\n\nFunction to calculate average mileage for cars with a specific number of cylinders:\n\n\navg_mileage_by_cyl &lt;- function(data, cyl) {\n  mean(data$mpg[data$cyl == cyl])\n}\n\n# Usage\n\n# Returns the average mileage of cars with 4 cylinders\navg_mileage_by_cyl(mtcars, 4) \n\n[1] 26.66364\n\n# Returns the average mileage of cars with 6 cylinders\navg_mileage_by_cyl(mtcars, 6) \n\n[1] 19.74286\n\n\n\nFunction to calculate average horsepower for cars with a specific number of gears:\n\n\navg_hp_by_gear &lt;- function(data, gear) {\n  mean(data$hp[data$gear == gear])\n}\n\n# Returns the average horsepower of cars with 3 gears\navg_hp_by_gear(mtcars, 3) \n\n[1] 176.1333\n\n# Returns the average horsepower of cars with 4 gears\navg_hp_by_gear(mtcars, 4) \n\n[1] 89.5"
  },
  {
    "objectID": "05ExploringDataframes.html#logical-operations",
    "href": "05ExploringDataframes.html#logical-operations",
    "title": "Exploring Dataframes",
    "section": "",
    "text": "Here are some examples of logical operations functions in R using the mtcars dataset:\n\nSubsetting based on a condition:\n\nThe logical expression [] and square bracket notation can be used to subset the mtcars dataset according to a criterion. For instance, to only choose the rows where the mpg is higher than 20:\n\n# Subset mtcars based on mpg &gt; 20\nmtcars_subset &lt;- mtcars[mtcars$mpg &gt; 20, ]\nmtcars_subset\n\n                mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nMazda RX4      21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag  21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710     22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive 21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nMerc 240D      24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nMerc 230       22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nFiat 128       32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nHonda Civic    30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nToyota Corolla 33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nToyota Corona  21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\nFiat X1-9      27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nPorsche 914-2  26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\nLotus Europa   30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\nVolvo 142E     21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2\n\n\n\nThe which() function:\n\nThe which() function returns the indexes of the vector’s members that adhere to a predicate. To determine the indices of the rows where mpg is larger than 20 for instance:\n\n# Find the indices of rows where mpg &gt; 20\nindices &lt;- which(mtcars$mpg &gt; 20)\nindices\n\n [1]  1  2  3  4  8  9 18 19 20 21 26 27 28 32\n\n\n\nThe ifelse() function:\n\nThe ifelse() function applies a logical condition to a vector and returns a new vector with values depending on whether the condition is TRUE or FALSE. It is a vectorized version of the if-else statement. For instance, to add a new column called high mpg that shows whether or not the mpg value is more than 20:\n\n# Create a new column \"high_mpg\" based on mpg &gt; 20\nmtcars$high_mpg &lt;- ifelse(mtcars$mpg &gt; 20, \"Yes\", \"No\")\n\n\nThe all() and any() functions:\n\nIf every element in a vector satisfies a logical criterion, the all() function returns TRUE; otherwise, it returns FALSE. If at least one element in a vector satisfies a logical criterion, the any() method returns TRUE; otherwise, it returns FALSE. To determine whether every value in the mpg column is larger than 20, for instance:\n\n# Check if all values in mpg column are greater than 20\nall(mtcars$mpg &gt; 20)\n\n[1] FALSE\n\n\nAnd to check if at least one value in the mpg column is greater than 20:\nCheck if any value in mpg column is greater than 20\n\nany(mtcars$mpg &gt; 20)\n\n[1] TRUE"
  },
  {
    "objectID": "05ExploringDataframes.html#creating-new-functions-in-r",
    "href": "05ExploringDataframes.html#creating-new-functions-in-r",
    "title": "Exploring Dataframes",
    "section": "",
    "text": "Function to calculate average mileage:\n\n\navg_mileage &lt;- function(data) {\n  mean(data$mpg)\n}\n\n# Usage\navg_mileage(mtcars) # Returns the average mileage of all cars in the dataset\n\n[1] 20.09062\n\n\n\nFunction to plot a scatter plot of horsepower vs. miles per gallon:\n\n\nplot_horsepower_vs_mpg &lt;- function(data) {\n  plot(data$hp, data$mpg, xlab = \"Horsepower\", ylab = \"Miles per gallon\")\n}\n\n# Usage\nplot_horsepower_vs_mpg(mtcars) # Plots a scatter plot of horsepower vs. miles per gallon\n\n\n\n\n\nFunction to calculate average mileage for cars with a specific number of cylinders:\n\n\navg_mileage_by_cyl &lt;- function(data, cyl) {\n  mean(data$mpg[data$cyl == cyl])\n}\n\n# Usage\n\n# Returns the average mileage of cars with 4 cylinders\navg_mileage_by_cyl(mtcars, 4) \n\n[1] 26.66364\n\n# Returns the average mileage of cars with 6 cylinders\navg_mileage_by_cyl(mtcars, 6) \n\n[1] 19.74286\n\n\n\nFunction to calculate average horsepower for cars with a specific number of gears:\n\n\navg_hp_by_gear &lt;- function(data, gear) {\n  mean(data$hp[data$gear == gear])\n}\n\n# Returns the average horsepower of cars with 3 gears\navg_hp_by_gear(mtcars, 3) \n\n[1] 176.1333\n\n# Returns the average horsepower of cars with 4 gears\navg_hp_by_gear(mtcars, 4) \n\n[1] 89.5"
  },
  {
    "objectID": "06LiveCase01.html",
    "href": "06LiveCase01.html",
    "title": "Case Analysis of SP500",
    "section": "",
    "text": "Case Analysis of SP500\nWe will investigate data related to the S&P500 stocks.\nThis is the URL of the data we will use: https://docs.google.com/spreadsheets/d/11ahk9uWxBkDqrhNm7qYmiTwrlSC53N1zvXYfv7ttOCM/edit#gid=0\n\n1) Reading S&P500 data from a Google Sheet into a tibble\nThe Google Sheet ID is: 11ahk9uWxBkDqrhNm7qYmiTwrlSC53N1zvXYfv7ttOCM. We can use the function gsheet2tbl in package gsheet to read the Google Sheet into a dataframe, as demonstrated in the following code.\n\n# Read S&P500 stock data present in a Google Sheet.\nlibrary(gsheet)\n\nprefix &lt;- \"https://docs.google.com/spreadsheets/d/\"\nsheetID &lt;- \"11ahk9uWxBkDqrhNm7qYmiTwrlSC53N1zvXYfv7ttOCM\"\n# Form the URL to connect to\nurl500 &lt;- paste(prefix, sheetID) \n# Read the Google Sheet located at the URL into a tibble called sp500\nsp500 &lt;- gsheet2tbl(url500)\n\nNo encoding supplied: defaulting to UTF-8.\n\n\n\n\n1) Reviewing the data\nWe run the str() function to better understand the data.\n\nstr(sp500)\n\nClasses 'tbl_df', 'tbl' and 'data.frame':   503 obs. of  36 variables:\n $ Date                                   : chr  \"7/18/2023\" \"7/18/2023\" \"7/18/2023\" \"7/18/2023\" ...\n $ Ticker                                 : chr  \"A\" \"AAL\" \"AAP\" \"AAPL\" ...\n $ Description                            : chr  \"Agilent Technologies, Inc.\" \"American Airlines Group, Inc.\" \"Advance Auto Parts Inc.\" \"Apple Inc.\" ...\n $ Sector                                 : chr  \"Health Technology\" \"Transportation\" \"Retail Trade\" \"Electronic Technology\" ...\n $ Industry                               : chr  \"Medical Specialties\" \"Airlines\" \"Specialty Stores\" \"Telecommunications Equipment\" ...\n $ Market.Capitalization                  : num  3.54e+10 1.18e+10 4.20e+09 3.00e+12 2.40e+11 ...\n $ Price                                  : num  119.8 18.1 70.7 190.7 136 ...\n $ X52.Week.Low                           : num  113.3 11.7 63.6 124.2 131 ...\n $ X52.Week.High                          : num  160 19.1 212 194 168 194 116 78.3 328 524 ...\n $ Return.on.Equity..TTM.                 : num  24.8 NA 14.6 146 51.1 389 16 14.8 30.7 33.7 ...\n $ Return.on.Assets..TTM.                 : num  12.7 2.64 3.35 27.6 5.43 2.79 7.82 4.98 14.9 17.9 ...\n $ Return.on.Invested.Capital..TTM.       : num  16.51 5.44 6.17 57.18 9.9 ...\n $ Gross.Margin..TTM.                     : num  54.1 21.7 43.8 43.2 72.2 ...\n $ Operating.Margin..TTM.                 : num  23.78 7.3 5.63 29.16 41.07 ...\n $ Net.Margin..TTM.                       : num  19.19 3.39 3.61 24.49 13.3 ...\n $ Price.to.Earnings.Ratio..TTM.          : num  26.39 7.13 10.51 32.4 32.03 ...\n $ Price.to.Book..FY.                     : num  6.66 NA 1.56 60 13.95 ...\n $ Enterprise.Value.EBITDA..TTM.          : num  18.8 7.03 8.84 24.7 9.3 12.8 19.3 NA 17.5 33.2 ...\n $ EBITDA..TTM.                           : num  1.97e+09 6.02e+09 9.21e+08 1.24e+11 3.18e+10 ...\n $ EPS.Diluted..TTM.                      : num  4.54 2.54 6.72 5.89 4.25 ...\n $ EBITDA..TTM.YoY.Growth.                : num  10.52 NA -16 -5.36 10.6 ...\n $ EBITDA..Quarterly.YoY.Growth.          : num  8.2 NA -39.01 -4.58 11.68 ...\n $ EPS.Diluted..TTM.YoY.Growth.           : num  9.17 NA -25.21 -4.33 -39.11 ...\n $ EPS.Diluted..Quarterly.YoY.Growth.     : num  11.69944 NA -68.36829 -0.00656 -94.89037 ...\n $ Price.to.Free.Cash.Flow..TTM.          : num  30.06 6.94 NA 31 10.27 ...\n $ Free.Cash.Flow..TTM.YoY.Growth.        : num  11.81 NA -100.23 -7.85 6.68 ...\n $ Free.Cash.Flow..Quarterly.YoY.Growth.  : num  55.7078 648.1481 -176.1352 -0.0312 -15.3392 ...\n $ Debt.to.Equity.Ratio..MRQ.             : num  0.473 NA 1.582 1.763 4.678 ...\n $ Current.Ratio..MRQ.                    : num  2.37 0.718 1.244 0.94 0.96 ...\n $ Quick.Ratio..MRQ.                      : num  1.708 0.624 0.238 0.878 0.821 ...\n $ Dividend.Yield.Forward                 : num  0.752 NA 1.416 0.503 4.353 ...\n $ Dividends.per.share..Annual.YoY.Growth.: num  8.25 NA 84.62 5.88 7.53 ...\n $ Price.to.Sales..FY.                    : num  5.246 0.243 0.385 7.895 4.166 ...\n $ Revenue..TTM.YoY.Growth.               : num  7.8597 50.2948 1.4153 -0.2544 0.0282 ...\n $ Revenue..Quarterly.YoY.Growth.         : num  6.85 36.97 1.29 -2.51 -9.7 ...\n $ Technical.Rating                       : chr  \"Sell\" \"Strong Buy\" \"Neutral\" \"Strong Buy\" ...\n\n\nThe str(sp500) output provides valuable insights into the structure and data types of the columns in the sp500 tibble. Let’s delve into the details:\nThe output reveals that sp500 is a tibble with dimensions [503 × 36]. This means it consists of 503 rows, each representing a specific S&P500 stock, and 36 columns containing information about each stock.\nHere’s a breakdown of the information associated with each column:\n\nThe columns labeled Date, Stock, Description, Sector, and Industry are character columns. They respectively represent the date, stock ticker symbol, description, sector, and industry of each S&P500 stock.\nColumns such as Market.Capitalization, Price, X52.Week.Low, X52.Week.High, and other numeric columns contain diverse financial metrics and stock prices related to the S&P500 stocks.\nThe column labeled Technical.Rating is a character column that assigns a technical rating to each stock.\n\nBy examining the str(sp500) output, you gain a comprehensive understanding of the data types and column names present in the sp500 tibble, enabling you to grasp the structure of the dataset effectively.\n\n\nRename Data Columns\n\nlibrary(dplyr)\n\n\n# Define a mapping of new column names\nnew_names &lt;- c(\n  \"Date\", \"Stock\", \"Desc\", \"Sector\", \"Industry\", \n  \"MarketCap\", \"Price\", \"Low52Wk\", \"High52Wk\", \n  \"ROE\", \"ROA\", \"ROIC\", \"GrossMargin\", \n  \"OperatingMargin\", \"NetMargin\", \"PE\", \n  \"PB\", \"EVEBITDA\", \"EBITDA\", \"EPS\", \n  \"EBITDA_YOY\", \"EBITDA_QYOY\", \"EPS_YOY\", \n  \"EPS_QYOY\", \"PFCF\", \"FCF\", \n  \"FCF_QYOY\", \"DebtToEquity\", \"CurrentRatio\", \n  \"QuickRatio\", \"DividendYield\", \n  \"DividendsPerShare_YOY\", \"PS\", \n  \"Revenue_YOY\", \"Revenue_QYOY\", \"TechRating\"\n)\n# Rename the columns using the new_names vector\nsp500 &lt;- sp500 %&gt;% \n  rename_with(~ new_names, everything())\n\nThis code is designed to rename the columns of the sp500 tibble using a predefined mapping of new column names. Let’s go through the code step by step:\n\nA vector named new_names is created, which contains the desired new names for each column in the sp500 tibble. Each element in the new_names vector corresponds to a specific column in the sp500 tibble and represents the desired new name for that column.\nThe %&gt;% operator, often referred to as the pipe operator, is used to pass the sp500 tibble to the subsequent operation in a more readable and concise manner.\nThe rename_with() function from the dplyr package is applied to the sp500 tibble. This function allows you to rename columns based on a specified function or formula.\nIn this case, a formula ~ new_names is used as the first argument of rename_with(). This formula indicates that the new names for the columns should be sourced from the new_names vector.\nThe second argument, everything(), specifies that the renaming should be applied to all columns in the sp500 tibble.\nFinally, the resulting tibble with the renamed columns is assigned back to the sp500 variable, effectively updating the tibble with the new column names.\n\nIn essence, the code uses the new_names vector as a mapping to assign new column names to the sp500 tibble, ensuring that each column is given the desired new name specified in new_names.\n\n\nReview Data Columns\n\n\nData Description\n\n\n\n\n\n\n\n\n\n\nTable 1: Desription of Column Names\n\n\n\nName\nDescription\n\n\n\n\nDate\nDate of the observation\n\n\nStock\nStock ticker symbol\n\n\nDesc\nDescription of the stock\n\n\nSector\nSector the stock belongs to\n\n\nIndustry\nIndustry the stock belongs to\n\n\nMarketCap\nMarket capitalization of the company\n\n\nPrice\nStock price\n\n\nLow52Wk\n52-week low price\n\n\nHigh52Wk\n52-week high price\n\n\nROE\nReturn on Equity\n\n\nROA\nReturn on Assets\n\n\nROIC\nReturn on Invested Capital\n\n\nGrossMargin\nGross Margin\n\n\nOperatingMargin\nOperating Margin\n\n\nNetMargin\nNet Margin\n\n\nPE\nPrice-to-Earnings Ratio\n\n\nPB\nPrice-to-Book Ratio\n\n\nEVEBITDA\nEnterprise Value to EBITDA\n\n\nEBITDA\nEBITDA\n\n\nEPS\nEarnings per Share\n\n\nEBITDA_YOY\nEBITDA Year-over-Year Growth\n\n\nEBITDA_QYOY\nEBITDA Quarterly Year-over-Year Growth\n\n\nEPS_YOY\nEPS Year-over-Year Growth\n\n\nEPS_QYOY\nEPS Quarterly Year-over-Year Growth\n\n\nPFCF\nPrice-to-Free Cash Flow\n\n\nFCF\nFree Cash Flow\n\n\nFCF_QYOY\nFree Cash Flow Quarterly Year-over-Year Growth\n\n\nDebtToEquity\nDebt-to-Equity Ratio\n\n\nCurrentRatio\nCurrent Ratio\n\n\nQuickRatio\nQuick Ratio\n\n\nDividendYield\nDividend Yield\n\n\nDividendsPerShare_YOY\nAnnual Dividends per Share Year-over-Year Growth\n\n\nPS\nPrice-to-Sales Ratio\n\n\nRevenue_YOY\nRevenue Year-over-Year Growth\n\n\nRevenue_QYOY\nRevenue Quarterly Year-over-Year Growth\n\n\nTechRating\nTechnical Rating"
  },
  {
    "objectID": "01GettingStarted.html",
    "href": "01GettingStarted.html",
    "title": "Getting Started",
    "section": "",
    "text": "Getting Started\nJanuary 21, 2024.\nPosit Cloud, known previously as RStudio Cloud, is a versatile and user-friendly cloud computing platform designed for data science activities. This platform enables users to perform a variety of data science tasks directly in a web browser without needing to install or configure software locally. Here are its main features and advantages:\n\nCloud-Based Environment: Posit RCloud provides access to essential data science tools like RStudio IDE and Jupyter Notebooks via a web browser, facilitating easy access to data analysis tools.\nEase of Use: The platform’s interface is accessible to users at all levels of expertise, from novices to experienced data scientists, and includes comprehensive tools for coding, analysis, and data visualization.\nCollaboration Tools: It offers robust collaboration features, making it ideal for team projects, educational settings, and workshops. Projects can easily be shared within teams or publicly.\nWide Accessibility: As a browser-based platform, Posit RCloud is highly accessible and does not require advanced computational resources, making it a convenient option for a broad range of users.\nIntegrated Data Science Features: The platform integrates various data science tools, including version control (with Git support), efficient package management, and capabilities for publishing and sharing results.\nSecurity Measures: Posit RCloud prioritizes data security, employing project isolation in containers, SSL encryption for access, secure login protocols, and encryption for metadata and project data.\nScalability: The platform’s scalable nature makes it suitable for different sizes of projects, from individual researchers to large teams or classes.\nEducational Applications: In academic circles, it is widely adopted for data science education, enabling instructors to create and manage classroom environments, assignments, and student evaluations with ease.\nBusiness Applications: For businesses, Posit RCloud streamlines teamwork, ensures uniformity in project environments, and enhances project management efficiency.\n\nOverall, Posit RCloud stands out as a comprehensive, cloud-based solution for diverse data science endeavors, making advanced data analysis and collaboration more accessible and efficient for a wide array of users.\nPlease consider creating an account on Posit Cloud."
  },
  {
    "objectID": "04TTest.html",
    "href": "04TTest.html",
    "title": "T-tests",
    "section": "",
    "text": "February 04, 2024."
  },
  {
    "objectID": "04TTest.html#business-applications-of-one-sample-t-test",
    "href": "04TTest.html#business-applications-of-one-sample-t-test",
    "title": "T-tests",
    "section": "Business Applications of One-Sample t-test",
    "text": "Business Applications of One-Sample t-test\n\nMarketing:\nA one-sample t-test can be used in various ways to make data-driven decisions. Here are some marketing applications of one-sample t-test:\n\nTesting for ad effectiveness: A one-sample t-test can be performed to ascertain whether a marketing campaign has been successful in raising brand recognition or sales. Customers can be randomly chosen and asked to complete surveys both before and after the promotion. To ascertain whether the campaign was successful, a one-sample t-test can be used to compare the mean survey scores from before and after the campaign.\nPrice testing: Customers’ acceptance of a product’s price can be assessed using a one-sample t-test. To ascertain a sample of customers’ purchasing intentions, a survey might be conducted. One-sample t-tests can be used to compare the sample mean to the product’s pricing to determine whether it is too high or too cheap.\nProduct testing: To find out if a product satisfies client expectations, utilise the one-sample t-test. Customers can be asked to complete a survey to review a product on its quality, dependability, and design, among other qualities. To ascertain whether the product matches customer expectations, one-sample t-tests can be used to compare the sample’s mean scores to the hypothesised mean.\nCustomer satisfaction testing: To find out whether customers are pleased with a product or service, utilise the one-sample t-test. To gauge customer happiness, a sample of customers can complete a survey. A one-sample t-test can be used to compare the sample’s mean score to a hypothesised mean in order to assess whether or not customers are happy.\nMarket research: One-sample t-tests can be used by businesses to examine survey data and determine whether a sample’s responses significantly deviate from an assumed mean. Understanding consumer preferences and making wise company decisions can both benefit from this.\n\n\n\nFinance:\nIn finance, one-sample t-test can be used in various ways to make data-driven decisions. Here are some finance applications of one-sample t-test:\n\nStock market analysis: If a stock’s returns are considerably different from a benchmark index or an expected return, one-sample t-test can be employed to detect this. If the stock’s performance differs significantly from expectations, a one-sample t-test can be used to compare a sample of the returns to the expected return.\nCredit risk analysis: If a borrower’s credit score differs significantly from the population’s average credit score, one-sample t-test can be employed to ascertain this. One-sample t-test can be used to compare a sample of borrower credit scores to the population’s average credit score to see if there is a significant difference.\nPortfolio performance analysis: If a portfolio’s returns differ considerably from a benchmark index or an expected return, one-sample t-test can be employed to assess this. If the portfolio’s performance differs significantly from expectations, a one-sample t-test can be used to compare a sample of the returns to the predicted return.\nHypothesis testing: Financial hypotheses can be tested using the one-sample t-test, including the ones that a variable has a normal distribution and that an asset class’s mean return is zero.\n\n\n\nOrganizational Behavior:\nIn organizational behavior, one-sample t-test can be used in various ways to make data-driven decisions. Here are some organizational behavior applications of one-sample t-test:\n\nEmployee satisfaction surveys: A one-sample t-test can be used to examine whether the average score in a survey of employees is substantially different from an average score that has been hypothesised. One-sample t-test can be used to compare the survey results to the estimated average score to see if employee satisfaction differs significantly from what was predicted.\nTraining program evaluations: If the average score of a training programme evaluation differs considerably from a predicted average score, a one-sample t-test can be performed to discover this. The effectiveness of the training programme can be assessed by comparing the evaluation results to the average score that was hypothesised using a one-sample t-test.\nEmployee performance evaluations: If the average score of an employee performance review differs considerably from a predicted average score, a one-sample t-test can be performed to determine this. One-sample t-tests can be used to compare the evaluation results to the hypothetical average score to see if the employee’s performance differed considerably from what was anticipated.\nOrganizational culture assessments: If the average score of an organisational culture evaluation differs considerably from a predicted average score, a one-sample t-test can be performed to determine this. One-sample t-test can be used to examine whether the organisational culture is significantly different from what was anticipated by comparing the assessment findings to the average score that was hypothesised."
  },
  {
    "objectID": "04TTest.html#concept-behind-the-one-sample-t-test",
    "href": "04TTest.html#concept-behind-the-one-sample-t-test",
    "title": "T-tests",
    "section": "Concept behind the One-Sample t-test",
    "text": "Concept behind the One-Sample t-test\n\nThe mean of a single sample of observations, commonly referred to as the “hypothesised mean,” is compared to a known value using a one-sample t-test, a statistical test.\nThe test can be used to derive conclusions about the population from which the sample was taken and determines whether the sample mean differs significantly from the anticipated mean.\nThe t-statistic, which is the difference between the sample mean and the hypothesised mean divided by the standard error of the mean, serves as the foundation for the test.\nThe chance of observing the sample mean if the genuine population mean is identical to the hypothesised mean is then calculated by comparing the t-statistic to a t-distribution.\nIf this probability is small, it suggests that the sample mean is significantly different from the hypothesized mean, and the null hypothesis (that the sample mean is equal to the hypothesized mean) can be rejected."
  },
  {
    "objectID": "04TTest.html#running-the-one-sample-t-test-in-r",
    "href": "04TTest.html#running-the-one-sample-t-test-in-r",
    "title": "T-tests",
    "section": "Running the One-Sample t-test in R",
    "text": "Running the One-Sample t-test in R\n\nRead the data into a dataframe\n\n\ndata(mtcars)\nattach(mtcars)\n\n\nConvert the data type of categorical data to factor\n\n\nmtcars$cyl &lt;- as.factor(mtcars$cyl)\nmtcars$am  &lt;- as.factor(mtcars$am)\nmtcars$gear  &lt;- as.factor(mtcars$gear)\nmtcars$vs  &lt;- as.factor(mtcars$vs)\n\n\nAs an illustration, suppose that our objective is to compare whether the mean weight of the cars is different from 3 (1000 lbs), a value determined in a previous study.\n\n\nH0: The mean weight of the cars is not different from 3 (1000 lbs).\nH1: The mean weight of the cars is different from 3 (1000 lbs).\n\n\nCode:\n\n\n# One-sample t-test\nres &lt;- t.test(wt, mu = 3)\n# Printing the results\nres \n\n\n    One Sample t-test\n\ndata:  wt\nt = 1.256, df = 31, p-value = 0.2185\nalternative hypothesis: true mean is not equal to 3\n95 percent confidence interval:\n 2.864478 3.570022\nsample estimates:\nmean of x \n  3.21725 \n\n\n\nThe p-value of the test is 0.2185, which is greater than the significance level alpha = 0.05.\nSo we fail to reject the null hypothesis.\nWe can conclude that the mean weight of the cars is not different from theoretical mean 3 (1000 lbs)"
  },
  {
    "objectID": "04TTest.html#business-applications-of-independent-samples-t-test",
    "href": "04TTest.html#business-applications-of-independent-samples-t-test",
    "title": "T-tests",
    "section": "Business Applications of Independent Samples t-test",
    "text": "Business Applications of Independent Samples t-test\n\nMarketing:\n\nA/B testing: The effectiveness of two versions of a website or marketing campaign can be compared using the two-sample t-test to see if there are any significant differences. To establish which version is more successful, a sample of consumers can be randomly assigned to either one, and their replies can then be compared using a two-sample t-test.\nProduct comparisons: To find out if the means of two different items are substantially different in terms of customer preferences, apply the two-sample t-test. To establish which product is preferred, a sample of consumers can be asked to rate the two goods, and the mean scores can be compared using a two-sample t-test.\nPrice comparisons: To find out if the means of two alternative prices are significantly different in terms of client preferences, apply the two-sample t-test. To find out which price is more acceptable, a sample of customers can be polled about their willingness to pay for a product at two different prices. The mean scores from the comparison can then be compared using a two-sample t-test.\nDemographic comparisons: The two-sample t-test can be used to determine whether the means of two different demographic groups’ purchasing patterns are significantly different from one another. To find out if there are any notable differences in the purchasing habits of a sample of customers from two different demographic groups, the mean scores can be compared using a two-sample t-test.\n\n\n\nFinance:\n\nInvestment strategy comparisons: To find out if the means of two alternative investing strategies are substantially different in terms of their returns, apply the two-sample t-test. The strategy with a significantly greater return can be determined by comparing a sample of investments in each strategy using a two-sample t-test.\nAsset class comparisons: To find out if the means of two separate asset classes have returns that differ considerably, apply the two-sample t-test. The asset class with the considerably greater return can be identified by comparing a sample of investments in each asset class using a two-sample t-test.\nIndustry comparisons: To find out if the means of two distinct industries have statistically different financial performance, apply the two-sample t-test. The two-sample t-test can be used to compare a sample of businesses from each industry to discover which has significantly better financial performance.\nPortfolio performance comparisons: If the means of two separate portfolios differ considerably in terms of returns, the two-sample t-test can be employed to determine this. The portfolio with a significantly better return can be determined by comparing a sample of investments in each portfolio using a two-sample t-test.\n\n\n\nOrganizational Behavior:\n\nGender and diversity studies: If there is a statistically significant difference between the means of two different gender or diversity groups with regard to their work attitudes, job satisfaction, or other pertinent factors, the two-sample t-test can be employed to ascertain this. The two-sample t-test can be used to examine a sample of employees from each group to see whether there are any notable differences in their attitudes or job satisfaction.\nLeadership studies: To find out if the means of two alternative leadership philosophies are noticeably different in terms of their efficacy, apply the two-sample t-test. To ascertain whether leadership style is more efficient in terms of employee performance, work happiness, or other pertinent characteristics, a sample of employees from each leadership style can be compared using a two-sample t-test.\nOrganizational change studies: The means of two distinct employee groups can be compared using the two-sample t-test to see if they differ significantly in terms of how they reacted to organisational change. To see if there are any appreciable variations in the employees’ attitudes or levels of job satisfaction, a sample of those who are exposed to the change and those who are not can be compared using a two-sample t-test.\nWorkload and job demands studies: To find out if the means of two different job roles or teams have significantly different workloads or job demands, apply the two-sample t-test. The two-sample t-test can be used to examine a sample of individuals from each job role or team to see whether there are any notable differences in the workload or job demands."
  },
  {
    "objectID": "04TTest.html#concept-behind-the-two-sample-t-test",
    "href": "04TTest.html#concept-behind-the-two-sample-t-test",
    "title": "T-tests",
    "section": "Concept behind the Two Sample t-test",
    "text": "Concept behind the Two Sample t-test\n\nExample: Compare whether the mean weight of the cars having am = 0 is significantly different from mean weight of the cars having am = 1.\n\n\nH0: The mean weight of the cars having am = 0 is not significantly different from mean weight of the cars having am = 1\nH1: The mean weight of the cars having am = 0 is significantly different from mean weight of the cars having am = 1\n\n\n# Computing t-test\ntst &lt;- t.test(wt~am, var.equal = TRUE)\ntst\n\n\n    Two Sample t-test\n\ndata:  wt by am\nt = 5.2576, df = 30, p-value = 1.125e-05\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n 0.8304317 1.8853577\nsample estimates:\nmean in group 0 mean in group 1 \n       3.768895        2.411000 \n\n\n\nThe p-value of the test is 1.125e-05, which is less than the significance level alpha = 0.05.\nWe can reject the null hypothesis.\nWe conclude that mean weight of the cars having am = 0 is significantly different from mean weight of the cars having am = 1"
  },
  {
    "objectID": "04TTest.html#business-applications-of-dependent-sample-t-test",
    "href": "04TTest.html#business-applications-of-dependent-sample-t-test",
    "title": "T-tests",
    "section": "Business Applications of Dependent Sample t-test",
    "text": "Business Applications of Dependent Sample t-test\n\nMarketing:\n\nBefore-and-after studies: If there is a significant difference between the outcomes of a marketing campaign or intervention before and after, the dependent sample t-test can be employed to ascertain this. To determine whether there has been a substantial shift in the customers’ opinions or behaviour, a sample of consumers can be polled before and after the campaign or intervention, and their replies can be compared using a dependent sample t-test.\nLoyalty program evaluations: To find out if there is a significant difference in consumer spending or loyalty before and after the launch of a loyalty programme, utilise the dependent sample t-test. To ascertain whether there is a significant difference, a sample of customers can be followed before and after the program’s launch, and their spending or loyalty can be compared using a dependent sample t-test.\nProduct feature evaluations: If there is a substantial difference in client preferences for a product with or without a certain feature, the dependent sample t-test can be employed to ascertain it. To ascertain whether there is a significant difference, a sample of consumers can be asked to rate the product both with and without the feature, and their replies can be compared using a dependent sample t-test.\nSales promotion evaluations: To find out if there is a significant difference in sales before and after a sales promotion, apply the dependent sample t-test. To find out if there is a substantial difference, sales data can be collected before and after the promotion and compared using dependent sample t-test.\n\n\n\nFinance:\n\nInvestment performance evaluations: If there is a substantial difference in investment performance between two separate time periods, the dependent sample t-test can be performed to ascertain this. The dependent sample t-test can be used to compare a sample of investments before and after a particular event or time period, like as the introduction of a new policy or a shift in the market, to see if there is a significant difference in performance.\nTrading strategy evaluations: To find out if there is a significant difference in the performance of a trading strategy before and after a particular event or time period, utilise the dependent sample t-test. If there is a significant difference in performance between before and after the event or time period, a sample of trading strategies can be compared using a dependent sample t-test to find out.\nRisk management evaluations: To find out if there is a significant difference in risk before and after a particular event or time period, apply the dependent sample t-test. To evaluate if there is a significant difference in risk before and after the event or time period, a sample of risk management solutions can be compared using a dependent sample t-test.\nPortfolio management evaluations: To find out if there is a significant difference in portfolio performance before and after a particular event or time period, apply the dependent sample t-test. To establish if there is a significant difference in performance before and after the event or time period, a sample of portfolios can be compared using a dependent sample t-test.\n\n\n\nOrganizational Behavior:\n\nTraining evaluations: To find out if there is a significant change in employee performance between before and after a certain training programme, utilise the dependent sample t-test. To determine whether there is a significant difference in the performance of a sample of employees, their performance can be assessed before and after the training programme and compared using a dependent sample t-test.\nEmployee engagement evaluations: To find out if there is a significant difference in employee engagement between before and after a given intervention or policy change, utilise the dependent sample t-test. The dependent sample t-test can be used to examine the engagement levels of a sample of employees before and after the intervention or policy change and see whether there is a statistically significant difference.\nLeadership evaluations: To evaluate whether there is a significant difference in leadership effectiveness between before and after a certain leadership development programme, a dependent sample t-test can be utilised. A dependent sample t-test can be used to assess the perceptions of leadership effectiveness of a sample of employees before and after the programme to see if there is a statistically significant difference.\nOrganizational change evaluations: To evaluate whether there is a significant difference in employee attitudes before and after a certain organisational change, utilise the dependent sample t-test. To determine whether there is a significant difference, a sample of employees can be assessed before and after the change, and their views of the change can be compared using a dependent sample t-test."
  },
  {
    "objectID": "04TTest.html#concept-behind-the-paired-sample-t-test",
    "href": "04TTest.html#concept-behind-the-paired-sample-t-test",
    "title": "T-tests",
    "section": "Concept behind the Paired sample t-test",
    "text": "Concept behind the Paired sample t-test\n\nIt is a statistical method used to check if the mean difference between two sets of observations is zero is the paired sample t-test, also known as the dependent sample t-test.\nEach subject or thing is measured twice in a paired sample t-test, yielding pairs of observations.\n\n\nHypothesis for Paired sample t-test\n\nSuppose a training program was conducted to improve the participants’ knowledge of ICT. Data were collected from a selected sample of 10 individuals before and after the ICT training program.\nTest the hypothesis that the training is effective to improve the participants’ knowledge of ICT at 95% level of significance.\nHypotheses\n\nH0: there is no difference in participants’ knowledge before and after the ICT training\nH1: ICT training affected the participant’s knowledge\n\n\nICT training data\n\nLet’s create this data set. First, create before and after as objects containing the scores of ICT training.\n\n\nbefore &lt;- c(12.2, 14.6, 13.4, 11.2, 12.7, 10.4, 15.8, 13.9, 9.5, 14.2)\nafter &lt;- c(13.5, 15.2, 13.6, 12.8, 13.7, 11.3, 16.5, 13.4, 8.7, 14.6)\n\n\nNow create a data matrix using data.frame() function.\n\n\ndf &lt;- data.frame(subject = rep(c(1:10), 2), \n                   time = rep(c(\"before\", \"after\"), each = 10),\n                   score = c(before, after))\nprint(df)\n\n   subject   time score\n1        1 before  12.2\n2        2 before  14.6\n3        3 before  13.4\n4        4 before  11.2\n5        5 before  12.7\n6        6 before  10.4\n7        7 before  15.8\n8        8 before  13.9\n9        9 before   9.5\n10      10 before  14.2\n11       1  after  13.5\n12       2  after  15.2\n13       3  after  13.6\n14       4  after  12.8\n15       5  after  13.7\n16       6  after  11.3\n17       7  after  16.5\n18       8  after  13.4\n19       9  after   8.7\n20      10  after  14.6\n\n\n\n\nVisualizing samples\n\nboxplot(score ~ time, data = df,\n        col = c(\"#003C67FF\", \"#EFC000FF\"),\n        main = \"ICT training score improves knowlege\",\n        xlab = \"Time\", ylab = \"Score\")\n\n\n\n\n\n\nRunning Paired sample t-test in R\n\nt.test(formula = score ~ time, data = df,\n       alternative = \"greater\",\n       mu = 0, \n       paired = TRUE,   \n       var.equal = TRUE,\n       conf.level = 0.95)\n\n\n    Paired t-test\n\ndata:  score by time\nt = 2.272, df = 9, p-value = 0.0246\nalternative hypothesis: true mean difference is greater than 0\n95 percent confidence interval:\n 0.1043169       Inf\nsample estimates:\nmean difference \n           0.54 \n\n# mu argument indicate the true value of difference in means for a two sample test. \n# TRUE for paired argument as this is paired sample data and each subject is measured twice before and after the ICT training. \n\n\nThe results showed that the probability value is lower than 0.05.\nLower the p-value, lower the evidence we have to support the null hypothesis.\nBased on this result, we shall reject the null hypothesis of no difference. It means ICT training significantly improved the participants’ knowledge."
  },
  {
    "objectID": "04TTest.html#assumptions-behind-one-sample-t-test",
    "href": "04TTest.html#assumptions-behind-one-sample-t-test",
    "title": "T-tests",
    "section": "Assumptions behind One-Sample t-test",
    "text": "Assumptions behind One-Sample t-test\nRecall that the one-sample t-test is a statistical test used to determine if the mean of a sample is significantly different from a known or hypothesized population mean. There are several assumptions that must be met before conducting a one-sample t-test:\n\nRandom sampling: The sample should be randomly selected from the population of interest. This assumption ensures that the sample is representative of the population.\nNormality: The data should follow a normal distribution. This assumption is important because the t-test assumes that the distribution of the means of the sample is normal.\nIndependence: The observations within the sample should be independent of each other. In other words, the value of one observation should not depend on or influence the value of another observation in the same sample.\nScale of measurement: The data should be measured on an interval or ratio scale. This assumption ensures that the data has equal intervals between values.\n\nIf these assumptions are not met, the results of the one-sample t-test may not be valid or reliable. It is important to check these assumptions before conducting a one-sample t-test and to use alternative tests if the assumptions are not met."
  },
  {
    "objectID": "04TTest.html#assumptions-behind-the-independent-samples-t-test",
    "href": "04TTest.html#assumptions-behind-the-independent-samples-t-test",
    "title": "T-tests",
    "section": "Assumptions behind the Independent samples t-test",
    "text": "Assumptions behind the Independent samples t-test\nRemember that the one-sample t-test is a statistical analysis used to assess whether a sample’s mean differs significantly from an actual or predicted population mean. Before doing a one-sample t-test, a number of conditions must be satisfied:\n\nIndependence: Each group’s observations must be separate from the others. In other words, the value of one observation in a group shouldn’t be influenced or dependent on the value of another observation.\nNormality: Each group’s data should have a normal distribution. This presumption is crucial since the t-test assumes a normal distribution for the distribution of the mean differences between the two groups.\nHomogeneity of variance: Each group’s data variance ought to be equal. Because the t-test implies that the variances of the two groups are identical, this presumption is crucial.\nScale of measurement: The data should be measured on an interval or ratio scale. This assumption ensures that the data has equal intervals between values.\n\nThe findings of the independent samples t-test might not be valid or accurate if these suppositions are not true. When performing an independent samples t-test, it is crucial to confirm these hypotheses. If necessary, other tests should be used. Non-parametric tests like the Mann-Whitney U test can be utilised if the normality assumption is violated. The standard independent samples t-test can be substituted with a Welch’s t-test if the homogeneity of variance assumption is not met."
  },
  {
    "objectID": "04TTest.html#assumptions-behind-the-dependent-samples-t-test",
    "href": "04TTest.html#assumptions-behind-the-dependent-samples-t-test",
    "title": "T-tests",
    "section": "Assumptions behind the Dependent samples t-test",
    "text": "Assumptions behind the Dependent samples t-test\nRemember that a paired samples t-test, sometimes referred to as a dependent samples t-test, is a statistical test used to compare the means of two related samples. The observations in one sample are paired with the observations in the other sample, so the samples are thought to be connected. The samples could, for instance, be measurements taken on the same group of people before and after a therapy. A dependent samples t-test is conducted with the following presumptions:\n\nIndependence: The differences between the pairs of observations should be independent of each other. In other words, the value of one difference should not depend on or influence the value of another difference.\nNormality: The differences between the paired observations should follow a normal distribution. This assumption is important because the t-test assumes that the distribution of the mean differences is normal.\nScale of measurement: The differences between the paired observations should be measured on an interval or ratio scale. This assumption ensures that the data has equal intervals between values.\n\nThe findings of the dependent samples t-test might not be valid or accurate if these suppositions are not true. When doing a dependent samples t-test, it is crucial to confirm these hypotheses. If necessary, other tests should be used. Non-parametric tests, such as the Wilcoxon signed-rank test, can be utilised if the normality assumption is violated."
  },
  {
    "objectID": "04TTest.html#checking-t-test-assumptions",
    "href": "04TTest.html#checking-t-test-assumptions",
    "title": "T-tests",
    "section": "Checking t-test Assumptions",
    "text": "Checking t-test Assumptions\nAssumption of Normality: We will check whether the dependent variable wt is normally distributed or not.\n\nQ-Q Plot for Normality\n\nqqnorm(mtcars$wt, pch = 1, frame = FALSE)\nqqline(mtcars$wt, col = \"steelblue\", lwd = 2)\n\n\n\n\nWe can see that all the data points are not on the 45 degree line, hence we assume that the distribuion of weight is not normal.\n\n\nShapiro-Wilk Test for Normality\nHere, the sample size must be between 3 and 5000)\nH0: mtcars$wt is normally distributed\n\n# run Shapiro-Wilk test\nshapiro.test(x = mtcars$wt)\n\n\n    Shapiro-Wilk normality test\n\ndata:  mtcars$wt\nW = 0.94326, p-value = 0.09265\n\n\n\n\nTwo-sample Kolmogorov-Smirnov test for Normality\nH0: mtcars$wt is normally distributed\n\nx &lt;- rnorm(32)\nks.test(x, mtcars$wt)\n\n\n    Exact two-sample Kolmogorov-Smirnov test\n\ndata:  x and mtcars$wt\nD = 0.9375, p-value = 2.2e-15\nalternative hypothesis: two-sided\n\n\n\n\nAssumption of Equal Variance\nWe will check whether the variances across the two groups are same or not.\n\n# comparing variances \n# H0: The variances of the two samples wt~am are not different\nvar.test(wt~am)\n\n\n    F test to compare two variances\n\ndata:  wt by am\nF = 1.5876, num df = 18, denom df = 12, p-value = 0.4177\nalternative hypothesis: true ratio of variances is not equal to 1\n95 percent confidence interval:\n 0.5107978 4.3959133\nsample estimates:\nratio of variances \n          1.587613 \n\n\nFrom the output above we can see that the p-value is not less than the significance level of 0.05. This means that there is no evidence to suggest that the variances are statistically significantly different. Therefore, we can assume the homogeneity of variances in the two groups (am = 1 & am = 0).\n\n\nIf the Assumption of Equal Variance is Violated?\nWe will use Welch Two Sample t-test. We can use this test by simply writting (var.equal = FALSE) using t.test() in R\n\n# Computing t-test,when assumption of equal variance is violated\ntst &lt;- t.test(wt~am, var.equal = FALSE)\ntst\n\n\n    Welch Two Sample t-test\n\ndata:  wt by am\nt = 5.4939, df = 29.234, p-value = 6.272e-06\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n 0.8525632 1.8632262\nsample estimates:\nmean in group 0 mean in group 1 \n       3.768895        2.411000"
  },
  {
    "objectID": "04TTest.html#one-sample-wilcoxon-signed-rank-test",
    "href": "04TTest.html#one-sample-wilcoxon-signed-rank-test",
    "title": "T-tests",
    "section": "One-Sample Wilcoxon Signed Rank Test",
    "text": "One-Sample Wilcoxon Signed Rank Test\n\nThe one-sample Wilcoxon signed rank test is a non-parametric statistical test alternative to one-sample t-test, when the data cannot be assumed to be normally distributed.\nIt is used to determine if the median of a single sample of paired observations is different from a hypothesized value.\nThe test works by first calculating the differences between each pair of observations and taking the absolute values of these differences.\nThe ranks of these absolute differences are then calculated, and the sign of each difference is retained.\nThe sum of the ranks of the positive differences and the sum of the ranks of the negative differences are then calculated, and the smaller of the two sums is used as the test statistic.\nHypothesis Testing\n\n\nThe null hypothesis of the test is that the median of the paired differences is equal to the hypothesized value.\nThe alternative hypothesis is that the median of the paired differences is not equal to the hypothesized value.\n\n\nThe significance of the test is determined by comparing the calculated value of the test statistic to a critical value from a table or by calculating a p-value using the normal approximation or exact method.\n\n\nIf the p-value is less than the chosen significance level (usually 0.05), then the null hypothesis is rejected, and it is concluded that the median of the paired differences is different from the hypothesized value.\nIf the p-value is greater than the chosen significance level, then there is insufficient evidence to reject the null hypothesis.\n\n\nExample: Compare whether the median weight of cars the differ from 3 (1000 lbs), a value determined in a previous study.\n\nH0: The median weight of the cars is not different from theoretical median 3 (1000 lbs).\nH1: The median weight of the cars is different from theoretical median 3 (1000 lbs).\n\n# One-sample wilcoxon test\nwil &lt;- wilcox.test(wt, mu = 3)\n# Printing the results\nwil \n\n\nThe p-value of the test is 0.3081, which is greater than the significance level alpha = 0.05.\nWe fail to reject the null hypothesis.\nWe can conclude that the median weight of the cars is not different from theoretical median 3 (1000 lbs)"
  },
  {
    "objectID": "04TTest.html#two-sample-wilcoxon-test-a.k.a-mann-whitney-u-test",
    "href": "04TTest.html#two-sample-wilcoxon-test-a.k.a-mann-whitney-u-test",
    "title": "T-tests",
    "section": "Two-Sample Wilcoxon test (a.k.a Mann-Whitney U test)",
    "text": "Two-Sample Wilcoxon test (a.k.a Mann-Whitney U test)\n\nThe two-sample Wilcoxon test, also known as the Wilcoxon rank-sum test or Mann-Whitney U test, is a non-parametric statistical test used to compare the distributions of two independent samples.\nIt is often used as an alternative to the two-sample t-test when the data does not meet the assumption of normality.\nThe test works by combining the two samples and ranking all the observations from lowest to highest. The ranks of the observations in each sample are then calculated, and the test statistic U is calculated as the smaller of the two sums of the ranks.\nHypothesis Testing\n\n\nThe null hypothesis of the test is that the two samples come from the same distribution, while the alternative hypothesis is that they come from different distributions.\n\n\nThe significance of the test is determined by comparing the calculated value of U to a critical value from a table or by calculating a p-value using the normal approximation or exact method.\n\n\nIf the p-value is less than the chosen significance level (usually 0.05), then the null hypothesis is rejected, and it is concluded that the two samples come from different distributions.\nIf the p-value is greater than the chosen significance level, then there is insufficient evidence to reject the null hypothesis.\n\n\nExample: Compare whether the median weight of the cars having am = 0 is significantly different from median weight of the cars having am = 1.\n\n\nH0: The median weight of the cars having am = 0 is not significantly different from median weight of the cars having am = 1\nH1: The median weight of the cars having am = 0 is significantly different from median weight of the cars having am = 1\n\n\n# Compute unpaired two-samples Wilcoxon test\ntwil &lt;- wilcox.test(wt~am)\n\nWarning in wilcox.test.default(x = DATA[[1L]], y = DATA[[2L]], ...): cannot\ncompute exact p-value with ties\n\ntwil\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  wt by am\nW = 230.5, p-value = 4.347e-05\nalternative hypothesis: true location shift is not equal to 0\n\n\n\nThe p-value of the test is 4.347e-05, which is less than the significance level alpha = 0.05.\nWe can reject the null hypothesis,and conclude that median weight of the cars having am = 0 is significantly different from median weight of the cars having am = 1"
  },
  {
    "objectID": "03ChiSquareTests.html",
    "href": "03ChiSquareTests.html",
    "title": "Chi-Square Tests",
    "section": "",
    "text": "January 22, 2024.\n\nThe chi-square test is a statistical test used to determine if there is a significant difference between the observed values and the expected values in a categorical data set.\nIt measures the deviation between the expected and observed frequencies in one or more categories and assesses whether this deviation is statistically significant.\nThe result of a chi-square test is a test statistic, and its p-value is compared against a threshold (e.g. \\(\\alpha = 0.05\\)) to determine the statistical significance of the observed differences.\nThe test is commonly used in hypothesis testing, contingency table analysis, and goodness-of-fit testing.\n\n\n\n\nThere are several types of chi-square tests. The most popular tests are as follows.\n\n\n\n\n\n\n\n\nTest\nUse\n\n\n\n\nA. Goodness-of-fit test\ndetermine if a sample of data fits a specified distribution\n\n\nB. Independence test\ndetermine if there is a relationship between two categorical variables\n\n\n\n\nAdditional chi-square tests include:\n\n\nHomogeneity test: used to determine if different populations have the same distribution of a categorical variable.\nContingency table test: used to analyze the relationship between two or more categorical variables in a multi-dimensional table.\nMcNemar’s test: used to determine if the difference between paired nominal data is significant.\nLikelihood-ratio test: used to compare nested models, where the more complex model is tested against a simpler model.\nMantel-Haenszel test: used to determine if there is a relationship between two categorical variables while controlling for the effect of a third variable.\n\n\n\n\n\n\n\nSuppose a retail company wants to know if the gender distribution of their customer base is representative of the general population. They collect data on the gender of a sample of their customers and compare it to the expected distribution (e.g. 50% male and 50% female).\nThe Chi-Square Goodness of Fit test is then used to evaluate the null hypothesis that the observed distribution of gender among the company’s customers is the same as the expected distribution.\nThe test could help us potentially conclude that there is a significant difference between the observed and expected distributions, and the company’s customer base may not be representative of the general population.\nFor example, the results might show that a significantly higher proportion of females than expected are customers of the company. This information could be used by the retail company to tailor their marketing strategies and product offerings to better attract male customers.\n\n\n\n\nThe Chi-Square Goodness-of-Fit test is a statistical tool used to determine if the observed data in a categorical dataset aligns with the expected data based on a particular hypothesis or anticipated distribution. It helps researchers evaluate if there exists a noteworthy difference between the actual data and what would be anticipated under a specific theoretical model or hypothesis.\nHere are the essential steps and concepts involved in carrying out a Chi-Square Goodness-of-Fit test:\n\nHypotheses:\n\nNull Hypothesis (H0): The observed data conforms to a specific theoretical distribution.\nAlternative Hypothesis (Ha): The observed data does not conform to the specified theoretical distribution.\n\nCategorical Data:\n\nThe data should be categorical, divided into distinct categories or groups.\nWe need both observed (actual) and expected frequencies for each category.\n\nExpected Frequencies:\n\nCalculate the expected frequencies for each category based on the null hypothesis or the theoretical distribution. These expected values represent what we would anticipate seeing if the null hypothesis were accurate.\nExpected frequencies can be computed using a formula or by multiplying the total sample size by the probabilities associated with each category according to the theoretical distribution.\n\nChi-Square Statistic:\n\nDetermine the Chi-Square statistic using this formula:\nχ² = Σ ((O - E)² / E)\n\nχ²: Chi-Square statistic\nO: Observed frequency for a category\nE: Expected frequency for the same category\nΣ: The sum symbol, signifying that we should perform this calculation for all categories\n\n\nDegrees of Freedom (df):\n\nThe degrees of freedom for a Chi-Square Goodness-of-Fit test is equal to the number of categories minus one, or df = (number of categories - 1).\n\nChi-Square Distribution:\n\nThe Chi-Square statistic follows a specific distribution known as the Chi-Square distribution, with df degrees of freedom.\n\nCritical Value and Significance Level:\n\nIdentify the critical value for the Chi-Square test at a chosen significance level (usually denoted as alpha). The critical value is typically derived from a Chi-Square distribution table or statistical software.\nCommon significance levels include 0.05 or 0.01.\n\nStatistic vs. Critical Value Comparison:\n\nIf the calculated Chi-Square statistic exceeds the critical value, we reject the null hypothesis.\nIf the calculated Chi-Square statistic is less than or equal to the critical value, we do not reject the null hypothesis.\n\nInterpretation:\n\nRejecting the null hypothesis implies a significant discrepancy between the observed data and the expected distribution, suggesting that the data does not adhere to the specified theoretical model.\nFailing to reject the null hypothesis suggests no significant difference, indicating that the observed data aligns with the expected distribution.\n\n\nIn summary, the Chi-Square Goodness-of-Fit test serves to evaluate whether categorical data conforms to a particular theoretical distribution. It helps us assess whether any deviations from the expected distribution are statistically meaningful or simply due to chance.\n\n\n\nBusiness Scenario\nWe work for an e-commerce company that sells electronic gadgets. Our goal is to determine if the distribution of the company’s most popular product categories among customers aligns with the expected distribution based on last year’s market research. The expected distribution percentages are as follows:\n\nSmartphones: 30%\nLaptops: 40%\nAccessories: 20%\nWearables: 10%\n\nWe aim to test if the observed distribution of product categories sold in a recent month matches this expected distribution.\nSimulated Observed Data\nThe distribution of product categories sold in the recent month was observed as:\n\nSmartphones: 25\nLaptops: 42\nAccessories: 18\nWearables: 15\n\nChi-Square Goodness-of-Fit Test\nStep 1: Set up Hypotheses\n\nNull Hypothesis (H0): The observed distribution matches the expected distribution.\nAlternative Hypothesis (Ha): The observed distribution does not match the expected distribution.\n\nStep 2: Organize Data\n\nExpected Distribution:\n\nSmartphones: 30%\nLaptops: 40%\nAccessories: 20%\nWearables: 10%\n\nObserved Distribution:\n\nSmartphones: 25\nLaptops: 42\nAccessories: 18\nWearables: 15\n\n\nStep 3: Calculate Expected Frequencies\n\nFirst, calculate the total number of products sold, which is the sum of the observed sales for all categories: 100.\nThen, calculate the expected frequencies:\n\nExpected Smartphones: 30 (100 * 30%)\nExpected Laptops: 40 (100 * 40%)\nExpected Accessories: 20 (100 * 20%)\nExpected Wearables: 10 (100 * 10%)\n\n\nStep 4: Calculate the Chi-Square Statistic\n\nUse the formula: χ² = Σ ((O - E)² / E) for each category and then sum them up.\nχ² = [(25 - 30)² / 30] + [(42 - 40)² / 40] + [(18 - 20)² / 20] + [(15 - 10)² / 10]\nχ² = (25 / 30) + (4 / 40) + (4 / 20) + (25 / 10)\nχ² ≈ 3.63 (rounded to two decimal places)\n\nStep 5: Determine Degrees of Freedom (df)\n\ndf = (Number of Categories - 1) = 4 - 1 = 3\n\nStep 6: Set Significance Level\n\nChoose a significance level (alpha), e.g., 0.05.\n\nStep 7: Find Critical Value\n\nUsing a Chi-Square distribution table or calculator for df = 3 and α = 0.05, the critical value is approximately 7.815.\n\nCritical Value: This is a point on the scale of the test statistic beyond which we reject the null hypothesis, and it’s dependent on the chosen significance level (alpha, α) and the degrees of freedom in the test. In this case, χ² (critical) ≈ 7.815 is the critical value for a Chi-Square test with a significance level of 0.05 (5%) and degrees of freedom (df) equal to 3. This critical value is derived from the Chi-Square distribution table, which provides critical values for different levels of α and df.\nStep 8: Compare Statistic to Critical Value\n\nχ² (calculated) ≈ 3.63 &lt; χ² (critical) ≈ 7.815\n\nStep 9: Make a Decision\n\nSince the calculated Chi-Square statistic (3.63) is less than the critical value (7.815), we fail to reject the null hypothesis.\n\nStep 10: Interpretation\n\nWe conclude that there is no significant difference between the observed distribution of product categories sold and the expected distribution based on market research. In other words, the product categories sold match the expected distribution.\n\n\n\n\nWe present another example of running this test, this time using the mtcars data in R.\nThe primary goal is to use statistical methods to assess whether the actual distribution of gears in cars (as observed in the mtcars dataset) matches a set of expected probabilities, thereby validating or refuting assumptions about gear distribution in the dataset.\nThe set of expected probabilities for each gear category is as follows:\n\n0.5 or 50% for cars with 3 gears\n0.3 or 30% for cars with 4 gears\n0.2 or 20% for cars with 5 gears\n\nThis test is used to determine if there is a significant difference between observed frequencies (in this case, the number of cars with different gears) and expected frequencies.\n1. Data Preparation\nWe start with the mtcars dataset, which contains information about 32 cars, each categorized with 3, 4, or 5 gears.\n\n# Load the mtcars dataset\ndata(mtcars)\n\n# Create a table to count the number of cars in each gear category\ngear_counts &lt;- table(mtcars$gear)\ngear_counts\n\n\n 3  4  5 \n15 12  5 \n\n\n2. Visualization of Contingency Table\nWe visualize the contingency table using a graphical balloon plot:\n\n# Load the gplots library for graphical plotting\nlibrary(\"gplots\")\n\n\nAttaching package: 'gplots'\n\n\nThe following object is masked from 'package:stats':\n\n    lowess\n\n# Convert gear counts into a table format\ngear_table &lt;- as.table(as.matrix(gear_counts))\n\n# Create a graphical balloon plot\nballoonplot(t(gear_table), main=\"Contingency Table\",\n            xlab=\"Gear\",\n            label=FALSE, show.margins=FALSE)\n\n\n\n\n3. Calculation of Proportions\nWe calculate the proportions of cars in each gear category:\n\n# Calculate proportions of cars in each gear category\ngear_proportions &lt;- prop.table(gear_counts)\ngear_proportions\n\n\n      3       4       5 \n0.46875 0.37500 0.15625 \n\n\n4. Running the Chi-Square Goodness-of-Fit Test\nWe specify the expected probabilities for each gear category and perform the chi-square goodness-of-fit test:\n\n# Specify the expected probabilities for each gear category\nexpected_probs &lt;- c(0.5, \n                    0.3, \n                    0.2)\n\n# Perform the chi-square goodness-of-fit test using observed proportions and expected probabilities\nchisq_test_result &lt;- chisq.test(gear_proportions, \n                                p=expected_probs)\nchisq_test_result\n\n\n    Chi-squared test for given probabilities\n\ndata:  gear_proportions\nX-squared = 0.030273, df = 2, p-value = 0.985\n\n\n5. Interpretation\nAfter running the chi-square goodness-of-fit test using the provided expected probabilities, the output provides important information:\n\nChi-squared test for given probabilities: This line indicates that the test conducted is specifically a chi-square goodness-of-fit test. It’s designed to assess whether the observed data fits the expected probabilities we’ve provided.\ndata: gear_proportions: This line specifies the dataset used for the test, which is the gear_proportions dataset. This dataset contains the observed proportions of cars in different gear categories.\nX-squared = 0.030273: This value represents the chi-square statistic (X²), which is a measure of how closely the observed proportions align with the expected probabilities. In this case, the calculated chi-square statistic is approximately 0.030273.\ndf = 2: This indicates the degrees of freedom (df) associated with the chi-square distribution. In a goodness-of-fit test like this one, the df is calculated as one less than the number of categories being analyzed. Here, since there are three gear categories (3, 4, and 5), df equals 2.\np-value = 0.985: The p-value is a crucial result of the test. It represents the probability of observing a chi-square statistic as extreme as the calculated value (0.030273) under the assumption that there is no significant difference between the observed and expected proportions. In this case, the high p-value of 0.985 suggests that the observed data aligns well with the expected probabilities. A high p-value implies that there is no strong evidence to reject the null hypothesis, which means that the data does not provide significant grounds to believe that the observed proportions are different from what was expected.\n\nIn summary, this output tells us that the chi-square goodness-of-fit test was conducted using the observed proportions of cars in different gear categories and the specified expected probabilities. The resulting high p-value (0.985) indicates that there is no strong reason to conclude that the observed proportions significantly differ from the expected probabilities, thus failing to reject the null hypothesis.\n\n\n\nThe Chi-Square Goodness of Fit test is employed to assess whether observed data conforms to an expected distribution, particularly when dealing with categorical data. It evaluates the similarity between observed and expected frequencies across different categories, typically through a chi-square statistic and follows a chi-square distribution.\nIt is used when you have categories or groups, and it helps determine if the observed data matches an expected distribution within those categories. It’s like checking if things are divided as you’d expect.\nIn contrast, the Z-test for Proportions focuses on comparing a sample proportion with a known population proportion, typically in binary data scenarios. It uses a Z-score to measure the difference and follows a standard normal distribution.\nIt helps us assess if the proportion of “yes” responses in our sample significantly differs from a known or expected proportion. It’s like comparing one group to a known standard.\n\n\n\n\n\n\n\nSuppose a grocery store wants to evaluate the association between the type of product a customer buys and their age group.\nSuppose the grocery store wants to know if there is a significant association between the type of product a customer buys (e.g. fruits, vegetables, or dairy products) and their age group (e.g. 18-30, 31-45, 46-60, 61 and older). They collect data on the age group and product type for a sample of customers and create a contingency table.\nThe Chi-square test is then used to evaluate the null hypothesis that the product type and age group are independent.\nDepending on the test result, we could potentially conclude that there is a significant association between the two variables.\nFor example, the results might show that a significantly higher proportion of customers in the 46-60 age group buy fruits compared to the other age groups.\nThis information could be used by the grocery store to adjust their marketing strategies and product offerings to better cater to their target customers.\n\n\n\n\n\nThe Chi-square test of independence is a statistical test used to determine if there is a significant association between two categorical variables.\nThe test evaluates the null hypothesis that the two variables are independent and calculates a test statistic (chi-square) based on the difference between the observed and expected frequencies of the two variables.\nIf the calculated test statistic is larger than the critical value from a chi-square distribution table, then the null hypothesis is rejected and it can be concluded that there is a significant association between the two variables.\n\n\n\n\nThe overall objective of this test is to qualitatively and quantitatively analyze and understand the interrelationships between different categorical variables in the mtcars dataset.\nSpecifically, in this illustration, the test aims to explore the association between the number of cylinders (cyl) and transmission type (am), in the dataset.\n\nConvert the categorical variables into factor variables in the dataset mtcars.\n\nThis step ensures that R treats these variables as categorical data, which is necessary for a Chi-Square test.\n\ndata(mtcars)\nmtcars$cyl &lt;- as.factor(mtcars$cyl)\nmtcars$am  &lt;- as.factor(mtcars$am)\nmtcars$gear  &lt;- as.factor(mtcars$gear)\n\n\nCreating a Contingency Table:\n\n\nctab &lt;- table(mtcars$am, \n              mtcars$cyl)\nctab\n\n   \n     4  6  8\n  0  3  4 12\n  1  8  3  2\n\n\nA contingency table (ctab) is created to summarize the relationship between two categorical variables.\n\nGraphical display of Contingency table:\n\nThe balloonplot function from the gplots library is used to visually represent the contingency table.\n\nlibrary(\"gplots\")\n# 1. convert the data as a table\ndt &lt;- as.table(\n  as.matrix(ctab))\n\n# 2. Graph\nballoonplot(t(dt), \n            main =\"Contingency Table\",\n            xlab =\"cyl\", ylab=\"am\",\n            label = FALSE, \n            show.margins = FALSE)\n\n\n\n\n\nCompute Chi-Square test: The Chi-square statistic can be easily computed using the function chisq.test() as follow:\n\n\nchisq &lt;- chisq.test(ctab)\n\nWarning in chisq.test(ctab): Chi-squared approximation may be incorrect\n\nchisq\n\n\n    Pearson's Chi-squared test\n\ndata:  ctab\nX-squared = 8.7407, df = 2, p-value = 0.01265\n\n\n\nIn our example, the row and the column variables are statistically significantly associated (p-value = 0.01265). This indicates a statistically significant association between the variables at common alpha levels (like 0.05).\nThe observed and the expected counts can be extracted from the result of the test as follows:\n\n\n# Observed counts\nchisq$observed\n\n   \n     4  6  8\n  0  3  4 12\n  1  8  3  2\n\n# Expected counts\nround(chisq$expected, 2)\n\n   \n       4    6    8\n  0 6.53 4.16 8.31\n  1 4.47 2.84 5.69\n\n\n\nPearson Residuals:\n\nPositive residuals are positive values in cells specify an attraction (positive association) between the corresponding row and column variables.\nNegative residuals implies a repulsion (negative association) between the corresponding row and column variables.\n\nround(chisq$residuals, 3)\n\n   \n         4      6      8\n  0 -1.382 -0.077  1.279\n  1  1.670  0.093 -1.546\n\n\nThe overall qualitative objective of running the Chi-Square Test of Independence, as described in the example using the mtcars dataset in R, is to determine whether there is a statistically significant relationship between different categorical variables in the dataset. Specifically, the test aims to explore the association between variables like the number of cylinders (cyl), transmission type (am), in the dataset. The key goals and insights sought from this test include:\n\nUnderstanding Relationships Between Variables: To identify if there is a statistically significant association between different categorical variables in the dataset. For instance, it might reveal whether the type of transmission (automatic or manual) is associated with the number of cylinders or gears in the cars.\nTesting Hypotheses About Associations: The Chi-Square Test of Independence is essentially a hypothesis test. The null hypothesis states that there is no association between the variables (they are independent), while the alternative hypothesis suggests there is an association (they are not independent).\nGuiding Data-Driven Decisions: By understanding the relationships between variables, businesses, analysts, or data scientists can make informed decisions. For example, if a significant association is found between certain car features, it might influence marketing strategies or product development.\nVisualizing Data Relationships: Through graphical representation like balloon plots, the test provides a visual understanding of the strength and nature of the relationships between categorical variables.\nQuantitative Analysis of Associations: Beyond just identifying if an association exists, the test provides measures like the chi-square statistic, p-value, observed and expected counts, and Pearson residuals. These measures quantify the strength of association and help in understanding the nature of the relationship (whether it’s positive or negative).\n\nThe overall objective of this test is to qualitatively and quantitatively analyze and understand the interrelationships between different categorical variables in the mtcars dataset.\n\n\n\n\n[1] Agresti, A. (2002). Categorical data analysis (2nd ed.). Wiley.\nHair Jr, J. F., Black, W. C., Babin, B. J., & Anderson, R. E. (2010). Multivariate data analysis: A global perspective (7th ed.). Pearson Education.\nEveritt, B. S. (1992). The analysis of contingency tables (2nd ed.). Chapman and Hall."
  },
  {
    "objectID": "03ChiSquareTests.html#types-of-tests",
    "href": "03ChiSquareTests.html#types-of-tests",
    "title": "Chi-Square Tests",
    "section": "",
    "text": "There are several types of chi-square tests. The most popular tests are as follows.\n\n\n\n\n\n\n\n\nTest\nUse\n\n\n\n\nA. Goodness-of-fit test\ndetermine if a sample of data fits a specified distribution\n\n\nB. Independence test\ndetermine if there is a relationship between two categorical variables\n\n\n\n\nAdditional chi-square tests include:\n\n\nHomogeneity test: used to determine if different populations have the same distribution of a categorical variable.\nContingency table test: used to analyze the relationship between two or more categorical variables in a multi-dimensional table.\nMcNemar’s test: used to determine if the difference between paired nominal data is significant.\nLikelihood-ratio test: used to compare nested models, where the more complex model is tested against a simpler model.\nMantel-Haenszel test: used to determine if there is a relationship between two categorical variables while controlling for the effect of a third variable."
  },
  {
    "objectID": "03ChiSquareTests.html#chi-square-goodness-of-fit-test",
    "href": "03ChiSquareTests.html#chi-square-goodness-of-fit-test",
    "title": "Chi-Square Tests",
    "section": "",
    "text": "Suppose a retail company wants to know if the gender distribution of their customer base is representative of the general population. They collect data on the gender of a sample of their customers and compare it to the expected distribution (e.g. 50% male and 50% female).\nThe Chi-Square Goodness of Fit test is then used to evaluate the null hypothesis that the observed distribution of gender among the company’s customers is the same as the expected distribution.\nThe test could help us potentially conclude that there is a significant difference between the observed and expected distributions, and the company’s customer base may not be representative of the general population.\nFor example, the results might show that a significantly higher proportion of females than expected are customers of the company. This information could be used by the retail company to tailor their marketing strategies and product offerings to better attract male customers.\n\n\n\n\nThe Chi-Square Goodness-of-Fit test is a statistical tool used to determine if the observed data in a categorical dataset aligns with the expected data based on a particular hypothesis or anticipated distribution. It helps researchers evaluate if there exists a noteworthy difference between the actual data and what would be anticipated under a specific theoretical model or hypothesis.\nHere are the essential steps and concepts involved in carrying out a Chi-Square Goodness-of-Fit test:\n\nHypotheses:\n\nNull Hypothesis (H0): The observed data conforms to a specific theoretical distribution.\nAlternative Hypothesis (Ha): The observed data does not conform to the specified theoretical distribution.\n\nCategorical Data:\n\nThe data should be categorical, divided into distinct categories or groups.\nWe need both observed (actual) and expected frequencies for each category.\n\nExpected Frequencies:\n\nCalculate the expected frequencies for each category based on the null hypothesis or the theoretical distribution. These expected values represent what we would anticipate seeing if the null hypothesis were accurate.\nExpected frequencies can be computed using a formula or by multiplying the total sample size by the probabilities associated with each category according to the theoretical distribution.\n\nChi-Square Statistic:\n\nDetermine the Chi-Square statistic using this formula:\nχ² = Σ ((O - E)² / E)\n\nχ²: Chi-Square statistic\nO: Observed frequency for a category\nE: Expected frequency for the same category\nΣ: The sum symbol, signifying that we should perform this calculation for all categories\n\n\nDegrees of Freedom (df):\n\nThe degrees of freedom for a Chi-Square Goodness-of-Fit test is equal to the number of categories minus one, or df = (number of categories - 1).\n\nChi-Square Distribution:\n\nThe Chi-Square statistic follows a specific distribution known as the Chi-Square distribution, with df degrees of freedom.\n\nCritical Value and Significance Level:\n\nIdentify the critical value for the Chi-Square test at a chosen significance level (usually denoted as alpha). The critical value is typically derived from a Chi-Square distribution table or statistical software.\nCommon significance levels include 0.05 or 0.01.\n\nStatistic vs. Critical Value Comparison:\n\nIf the calculated Chi-Square statistic exceeds the critical value, we reject the null hypothesis.\nIf the calculated Chi-Square statistic is less than or equal to the critical value, we do not reject the null hypothesis.\n\nInterpretation:\n\nRejecting the null hypothesis implies a significant discrepancy between the observed data and the expected distribution, suggesting that the data does not adhere to the specified theoretical model.\nFailing to reject the null hypothesis suggests no significant difference, indicating that the observed data aligns with the expected distribution.\n\n\nIn summary, the Chi-Square Goodness-of-Fit test serves to evaluate whether categorical data conforms to a particular theoretical distribution. It helps us assess whether any deviations from the expected distribution are statistically meaningful or simply due to chance.\n\n\n\nBusiness Scenario\nWe work for an e-commerce company that sells electronic gadgets. Our goal is to determine if the distribution of the company’s most popular product categories among customers aligns with the expected distribution based on last year’s market research. The expected distribution percentages are as follows:\n\nSmartphones: 30%\nLaptops: 40%\nAccessories: 20%\nWearables: 10%\n\nWe aim to test if the observed distribution of product categories sold in a recent month matches this expected distribution.\nSimulated Observed Data\nThe distribution of product categories sold in the recent month was observed as:\n\nSmartphones: 25\nLaptops: 42\nAccessories: 18\nWearables: 15\n\nChi-Square Goodness-of-Fit Test\nStep 1: Set up Hypotheses\n\nNull Hypothesis (H0): The observed distribution matches the expected distribution.\nAlternative Hypothesis (Ha): The observed distribution does not match the expected distribution.\n\nStep 2: Organize Data\n\nExpected Distribution:\n\nSmartphones: 30%\nLaptops: 40%\nAccessories: 20%\nWearables: 10%\n\nObserved Distribution:\n\nSmartphones: 25\nLaptops: 42\nAccessories: 18\nWearables: 15\n\n\nStep 3: Calculate Expected Frequencies\n\nFirst, calculate the total number of products sold, which is the sum of the observed sales for all categories: 100.\nThen, calculate the expected frequencies:\n\nExpected Smartphones: 30 (100 * 30%)\nExpected Laptops: 40 (100 * 40%)\nExpected Accessories: 20 (100 * 20%)\nExpected Wearables: 10 (100 * 10%)\n\n\nStep 4: Calculate the Chi-Square Statistic\n\nUse the formula: χ² = Σ ((O - E)² / E) for each category and then sum them up.\nχ² = [(25 - 30)² / 30] + [(42 - 40)² / 40] + [(18 - 20)² / 20] + [(15 - 10)² / 10]\nχ² = (25 / 30) + (4 / 40) + (4 / 20) + (25 / 10)\nχ² ≈ 3.63 (rounded to two decimal places)\n\nStep 5: Determine Degrees of Freedom (df)\n\ndf = (Number of Categories - 1) = 4 - 1 = 3\n\nStep 6: Set Significance Level\n\nChoose a significance level (alpha), e.g., 0.05.\n\nStep 7: Find Critical Value\n\nUsing a Chi-Square distribution table or calculator for df = 3 and α = 0.05, the critical value is approximately 7.815.\n\nCritical Value: This is a point on the scale of the test statistic beyond which we reject the null hypothesis, and it’s dependent on the chosen significance level (alpha, α) and the degrees of freedom in the test. In this case, χ² (critical) ≈ 7.815 is the critical value for a Chi-Square test with a significance level of 0.05 (5%) and degrees of freedom (df) equal to 3. This critical value is derived from the Chi-Square distribution table, which provides critical values for different levels of α and df.\nStep 8: Compare Statistic to Critical Value\n\nχ² (calculated) ≈ 3.63 &lt; χ² (critical) ≈ 7.815\n\nStep 9: Make a Decision\n\nSince the calculated Chi-Square statistic (3.63) is less than the critical value (7.815), we fail to reject the null hypothesis.\n\nStep 10: Interpretation\n\nWe conclude that there is no significant difference between the observed distribution of product categories sold and the expected distribution based on market research. In other words, the product categories sold match the expected distribution.\n\n\n\n\nWe present another example of running this test, this time using the mtcars data in R.\nThe primary goal is to use statistical methods to assess whether the actual distribution of gears in cars (as observed in the mtcars dataset) matches a set of expected probabilities, thereby validating or refuting assumptions about gear distribution in the dataset.\nThe set of expected probabilities for each gear category is as follows:\n\n0.5 or 50% for cars with 3 gears\n0.3 or 30% for cars with 4 gears\n0.2 or 20% for cars with 5 gears\n\nThis test is used to determine if there is a significant difference between observed frequencies (in this case, the number of cars with different gears) and expected frequencies.\n1. Data Preparation\nWe start with the mtcars dataset, which contains information about 32 cars, each categorized with 3, 4, or 5 gears.\n\n# Load the mtcars dataset\ndata(mtcars)\n\n# Create a table to count the number of cars in each gear category\ngear_counts &lt;- table(mtcars$gear)\ngear_counts\n\n\n 3  4  5 \n15 12  5 \n\n\n2. Visualization of Contingency Table\nWe visualize the contingency table using a graphical balloon plot:\n\n# Load the gplots library for graphical plotting\nlibrary(\"gplots\")\n\n\nAttaching package: 'gplots'\n\n\nThe following object is masked from 'package:stats':\n\n    lowess\n\n# Convert gear counts into a table format\ngear_table &lt;- as.table(as.matrix(gear_counts))\n\n# Create a graphical balloon plot\nballoonplot(t(gear_table), main=\"Contingency Table\",\n            xlab=\"Gear\",\n            label=FALSE, show.margins=FALSE)\n\n\n\n\n3. Calculation of Proportions\nWe calculate the proportions of cars in each gear category:\n\n# Calculate proportions of cars in each gear category\ngear_proportions &lt;- prop.table(gear_counts)\ngear_proportions\n\n\n      3       4       5 \n0.46875 0.37500 0.15625 \n\n\n4. Running the Chi-Square Goodness-of-Fit Test\nWe specify the expected probabilities for each gear category and perform the chi-square goodness-of-fit test:\n\n# Specify the expected probabilities for each gear category\nexpected_probs &lt;- c(0.5, \n                    0.3, \n                    0.2)\n\n# Perform the chi-square goodness-of-fit test using observed proportions and expected probabilities\nchisq_test_result &lt;- chisq.test(gear_proportions, \n                                p=expected_probs)\nchisq_test_result\n\n\n    Chi-squared test for given probabilities\n\ndata:  gear_proportions\nX-squared = 0.030273, df = 2, p-value = 0.985\n\n\n5. Interpretation\nAfter running the chi-square goodness-of-fit test using the provided expected probabilities, the output provides important information:\n\nChi-squared test for given probabilities: This line indicates that the test conducted is specifically a chi-square goodness-of-fit test. It’s designed to assess whether the observed data fits the expected probabilities we’ve provided.\ndata: gear_proportions: This line specifies the dataset used for the test, which is the gear_proportions dataset. This dataset contains the observed proportions of cars in different gear categories.\nX-squared = 0.030273: This value represents the chi-square statistic (X²), which is a measure of how closely the observed proportions align with the expected probabilities. In this case, the calculated chi-square statistic is approximately 0.030273.\ndf = 2: This indicates the degrees of freedom (df) associated with the chi-square distribution. In a goodness-of-fit test like this one, the df is calculated as one less than the number of categories being analyzed. Here, since there are three gear categories (3, 4, and 5), df equals 2.\np-value = 0.985: The p-value is a crucial result of the test. It represents the probability of observing a chi-square statistic as extreme as the calculated value (0.030273) under the assumption that there is no significant difference between the observed and expected proportions. In this case, the high p-value of 0.985 suggests that the observed data aligns well with the expected probabilities. A high p-value implies that there is no strong evidence to reject the null hypothesis, which means that the data does not provide significant grounds to believe that the observed proportions are different from what was expected.\n\nIn summary, this output tells us that the chi-square goodness-of-fit test was conducted using the observed proportions of cars in different gear categories and the specified expected probabilities. The resulting high p-value (0.985) indicates that there is no strong reason to conclude that the observed proportions significantly differ from the expected probabilities, thus failing to reject the null hypothesis.\n\n\n\nThe Chi-Square Goodness of Fit test is employed to assess whether observed data conforms to an expected distribution, particularly when dealing with categorical data. It evaluates the similarity between observed and expected frequencies across different categories, typically through a chi-square statistic and follows a chi-square distribution.\nIt is used when you have categories or groups, and it helps determine if the observed data matches an expected distribution within those categories. It’s like checking if things are divided as you’d expect.\nIn contrast, the Z-test for Proportions focuses on comparing a sample proportion with a known population proportion, typically in binary data scenarios. It uses a Z-score to measure the difference and follows a standard normal distribution.\nIt helps us assess if the proportion of “yes” responses in our sample significantly differs from a known or expected proportion. It’s like comparing one group to a known standard."
  },
  {
    "objectID": "03ChiSquareTests.html#chi-square-test-of-independence",
    "href": "03ChiSquareTests.html#chi-square-test-of-independence",
    "title": "Chi-Square Tests",
    "section": "",
    "text": "Suppose a grocery store wants to evaluate the association between the type of product a customer buys and their age group.\nSuppose the grocery store wants to know if there is a significant association between the type of product a customer buys (e.g. fruits, vegetables, or dairy products) and their age group (e.g. 18-30, 31-45, 46-60, 61 and older). They collect data on the age group and product type for a sample of customers and create a contingency table.\nThe Chi-square test is then used to evaluate the null hypothesis that the product type and age group are independent.\nDepending on the test result, we could potentially conclude that there is a significant association between the two variables.\nFor example, the results might show that a significantly higher proportion of customers in the 46-60 age group buy fruits compared to the other age groups.\nThis information could be used by the grocery store to adjust their marketing strategies and product offerings to better cater to their target customers.\n\n\n\n\n\nThe Chi-square test of independence is a statistical test used to determine if there is a significant association between two categorical variables.\nThe test evaluates the null hypothesis that the two variables are independent and calculates a test statistic (chi-square) based on the difference between the observed and expected frequencies of the two variables.\nIf the calculated test statistic is larger than the critical value from a chi-square distribution table, then the null hypothesis is rejected and it can be concluded that there is a significant association between the two variables.\n\n\n\n\nThe overall objective of this test is to qualitatively and quantitatively analyze and understand the interrelationships between different categorical variables in the mtcars dataset.\nSpecifically, in this illustration, the test aims to explore the association between the number of cylinders (cyl) and transmission type (am), in the dataset.\n\nConvert the categorical variables into factor variables in the dataset mtcars.\n\nThis step ensures that R treats these variables as categorical data, which is necessary for a Chi-Square test.\n\ndata(mtcars)\nmtcars$cyl &lt;- as.factor(mtcars$cyl)\nmtcars$am  &lt;- as.factor(mtcars$am)\nmtcars$gear  &lt;- as.factor(mtcars$gear)\n\n\nCreating a Contingency Table:\n\n\nctab &lt;- table(mtcars$am, \n              mtcars$cyl)\nctab\n\n   \n     4  6  8\n  0  3  4 12\n  1  8  3  2\n\n\nA contingency table (ctab) is created to summarize the relationship between two categorical variables.\n\nGraphical display of Contingency table:\n\nThe balloonplot function from the gplots library is used to visually represent the contingency table.\n\nlibrary(\"gplots\")\n# 1. convert the data as a table\ndt &lt;- as.table(\n  as.matrix(ctab))\n\n# 2. Graph\nballoonplot(t(dt), \n            main =\"Contingency Table\",\n            xlab =\"cyl\", ylab=\"am\",\n            label = FALSE, \n            show.margins = FALSE)\n\n\n\n\n\nCompute Chi-Square test: The Chi-square statistic can be easily computed using the function chisq.test() as follow:\n\n\nchisq &lt;- chisq.test(ctab)\n\nWarning in chisq.test(ctab): Chi-squared approximation may be incorrect\n\nchisq\n\n\n    Pearson's Chi-squared test\n\ndata:  ctab\nX-squared = 8.7407, df = 2, p-value = 0.01265\n\n\n\nIn our example, the row and the column variables are statistically significantly associated (p-value = 0.01265). This indicates a statistically significant association between the variables at common alpha levels (like 0.05).\nThe observed and the expected counts can be extracted from the result of the test as follows:\n\n\n# Observed counts\nchisq$observed\n\n   \n     4  6  8\n  0  3  4 12\n  1  8  3  2\n\n# Expected counts\nround(chisq$expected, 2)\n\n   \n       4    6    8\n  0 6.53 4.16 8.31\n  1 4.47 2.84 5.69\n\n\n\nPearson Residuals:\n\nPositive residuals are positive values in cells specify an attraction (positive association) between the corresponding row and column variables.\nNegative residuals implies a repulsion (negative association) between the corresponding row and column variables.\n\nround(chisq$residuals, 3)\n\n   \n         4      6      8\n  0 -1.382 -0.077  1.279\n  1  1.670  0.093 -1.546\n\n\nThe overall qualitative objective of running the Chi-Square Test of Independence, as described in the example using the mtcars dataset in R, is to determine whether there is a statistically significant relationship between different categorical variables in the dataset. Specifically, the test aims to explore the association between variables like the number of cylinders (cyl), transmission type (am), in the dataset. The key goals and insights sought from this test include:\n\nUnderstanding Relationships Between Variables: To identify if there is a statistically significant association between different categorical variables in the dataset. For instance, it might reveal whether the type of transmission (automatic or manual) is associated with the number of cylinders or gears in the cars.\nTesting Hypotheses About Associations: The Chi-Square Test of Independence is essentially a hypothesis test. The null hypothesis states that there is no association between the variables (they are independent), while the alternative hypothesis suggests there is an association (they are not independent).\nGuiding Data-Driven Decisions: By understanding the relationships between variables, businesses, analysts, or data scientists can make informed decisions. For example, if a significant association is found between certain car features, it might influence marketing strategies or product development.\nVisualizing Data Relationships: Through graphical representation like balloon plots, the test provides a visual understanding of the strength and nature of the relationships between categorical variables.\nQuantitative Analysis of Associations: Beyond just identifying if an association exists, the test provides measures like the chi-square statistic, p-value, observed and expected counts, and Pearson residuals. These measures quantify the strength of association and help in understanding the nature of the relationship (whether it’s positive or negative).\n\nThe overall objective of this test is to qualitatively and quantitatively analyze and understand the interrelationships between different categorical variables in the mtcars dataset."
  },
  {
    "objectID": "06Correlation.html",
    "href": "06Correlation.html",
    "title": "Correlation Analysis",
    "section": "",
    "text": "July 26, 2023\n\nCorrelation is a statistical measure that describes the direction and strength of two variables’ relationship.\nIt is used to determine whether two variables have a relationship and how closely are the two variables are related.\nThe Pearson’s \\(r\\), or correlation coefficient, ranges from -1 to 1.\n\n\nA value of -1 denotes a perfect negative correlation, in which one variable decreases as the other increases.\nA value of 1 denotes a perfect positive correlation, in which both variables increase or decrease simultaneously.\nA value of 0 indicates that there is no relationship between the two variables.\n\n\n\n\n\nCorrelation analysis can prove beneficial in analyzing different aspects of Marketing such as customer behavior, advertising effectiveness and customer satisfaction.\n\nCustomer Behavior: Correlation analysis can be used to understand the relationships between customer behavior, such as product purchase history, and demographic data, such as age and income.\nAdvertising effectiveness: Correlation analysis can be used to better understand the relationship between advertising spend and sales. This can assist marketers in determining the most effective advertising channels and optimizing their advertising budget.\nCustomer Satisfaction: Correlation analysis can help understand the connection between customer satisfaction and product features. This can help marketers understand what features are most important to their customers and inform product design decisions.\n\n\n\n\nStudying numerous facets of finance, such as portfolio diversification, risk management, asset pricing, and credit risk management, might benefit from correlation analysis.\n\nPortfolio diversification: Correlation can describe how closely related are the assets in a portfolio to one another. Investors can reduce their exposure to risk and diversify their holdings by knowing the correlation between various assets. If two assets have a high positive correlation, investment in both assets may not offer much diversification benefit. The benefits of diversification become larger, in case the correlation between two assets is low or negative.\nRisk management: Correlation can be used to assess the interaction between different risk indicators including interest rates, market volatility, and credit risk. By understanding the relationship between these components, investors may more effectively control their risk exposure and safeguard their portfolios from potential losses.\nCredit risk management: The relationship between various credit risk parameters, such as borrower creditworthiness and loan performance, could be measured using correlation in credit risk management. Lenders can more accurately analyse the risk of potential borrowers and make more informed lending decisions by being aware of the association between these attributes.\n\n\n\n\nWhen researching different facets of organisational behaviour, such as employee engagement, diversity and inclusion, organisational culture, and job performance, correlation analysis could prove beneficial.\n\nEmployee engagement: The relationship between many elements that affect employee engagement, such as job satisfaction, employee motivation, and organisational culture, can be measured using correlation. Companies can discover the major factors that influence employee engagement and make positive interventions to enhance employee engagement by a deeper understanding of the relationship between these variables.\nDiversity and inclusion: The relationship between diversity and inclusion variables and their influence on organisational outcomes, such as employee performance, customer satisfaction, and profitability, can be measured using correlation. Companies may create diversity and inclusion policies and initiatives that are more effective by deeply recognizing the relationship between these elements.\nJob performance: Correlation analysis could quantify the relationship between many elements that affect worker motivation, skill level, and training. Companies can identify the primary influences on work performance and create more effective performance management programs and strategies by understanding the relationship between these variables."
  },
  {
    "objectID": "06Correlation.html#business-applications-of-correlation-analysis",
    "href": "06Correlation.html#business-applications-of-correlation-analysis",
    "title": "Correlation Analysis",
    "section": "",
    "text": "Correlation analysis can prove beneficial in analyzing different aspects of Marketing such as customer behavior, advertising effectiveness and customer satisfaction.\n\nCustomer Behavior: Correlation analysis can be used to understand the relationships between customer behavior, such as product purchase history, and demographic data, such as age and income.\nAdvertising effectiveness: Correlation analysis can be used to better understand the relationship between advertising spend and sales. This can assist marketers in determining the most effective advertising channels and optimizing their advertising budget.\nCustomer Satisfaction: Correlation analysis can help understand the connection between customer satisfaction and product features. This can help marketers understand what features are most important to their customers and inform product design decisions.\n\n\n\n\nStudying numerous facets of finance, such as portfolio diversification, risk management, asset pricing, and credit risk management, might benefit from correlation analysis.\n\nPortfolio diversification: Correlation can describe how closely related are the assets in a portfolio to one another. Investors can reduce their exposure to risk and diversify their holdings by knowing the correlation between various assets. If two assets have a high positive correlation, investment in both assets may not offer much diversification benefit. The benefits of diversification become larger, in case the correlation between two assets is low or negative.\nRisk management: Correlation can be used to assess the interaction between different risk indicators including interest rates, market volatility, and credit risk. By understanding the relationship between these components, investors may more effectively control their risk exposure and safeguard their portfolios from potential losses.\nCredit risk management: The relationship between various credit risk parameters, such as borrower creditworthiness and loan performance, could be measured using correlation in credit risk management. Lenders can more accurately analyse the risk of potential borrowers and make more informed lending decisions by being aware of the association between these attributes.\n\n\n\n\nWhen researching different facets of organisational behaviour, such as employee engagement, diversity and inclusion, organisational culture, and job performance, correlation analysis could prove beneficial.\n\nEmployee engagement: The relationship between many elements that affect employee engagement, such as job satisfaction, employee motivation, and organisational culture, can be measured using correlation. Companies can discover the major factors that influence employee engagement and make positive interventions to enhance employee engagement by a deeper understanding of the relationship between these variables.\nDiversity and inclusion: The relationship between diversity and inclusion variables and their influence on organisational outcomes, such as employee performance, customer satisfaction, and profitability, can be measured using correlation. Companies may create diversity and inclusion policies and initiatives that are more effective by deeply recognizing the relationship between these elements.\nJob performance: Correlation analysis could quantify the relationship between many elements that affect worker motivation, skill level, and training. Companies can identify the primary influences on work performance and create more effective performance management programs and strategies by understanding the relationship between these variables."
  },
  {
    "objectID": "06Correlation.html#mean",
    "href": "06Correlation.html#mean",
    "title": "Correlation Analysis",
    "section": "Mean",
    "text": "Mean\n\nThe average value of a set of numbers is represented by the mean, which is a statistical measure of central tendency.\nIt is determined by adding all of the values in the set and dividing by the total number of values."
  },
  {
    "objectID": "06Correlation.html#standard-deviation",
    "href": "06Correlation.html#standard-deviation",
    "title": "Correlation Analysis",
    "section": "Standard Deviation",
    "text": "Standard Deviation\n\nStandard deviation is a measure of the amount of variation or dispersion of a set of data values.\nIt provides a way to quantify how far apart the values in a data set are from the mean.\nIt is calculated by finding the square root of the variance, which is the average of the squared differences between each value and the mean.\nThe formula for standard deviation (s) is:\n\n\\[ s = \\sqrt(∑(x_i - x_m)^2 / n) \\]\n\n\\(x_i\\) is an individual data value\n\\(x_m\\) is the mean of the data set\n\\(n\\) is the number of observations in the data set\n\n\nA small standard deviation indicates that the values in a data set are close to the mean, while a large standard deviation indicates that the values are relatively spread out from the mean."
  },
  {
    "objectID": "06Correlation.html#calculating-mean-and-standard-deviation-in-r",
    "href": "06Correlation.html#calculating-mean-and-standard-deviation-in-r",
    "title": "Correlation Analysis",
    "section": "Calculating Mean and Standard Deviation in R",
    "text": "Calculating Mean and Standard Deviation in R\n\n# Load the mtcars data set\ndata(mtcars)\n# Calculate the mean and standard deviation of the \"mpg\"\nmpg_mean &lt;- mean(mtcars$mpg)\nmpg_sd &lt;- sd(mtcars$mpg)\n# Print the results\ncat(\"Mean:\", round(mpg_mean, 2), \"\\n\")\n\nMean: 20.09 \n\ncat(\"Standard deviation:\", round(mpg_sd, 2), \"\\n\")\n\nStandard deviation: 6.03"
  },
  {
    "objectID": "06Correlation.html#covariance",
    "href": "06Correlation.html#covariance",
    "title": "Correlation Analysis",
    "section": "Covariance",
    "text": "Covariance\n\nCovariance is a measure of the relationship between two variables and the extent to which they vary together.\nIt is a value that describes the direction of the relationship between two variables.\nThe formula for covariance (cov) is:\n\n\\[ cov = (∑(x_i - x_m) * (y_i - y_m)) / (n-1) \\]\n\n\\(x_i\\) and \\(y_i\\) are individual values of two variables\n\\(x_m\\) and \\(y_m\\) are the means of the two variables\n\\(n\\) is the number of observations\n\n\nCovariance provides a way to quantify the relationship between two variables.\n\n\nIf the covariance is positive, the two variables are said to positively co-vary, which means that if one variable rises, the other rises as well.\nIf the covariance is negative, the two variables are said to negatively co-vary, which means that when one variable rises, the other one falls.\n\n\nThe limitation of covariance is that it does not reveal the strength of the relationship between the variables."
  },
  {
    "objectID": "06Correlation.html#calculating-covariance-in-r",
    "href": "06Correlation.html#calculating-covariance-in-r",
    "title": "Correlation Analysis",
    "section": "Calculating Covariance in R",
    "text": "Calculating Covariance in R\n\n# Load the mtcars dataset\ndata(mtcars)\n# Calculate the covariance between the mpg and wt variables\ncov(mtcars$mpg, mtcars$wt)\n\n[1] -5.116685"
  },
  {
    "objectID": "06Correlation.html#pearsons-correlation-coefficient",
    "href": "06Correlation.html#pearsons-correlation-coefficient",
    "title": "Correlation Analysis",
    "section": "Pearson’s Correlation Coefficient",
    "text": "Pearson’s Correlation Coefficient\n\nThe Pearson’s Correlation Coefficient, a statistical metric commonly referred to as Pearson’s r, describes the degree and direction of a linear relationship between two variables.\nThe Pearson’s Correlation Coefficient ranges from -1 to 1, where:\n\n\nIf the value is 1, there is a perfect positive correlation, implying that when one measure rises, the other rises linearly.\nIf the value is -1, there is a perfect negative correlation, which means that as one measure rises, the other one falls linearly.\nNo correlation, or a value of 0, denotes the absence of a linear relationship between the two variables.\n\n\nThe Pearson’s Correlation Coefficient is calculated by dividing the covariance between the two variables by the product of their standard deviations."
  },
  {
    "objectID": "06Correlation.html#formula",
    "href": "06Correlation.html#formula",
    "title": "Correlation Analysis",
    "section": "Formula",
    "text": "Formula\n\nThe formula for Pearson’s Correlation Coefficient (r) is:\n\n\\[ r = ∑((x_i - x_m) * (y_i - y_m)) / \\sqrt((∑(x_i - x_m)^2) * (∑(y_i - y_m)^2)) \\]\n\n\\(x_i\\) and \\(y_i\\) are the individual values of the two variables being analyzed\n\\(x_m\\) and \\(y_m\\) are the means of the two variables\n\\(n\\) is the number of observations\n\n\nThis formula measures the linear relationship between two variables by dividing the covariance between the two variables by the product of their standard deviations."
  },
  {
    "objectID": "06Correlation.html#calculating-pearsons-correlation-coefficient-in-r",
    "href": "06Correlation.html#calculating-pearsons-correlation-coefficient-in-r",
    "title": "Correlation Analysis",
    "section": "Calculating Pearson’s Correlation Coefficient in R",
    "text": "Calculating Pearson’s Correlation Coefficient in R\n\n# Load the mtcars dataset\ndata(mtcars)\n# Calculate the correlation between the mpg and wt variables\ncor(mtcars$mpg, mtcars$wt)\n\n[1] -0.8676594"
  },
  {
    "objectID": "06Correlation.html#assessing-the-relationship-using-scatter-plot",
    "href": "06Correlation.html#assessing-the-relationship-using-scatter-plot",
    "title": "Correlation Analysis",
    "section": "Assessing the Relationship using Scatter Plot",
    "text": "Assessing the Relationship using Scatter Plot\n\n# Load the mtcars dataset\ndata(mtcars)\n# relationship between the mpg and wt variables\nplot(mtcars$mpg, mtcars$wt, xlab = \"MPG\", ylab = \"Wt\")"
  },
  {
    "objectID": "06Correlation.html#correlation-matrix",
    "href": "06Correlation.html#correlation-matrix",
    "title": "Correlation Analysis",
    "section": "Correlation Matrix",
    "text": "Correlation Matrix\n\nThe correlation coefficients between two variables are displayed in a table called a correlation matrix.\nIt is a symmetric matrix where the upper and lower triangles hold the correlation coefficients between each pair of variables, and the diagonal holds the correlation of each variable with itself, which is always 1.\nIt is an effective tool for investigating the connections between variables in a dataset."
  },
  {
    "objectID": "06Correlation.html#creating-a-correlation-matrix-in-r",
    "href": "06Correlation.html#creating-a-correlation-matrix-in-r",
    "title": "Correlation Analysis",
    "section": "Creating a Correlation Matrix in R",
    "text": "Creating a Correlation Matrix in R\n\n# Load the mtcars dataset\ndata(mtcars)\n# Calculate the correlation matrix for mpg, wt, hp, drat\ncor(mtcars[,c(\"mpg\",\"wt\",\"hp\",\"drat\")])\n\n            mpg         wt         hp       drat\nmpg   1.0000000 -0.8676594 -0.7761684  0.6811719\nwt   -0.8676594  1.0000000  0.6587479 -0.7124406\nhp   -0.7761684  0.6587479  1.0000000 -0.4487591\ndrat  0.6811719 -0.7124406 -0.4487591  1.0000000"
  },
  {
    "objectID": "06Correlation.html#creating-a-scatter-plot-matrix-in-r",
    "href": "06Correlation.html#creating-a-scatter-plot-matrix-in-r",
    "title": "Correlation Analysis",
    "section": "Creating a Scatter Plot Matrix in R",
    "text": "Creating a Scatter Plot Matrix in R\n\n# Load the mtcars dataset\ndata(mtcars)\n# scatter plot matrix for mpg, wt, hp, drat\npairs(mtcars[,c(\"mpg\",\"wt\",\"hp\",\"drat\")], pch = 19)"
  },
  {
    "objectID": "06Correlation.html#hypothesis-testing-for-correlation",
    "href": "06Correlation.html#hypothesis-testing-for-correlation",
    "title": "Correlation Analysis",
    "section": "Hypothesis Testing for Correlation",
    "text": "Hypothesis Testing for Correlation\nThe hypothesis testing process can be used to examine the null hypothesis that there is no significant correlation between two variables using Pearson’s correlation coefficient:\n\nState the null hypothesis (H0) and the alternative hypothesis (H1):\n\n\nH0: There is no significant correlation between the two variables (the correlation coefficient is zero).\nH1: There is a significant correlation between the two variables (the correlation coefficient is not zero).\n\n\nDetermine the level of significance (\\(α\\)) that you will use to test the hypothesis.\nCalculate the sample correlation coefficient (\\(r\\)) and the sample size (\\(n\\)).\nCalculate the degrees of freedom (\\(df\\)), which is equal to \\(n-2\\).\nCalculate the critical values for the test statistic, using a table or calculator based on the significance level and degrees of freedom.\nCalculate the test statistic (\\(t\\)), which is equal to \\(r\\sqrt{df/(1-r^2)}\\).\nCompare the test statistic to the crucial values. Reject the null hypothesis if the test statistic is outside the crucial range. Reject the null hypothesis if the test statistic is inside the crucial range.\nCalculate a p-value for the test statistic.\n\nReject the null hypothesis if the p-value is less than the significance level.\nFail to reject the null hypothesis if the p-value exceeds the significance level."
  },
  {
    "objectID": "06Correlation.html#hypothesis-test-for-pearsons-correlation-coefficient-in-r",
    "href": "06Correlation.html#hypothesis-test-for-pearsons-correlation-coefficient-in-r",
    "title": "Correlation Analysis",
    "section": "Hypothesis Test for Pearson’s correlation coefficient in R",
    "text": "Hypothesis Test for Pearson’s correlation coefficient in R\n\nRun the following code using cor.test()\n\n\n# Load the mtcars dataset\ndata(mtcars)\n# Perform a hypothesis test for the correlation between mpg and wt\ncor.test(mtcars$mpg, mtcars$wt)\n\n\n    Pearson's product-moment correlation\n\ndata:  mtcars$mpg and mtcars$wt\nt = -9.559, df = 30, p-value = 1.294e-10\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.9338264 -0.7440872\nsample estimates:\n       cor \n-0.8676594 \n\n\n\nThis will execute a hypothesis test and calculate the Pearson correlation coefficient, returning the test statistic, degrees of freedom, p-value, and correlation coefficient confidence interval.\nWe can reject the null hypothesis and determine that there is a significant correlation between the two variables if the p-value is smaller than the significance level.\n\n\nAlternately, the corr.test() function, part of the psych package, can be used to perform hypothesis testing and confidence interval estimation for Pearson’s correlation coefficient.\n\n\n# Load the mtcars dataset\ndata(mtcars)\n\n# Calculate the correlation between mpg and wt\nlibrary(psych)\ncorr.test(mtcars$mpg, \n          mtcars$wt, \n          method = \"pearson\", \n          use = \"pairwise\", \n          adjust = \"none\", \n          ci = TRUE, \n          alpha = 0.05)\n\nCall:corr.test(x = mtcars$mpg, y = mtcars$wt, use = \"pairwise\", method = \"pearson\", \n    adjust = \"none\", alpha = 0.05, ci = TRUE)\nCorrelation matrix \n[1] -0.87\nSample Size \n[1] 32\n[1] 0\n\n To see confidence intervals of the correlations, print with the short=FALSE option\n\n\n\nThis will return the correlation coefficient, p-value, and confidence interval for the correlation between mpg and wt.\nThe output will also include information on the sample size, missing values, and adjustment method used.\nTo use the corr.test() function on a dataframe in R, it is helpful to first select the columns of interest and pass them as arguments to the function.\n\n\n# Load the mtcars dataset\ndata(mtcars)\n\n# Select the columns of interest\nvars &lt;- c(\"mpg\", \"disp\", \"hp\", \"wt\")\n\n# Calculate the correlation matrix and perform hypothesis tests\nlibrary(psych)\ncorr.test(mtcars[, vars], \n          method = \"pearson\", \n          use = \"pairwise\", \n          adjust = \"none\", \n          ci = TRUE, \n          alpha = 0.05)\n\nCall:corr.test(x = mtcars[, vars], use = \"pairwise\", method = \"pearson\", \n    adjust = \"none\", alpha = 0.05, ci = TRUE)\nCorrelation matrix \n       mpg  disp    hp    wt\nmpg   1.00 -0.85 -0.78 -0.87\ndisp -0.85  1.00  0.79  0.89\nhp   -0.78  0.79  1.00  0.66\nwt   -0.87  0.89  0.66  1.00\nSample Size \n[1] 32\nProbability values (Entries above the diagonal are adjusted for multiple tests.) \n     mpg disp hp wt\nmpg    0    0  0  0\ndisp   0    0  0  0\nhp     0    0  0  0\nwt     0    0  0  0\n\n To see confidence intervals of the correlations, print with the short=FALSE option\n\n\n\nIn this example, the names of the relevant columns from the mtcars dataset are contained in a character vector in the vars variable. The mtcars dataset subset containing only these columns is subsequently submitted to the corr.test() function.\nThe result will be a correlation matrix with rows and columns matching to the relevant variables, as well as a set of hypothesis tests.\nThe hypothesis tests will report the correlation coefficient, p-value, and confidence range for each correlation, and the matrix will display the pairwise correlations between the variables."
  },
  {
    "objectID": "06Correlation.html#visualizing-correlation-matrix-using-corrgram",
    "href": "06Correlation.html#visualizing-correlation-matrix-using-corrgram",
    "title": "Correlation Analysis",
    "section": "Visualizing Correlation Matrix using corrgram",
    "text": "Visualizing Correlation Matrix using corrgram\n\n# Load the mtcars dataset\ndata(mtcars)\n# Select the columns of interest\nvars &lt;- c(\"mpg\", \"disp\", \"hp\", \"wt\")\n\n# Calculate the correlation matrix and perform hypothesis tests\nlibrary(corrgram)\n\n# Create the corrgram plot\ncorrgram(mtcars[, vars],\n         order = TRUE, \n         lower.panel = panel.ellipse, \n         upper.panel = panel.cor)\n\nWarning in par(usr): argument 1 does not name a graphical parameter\n\nWarning in par(usr): argument 1 does not name a graphical parameter\n\nWarning in par(usr): argument 1 does not name a graphical parameter\n\nWarning in par(usr): argument 1 does not name a graphical parameter\n\nWarning in par(usr): argument 1 does not name a graphical parameter\n\nWarning in par(usr): argument 1 does not name a graphical parameter"
  },
  {
    "objectID": "06Correlation.html#visualizing-correlation-matrix-using-corrplot",
    "href": "06Correlation.html#visualizing-correlation-matrix-using-corrplot",
    "title": "Correlation Analysis",
    "section": "Visualizing Correlation Matrix using corrplot",
    "text": "Visualizing Correlation Matrix using corrplot\n\n# Load the mtcars dataset\ndata(mtcars)\nlibrary(corrplot)\n\ncorrplot 0.92 loaded\n\n# Select the columns of interest\nvars &lt;- c(\"mpg\", \"disp\", \"hp\", \"wt\")\nM &lt;- cor(mtcars[, vars])\ncorrplot(M, type=\"upper\", order=\"hclust\")"
  },
  {
    "objectID": "06Correlation.html#visualizing-correlation-matrix-using-corrplot-1",
    "href": "06Correlation.html#visualizing-correlation-matrix-using-corrplot-1",
    "title": "Correlation Analysis",
    "section": "Visualizing Correlation Matrix using corrplot",
    "text": "Visualizing Correlation Matrix using corrplot\n\n# Load the mtcars dataset\ndata(mtcars)\nlibrary(corrplot)\n# Select the columns of interest\nvars &lt;- c(\"mpg\", \"disp\", \"hp\", \"wt\")\nM &lt;- cor(mtcars[, vars]) \ncorrplot(M, method=\"number\")"
  },
  {
    "objectID": "06Correlation.html#assumptions-in-pearsons-correlation-coefficient",
    "href": "06Correlation.html#assumptions-in-pearsons-correlation-coefficient",
    "title": "Correlation Analysis",
    "section": "Assumptions in Pearson’s Correlation Coefficient",
    "text": "Assumptions in Pearson’s Correlation Coefficient\nPearson’s correlation coefficient is a parametric measure of the linear relationship between two variables. It relies on several assumptions, including:\n\nLinearity: The two variables’ relationship should be linear. The Pearson’s correlation coefficient may not adequately represent the strength of the association between the variables if the relationship is nonlinear.\nNormality: The correlated variables should have a normal distribution. The reliability of Pearson’s correlation coefficient may be compromised if the variables are not normally distributed.\nHomoscedasticity: All levels of the variables should have the same variance for the two variables. The Pearson correlation coefficient may be impacted by unequal variance, making the relationship between the variables look stronger or weaker than it actually is.\nIndependence: The two variables need to be unrelated to one another. The Pearson correlation coefficient may not effectively depict the relationship between the variables if the two variables are not independent.\nOutliers: The correlation coefficient might be distorted by outliers, which can affect the outcomes. As a result, it is advised to check the data for outliers prior to determining the Pearson’s correlation coefficient.\n\nBefore using Pearson’s correlation coefficient, it is essential to validate these hypotheses to avoid incorrect interpretations of the data. Where these assumptions are not true, nonparametric correlation measurements, such as Spearman’s correlation coefficient, may be more appropriate."
  },
  {
    "objectID": "06Correlation.html#spearmans-correlation-coefficient",
    "href": "06Correlation.html#spearmans-correlation-coefficient",
    "title": "Correlation Analysis",
    "section": "Spearman’s Correlation Coefficient",
    "text": "Spearman’s Correlation Coefficient\n\nA nonparametric indicator of the relationship between two variables is the Spearman correlation. It evaluates how well a monotonic function, which can only be strictly increasing or decreasing, can describe the relationship between two variables.\nIt is independent of the distributional assumptions underlying the correlated variables.\nInstead, it computes the correlation coefficient based on the ranks of the variables, rather than their actual values."
  },
  {
    "objectID": "06Correlation.html#calculating-spearmans-correlation-coefficient-in-r",
    "href": "06Correlation.html#calculating-spearmans-correlation-coefficient-in-r",
    "title": "Correlation Analysis",
    "section": "Calculating Spearman’s Correlation Coefficient in R",
    "text": "Calculating Spearman’s Correlation Coefficient in R\n\n# Load a dataset\ndata(mtcars)\n# Calculate the Spearman correlation between mpg and wt\ncor(mtcars$mpg, \n    mtcars$wt, \n    method = \"spearman\")\n\n[1] -0.886422"
  },
  {
    "objectID": "02ZTests.html",
    "href": "02ZTests.html",
    "title": "Z Tests",
    "section": "",
    "text": "January 21, 2024.\n\n\nWe will begin with the Z-test.\nThe Z-test for proportions is a statistical method used to compare observed proportions, such as the proportion of successes in two groups, to see if there is a significant difference between them. This test is particularly useful when dealing with large sample sizes.\nDepending on the research question and the type of data being analyzed, different types of Z-tests are frequently employed in statistics. Variations of the Z-test can be used to compare sample means or sample proportions. In this chapter, we will confine our attention to the Z-tests used to analyze categorical data and sample proportions.\nThe following are three prevalent types of Z-tests for Categorical data:\n\nOne-sample Z-test: This test determines whether a sample proportion significantly deviates from a known or hypothesized population mean or proportion.\nTwo-sample Z-test: This test is used to determine whether there is a statistically significant difference between the proportions of two independent groups.\nPaired Z-test for proportions: This test determines whether there is a statistically significant difference in proportions between two related groups, such as before and after a treatment or intervention. [1]\n\n\n\n\n\n\nIn marketing research and analysis, Z-tests for categorical data are useful tools for testing hypotheses and determining the significance of relationships between categorical variables. Here are some possible marketing applications of Z-tests for categorical data:\n\nBrand loyalty: A marketer may wish to determine whether customers who purchase a particular brand are more loyal to the brand based on repeat purchase, than those who purchase a competing brand. To compare the proportion of loyal customers between the two brands, a two-sample Z-test for proportions could be utilized.\nCustomer satisfaction: A marketer may wish to determine whether a new product or service is more satisfying to customers than its predecessor. Before and after the introduction of the new product or service, the proportion of satisfied customers could be compared.\nMarket segmentation: A marketer may use market segmentation to determine whether there are significant differences in customer demographics or behavior across market segments. A test could be utilized to determine if there is a significant relationship between the demographic variables and market segments.\nAdvertising effectiveness: A marketer may wish to determine if one advertising campaign is more effective than another at reaching the target audience. A two-sample Z-test for proportions could be used to compare the proportion of viewers who remember the message or take action following exposure to each campaign.\nProduct preference: A marketer may wish to determine whether a particular product feature or characteristic is more preferred by customers than another. A one-sample Z-test for proportions could be utilized to determine if the proportion of customers who prefer the feature significantly deviates from the null hypothesis. [2]\n\n\n\n\nZ tests can be helpful in studying different aspects of Finance such as default rates, investment decisions, stock market analysis, risk analysis.\n\nCredit risk assessment: A financial institution assessing credit risk may wish to determine whether the proportion of delinquent loans varies significantly across risk categories. A two-sample Z-test for proportions could be utilized to compare the proportion of delinquent loans across various risk categories.\nInvestment portfolio analysis: An investor may wish to determine whether the proportion of winning trades varies significantly between different types of investments, including stocks and bonds. Using a two-sample Z-test for proportions, it is possible to compare the proportion of profitable trades for the various types of investments.\nCustomer behavior analysis: A financial institution may conduct an analysis of customer behavior to determine whether the proportion of customers who use online banking varies significantly by age group. A Chi-square test could be utilized to determine if there is a significant relationship between the demographic variable (age group) and the proportion of customers who use online banking.\nMarket analysis: An investment bank may wish to determine if there is a significant correlation between the market type (bullish or bearish) and the proportion of investors who sell or purchase stocks. A Chi-square test could be used to determine if there is a significant relationship between the type of market and the proportion of investors who sell or purchase stocks.\nFraud detection: A financial institution may wish to determine whether the proportion of fraudulent transactions varies significantly between different types of transactions, such as ATM withdrawals, online purchases, and wire transfers, for the purpose of detecting fraud. A one-sample Z-test for proportions could be used to determine if the proportion of fraudulent transactions in a specific type of transaction significantly deviates from a null hypothesis. [3]\n\n\n\n\nIn organizational behavior research, Z-tests for categorical data can be useful tools for analyzing data and making informed decisions. Here are some potential organizational behavior applications of Z-tests for categorical data:\n\nEmployee satisfaction: An organization may wish to determine whether the proportion of satisfied employees varies significantly across departments. Using a two-sample Z-test for proportions, it is possible to compare the percentage of satisfied employees across departments.\nDiversity and inclusion: An organization may wish to determine whether the proportion of employees from diverse backgrounds varies significantly across management levels. A Chi-square test could be used to determine whether there is a significant relationship between the demographic variable (race, gender, etc.) and the management level.\nEmployee engagement: An organization may wish to determine whether the proportion of engaged employees differs significantly across teams or work units. Using a two-sample Z-test for proportions, it is possible to compare the proportion of engaged employees among various teams or work units.\nTraining effectiveness: An organization may wish to determine whether the percentage of employees who pass a certification exam differs significantly between those who have completed a training program and those who have not. The proportion of employees who pass the certification exam could be compared between the two groups using a two-sample Z-test for proportions.\nEmployee turnover: An organization may wish to determine whether the proportion of employees who voluntarily leave the organization varies significantly across job functions or departments. A Chi-square test could be used to determine whether there is a significant relationship between the job function or department and the percentage of employees who leave the organization voluntarily. [4]\n\n\n\n\n\n\nThis type of Z-test is used to examine a population proportion hypothesis.\nFor instance, a business may wish to determine whether the proportion of customers who are satisfied with their product exceeds 0.5.\nThe company could collect a sample of customers, determine the proportion of satisfied customers in the sample, and use a one-sample Z-test to determine if the proportion is significantly different from 0.5.\n\n\n\n\nThe general steps to perform a one-sample Z-test are:\n\nState the null hypothesis and alternative hypothesis: Typically, the null hypothesis states that the sample proportion matches the hypothesized population proportion, whereas the alternative hypothesis states that the sample proportion differs from the hypothesized population proportion.\nDetermine the level of significance (α): This is the maximum probability of rejecting the null hypothesis when it is true. Typical values for α are 0.05 or 0.01.\nCollect sample data: Obtain a random sample from the population and calculate the sample proportion \\((p)\\).\nCalculate the test statistic: The test statistic is calculated as follows: \\(z = (p_0 - p) / \\sqrt{(p * (1 - p) / n)}\\) , where \\(p\\) is the population proportion assumed under the null hypothesis, and \\(n\\) is the sample size.\nDetermine the critical value: The critical value is the value of \\(z\\) that corresponds to the chosen level of significance and degrees of freedom \\((df = n-1)\\).\nCompare the test statistic to the critical value: If the absolute value of the test statistic is greater than the critical value, we reject the null hypothesis. Otherwise, we fail to reject the null hypothesis.\nInterpret the results: If the null hypothesis is rejected, one can conclude that the proportion of the sample is significantly different from that of the population. If the null hypothesis is not rejected, it can be concluded that the sample proportion is not significantly different from the population proportion.\nDetermine the p-value using a Z-table or calculator, and compare the p-value to the level of significance (alpha) in order to reach a conclusion.\nDraw a conclusion. [5]\n\n\n\n\n\n\nResearch Objective:\nA researcher wants to determine if the proportion of adults in a population who own a car significantly deviates from 70%. A random sample of 200 adults shows that 150 of them own a car.\nHypotheses:\n\nNull Hypothesis (H0): p = 0.7\nAlternative Hypothesis (Ha): p ≠ 0.7\nHere, p is the population proportion.\n\nSignificance Level:\nThe significance level is set at 0.05.\nCritical Value:\nThe critical value for a two-tailed test at the 0.05 significance level is ±1.96.\nTest Statistic Formula:\nThe test statistic (Z) is calculated using the formula: \\[ Z = \\frac{p_0 - p}{\\sqrt{p \\cdot (1 - p) / n}} \\]\n\np is the hypothesized population proportion.\np_0 is the sample proportion.\nn is the sample size.\n\nCalculation:\n\nSample proportion (p_0): 0.75 (150 out of 200)\nHypothesized population proportion (p): 0.7\nSample size (n): 200\nThe Z-score is calculated as approximately 1.54.\n\n\nVerify by plugging in the values: \\[ Z = \\frac{0.75 - 0.7}{\\sqrt{0.7 \\cdot (1 - 0.7) / 200}} \\]\nThe calculated Z-score is approximately 1.54.\n\nDecision Rule:\n\nCompare the absolute value of the test statistic with the critical value. If it’s greater than 1.96, reject the null hypothesis.\n\nConclusion:\n\nSince the absolute value of the test statistic (1.54) is less than the critical value (1.96), we fail to reject the null hypothesis. There is not enough statistical evidence to conclude that the proportion of adults who own a car in the population is significantly different from 70%. [6]\n\n\n\n\n\n\nConsider the mtcars data set. Suppose we want to test whether the proportion of cars with a 6-cylinder engine (cyl=6) is significantly different from a hypothesized value of 50%.\n\n\nNull hypothesis: The proportion of 6-cylinder engine cars in the mtcars dataset is 0.5 (p = 0.5).\nAlternative hypothesis: The proportion of 6-cylinder engine cars in the mtcars dataset is not equal to 0.5 (p ≠ 0.5).\n\n\n# Load the mtcars dataset\ndata(mtcars)\n\n# Create a binary variable indicating whether a car has a 6-cylinder engine or not\nmtcars$is_cyl6 &lt;- ifelse(mtcars$cyl == 6, 1, 0)\n\n# Count the number of 6 cylinder cars\nn6 &lt;- sum(mtcars$is_cyl6)\n\n# Count the total number of cars\nn &lt;- length(mtcars$is_cyl6)\n  # alternate code\n  # n &lt;- nrow(mtcars)\n\n# Calculate the sample proportion of cars with a 6-cylinder engine\nprop_cyl6 &lt;- mean(mtcars$is_cyl6)\n\n# Conduct a one-sample Z-test for proportions using the prop.test function\nprop.test(n6, \n          n, \n          p = 0.5)\n\n\n    1-sample proportions test with continuity correction\n\ndata:  n6 out of n, null probability 0.5\nX-squared = 9.0312, df = 1, p-value = 0.002654\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.09944097 0.40441815\nsample estimates:\n      p \n0.21875 \n\n\n\nThe sample proportion is 0.21875, indicating that only 21.875% of the cars in the dataset are equipped with a six-cylinder engine.\nUsing the prop.test function, we can now perform a one-sample Z-test on proportions:\nThe prop.test function requires three arguments:\nthe number of successes (in this case, the number of cars with a 6-cylinder engine),\nthe total number of trials (in this case, the total number of cars), and\nthe population proportion hypothesis (0.5 in this case)\nThe output reveals that the test statistic is X-squared = 9.0312 with a p-value of 0.002654 and one degree of freedom.\nSince the p-value is less than the significance level (0.05), we reject the null hypothesis and conclude that the proportion of cars in the population with a 6-cylinder engine is significantly different from 50%.\nThe sample proportion estimate is 0.21875, with a 95% confidence interval encompassing this value (0.099, 0.404). [7]\n\n\n\n\n\n\nIntroduction: The independent samples Z-test for proportions is used to determine if there’s a significant difference between two sample proportions drawn from independent populations.\nExample Scenario: A company might want to know if the proportion of male customers who purchase their product is significantly different from the proportion of female customers.\nApplication: The company collects separate samples of male and female customers, calculates the purchase proportions, and uses an independent samples Z-test to assess the significance of any difference. [8]\n\n\n\nConducting the Test:\n\nHypotheses:\n\nNull Hypothesis (H0): The two population proportions are equal.\nAlternative Hypothesis (Ha): The two population proportions are not equal.\n\nSignificance Level: Typically set at 0.05, it’s the acceptable risk level for incorrectly rejecting the null hypothesis.\nTest Statistic: The test statistic is calculated using the formula: \\[ Z = \\frac{p1 - p2}{\\sqrt{p \\cdot (1 - p) \\cdot (\\frac{1}{n1} + \\frac{1}{n2})}} \\] where ( p1 ) and ( p2 ) are sample proportions, ( n1 ) and ( n2 ) are sample sizes, and ( p ) is the pooled sample proportion: \\[ p = \\frac{x1 + x2}{n1 + n2} \\] ( x1 ) and ( x2 ) are the number of successes in each sample.\nDecision Rule: The null hypothesis is rejected if the absolute value of ( Z ) is greater than the critical value (e.g., 1.96 for a two-tailed test at 0.05 significance level).\nConclusion: Based on the test results, a conclusion is drawn about the difference in population proportions. [8]\n\n\n\n\nScenario:\n\nA researcher studies the purchasing behavior of men and women for a specific product. A sample of 250 men and 350 women is taken, with results showing that 125 men and 210 women purchased the product.\nHypotheses:\n\nNull Hypothesis (H0): p1 = p2\nAlternative Hypothesis (Ha): p1 ≠ p2\n\nwhere (p1) is the proportion of men, and (p2) is the proportion of women purchasing the product.\nSignificance Level: Set at 0.05.\nCritical Value: For a two-tailed test at the 0.05 significance level, the critical value is approximately 1.96.\nTest Statistic Calculation:\nThe test statistic Z is calculated using the formula:\n\\[ Z = \\frac{p1 - p2}{\\sqrt{p \\cdot (1 - p) \\cdot \\left(\\frac{1}{n1} + \\frac{1}{n2}\\right)}} \\]\nwhere ( p ) is the pooled proportion, computed as:\n\\[ p = \\frac{x1 + x2}{n1 + n2} \\]\nUsing the Data:\nFor men: \\[ p1 = \\frac{125}{250} = 0.5 \\]\nFor women: \\[ p2 = \\frac{210}{350} = 0.6 \\]\nPooled proportion: \\[ p = \\frac{125 + 210}{250 + 350} = 0.55 \\]\nTest Statistic:\nCalculation of the Z value:\n\\[ Z = \\frac{0.5 - 0.6}{\\sqrt{0.55 \\cdot (1 - 0.55) \\cdot \\left(\\frac{1}{250} + \\frac{1}{350}\\right)}} \\]\n\nThe calculated Z value is approximately -2.43. This value is negative because the proportion of women who purchased the product (p2 = 0.6) is higher than that of men (p1 = 0.5). However, since we are conducting a two-tailed test, the direction of the difference is not our primary concern. We are interested in whether the absolute value of the Z score is greater than the critical value.\n\nConclusion:\nSince ( |Z| = 2.43 ) is greater than the critical value of 1.96, we reject the null hypothesis. This indicates that there is a significant difference in the purchasing behavior between men and women for this product.\nInterpretation:\nWith a 95% confidence level, we conclude that the proportions of men and women purchasing this product are significantly different. Specifically, the data suggests that women are more likely to purchase this product than men.\n\nThis example demonstrates a scenario where the Z-test reveals a significant difference in purchasing behaviors between the two groups, aligned with your request for a scenario where the proportions are different and the Z-value is positive. Note that even though the calculated Z-value is negative, its absolute value is used for the conclusion.\n\n\n\nDemonstration using simulated data:\nTo perform the test described in the scenario using R, we can use the prop.test function. This function is designed to handle comparisons of proportions. Here’s how we can set it up with the data from the above scenario:\n\nx &lt;- c(125, 210) # number of men and women who purchased the product\nn &lt;- c(250, 350) # total number of men and women\n\n# Perform the two-proportion z-test\ntest_result &lt;- prop.test(x, \n                         n, \n                         alternative = \"two.sided\", \n                         correct = FALSE)\n# Output the results\nprint(test_result)\n\n\n    2-sample test for equality of proportions without continuity correction\n\ndata:  x out of n\nX-squared = 5.9138, df = 1, p-value = 0.01502\nalternative hypothesis: two.sided\n95 percent confidence interval:\n -0.18047113 -0.01952887\nsample estimates:\nprop 1 prop 2 \n   0.5    0.6 \n\n\nNotes about the R code\n\nx contains the number of successes (i.e., purchases) for each group (men first, then women).\nn contains the total number of observations for each group.\nprop.test is called with correct = FALSE to perform a standard two-proportion Z-test without continuity correction, which is appropriate for large sample sizes.\nwe can specify alternative = two.sided in the prop.test function to explicitly indicate that we are performing a two-sided test. However, it’s worth noting that the default setting for the alternative parameter in prop.test is already two.sided\nThe function will return various information, including the Z statistic and the p-value.\n\nInterpreting the output\n\nTest Type: We performed a “2-sample test for equality of proportions without continuity correction”. This means we compared two independent proportions (proportions of men and women who purchased the product in our scenario). The ‘without continuity correction’ part indicates that we didn’t apply any correction for continuity, suitable for large sample sizes.\nData: “x out of n” refers to our input data structure, where x represents the number of successes (men and women who purchased the product) and n represents the total number of observations (total men and women in our sample).\nX-squared: The test statistic value is 5.9138. In a two-proportion Z-test like ours, this statistic follows a chi-square distribution with one degree of freedom under the null hypothesis.\nDegrees of Freedom (df): It’s 1 in our case. For a two-sample test of proportions, the degrees of freedom is typically n-1, where n is the number of groups (2 in our case), but since 2-1 equals 1, our df is 1.\np-value: Our p-value is 0.01502. This measures the strength of the evidence against the null hypothesis. As it’s less than the typical alpha level of 0.05, we reject the null hypothesis, indicating a statistically significant difference between the proportions of men and women purchasing the product.\nAlternative Hypothesis: “two.sided” - This indicates that our test was two-tailed, meaning we were testing for any difference in proportions, not specifically if one proportion was greater or lesser than the other.\n95 Percent Confidence Interval: This interval, ranging from -0.1805 to -0.0195, provides a range of plausible values for the difference in proportions (p1 - p2). Since this interval doesn’t include 0, it suggests that the true difference in proportions is likely non-zero, reinforcing our conclusion from the p-value.\nSample Estimates: These are the observed proportions in our study - prop 1 is 0.5 (proportion of men who purchased the product) and prop 2 is 0.6 (proportion of women who purchased the product).\n\nConclusion: There is a statistically significant difference in the purchasing behavior between men and women for the product in our study, with the proportion of women purchasing the product being higher than that of men.\nAnother Demonstration using simulated data:\nWe can perform a one-sided test using the prop.test function in R by changing the alternative parameter from \"two.sided\" to either \"greater\" or \"less\". This choice depends on the direction of the alternative hypothesis we wish to test.\nIn our scenario, if we aim to test whether the proportion of men who purchased the product is greater than the proportion of women, we would use alternative = \"greater\". Conversely, if our hypothesis is that the proportion of men is less, we would use alternative = \"less\".\nHere’s how we can modify our code for a one-sided test:\n\nx &lt;- c(125, 210) # number of men and women who purchased the product\nn &lt;- c(250, 350) # total number of men and women\n\n# Perform the one-sided two-proportion z-test\n# We will use \"less\" to test if the proportion of men is less than that of women\n# Change to \"greater\" if testing for the opposite\ntest_result &lt;- prop.test(x, \n                         n, \n                         alternative = \"less\", \n                         correct = FALSE)\n# Output the results\nprint(test_result)\n\n\n    2-sample test for equality of proportions without continuity correction\n\ndata:  x out of n\nX-squared = 5.9138, df = 1, p-value = 0.007511\nalternative hypothesis: less\n95 percent confidence interval:\n -1.0000000 -0.0324665\nsample estimates:\nprop 1 prop 2 \n   0.5    0.6 \n\n\nIn this code, we use alternative = \"less\" for a one-sided test where our hypothesis is that the proportion for the first group (men) is less than that for the second group (women). We need to choose the appropriate direction (“greater” or “less”) based on our specific research hypothesis.\nInterpreting the output\n\nX-squared: The test statistic value is 5.9138. This value is derived from the comparison of the two proportions.\nP-value: The p-value is 0.007511. In a one-sided test, this value indicates the probability of observing a test statistic as extreme as, or more extreme than, the one observed, under the null hypothesis. Since it’s less than 0.05, it suggests significant evidence against the null hypothesis in favor of the alternative.\nAlternative Hypothesis: “less” - This specifies that the test is one-sided, testing whether the first proportion (prop 1) is less than the second proportion (prop 2).\n95 Percent Confidence Interval: Ranges from -1.000 to -0.0325. This interval indicates where the true difference in proportions (prop 1 - prop 2) likely lies. Since the interval does not include 0 and is entirely negative, it suggests that prop 1 is likely less than prop 2.\nSample Estimates: Shows the estimated proportions for each group - 0.5 (prop 1) and 0.6 (prop 2).\n\nConclusion: The test results suggest that there is a significant difference between the two groups, with the first group having a lower proportion compared to the second group, supporting the alternative hypothesis that the proportion of men purchasing the product is less than that of women.\n\n\n\nObjective: We aim to compare the proportion of cars with automatic transmission (indicated by am = 1) to those with V-shaped engines (indicated by vs = 1) in the mtcars dataset. Our goal is to determine if there is a significant difference in the proportions of these two categories of cars.\n\n# Loading the mtcars dataset\n# Loading the mtcars dataset\ndata(mtcars)\n\n# Defining the samples\n# sample1 represents cars with automatic transmission (am = 1)\n# sample2 represents cars with V-shaped engines (vs = 1)\nsample1 &lt;- mtcars$am\nsample2 &lt;- mtcars$vs\n\n# Calculating the number of successes in each sample\n# Success for sample1 is defined as having an automatic transmission (am = 1)\n# Success for sample2 is defined as having a V-shaped engine (vs = 1)\nsuccesses_sample1 &lt;- sum(sample1 == 1)\nsuccesses_sample2 &lt;- sum(sample2 == 1)\n\n# Calculating the sample sizes\n# The sample size for each group is the total number of observations in that group\nsize_sample1 &lt;- length(sample1)\nsize_sample2 &lt;- length(sample2)\n\n# Performing the Z-test for proportions\n# The test compares the proportion of cars with automatic transmission to those with V-shaped engines\nprop.test(x = c(successes_sample1, successes_sample2),\n          n = c(size_sample1, size_sample2),\n          alternative = \"two.sided\")\n\n\n    2-sample test for equality of proportions with continuity correction\n\ndata:  c(successes_sample1, successes_sample2) out of c(size_sample1, size_sample2)\nX-squared = 0, df = 1, p-value = 1\nalternative hypothesis: two.sided\n95 percent confidence interval:\n -0.3043652  0.2418652\nsample estimates:\n prop 1  prop 2 \n0.40625 0.43750 \n\n\nInterpreting the output\n\nTest Type: “2-sample test for equality of proportions with continuity correction” - This test compares two independent proportions (in this case, cars with automatic transmission vs. cars with V-shaped engines in the mtcars dataset). The continuity correction is applied, which is typically used for small sample sizes to improve the approximation of the binomial distribution to the normal distribution.\nX-squared: The test statistic value is 0. This value is used to determine the p-value. An X-squared of 0 usually indicates no observed difference between the proportions.\np-value: With a p-value of 1, there’s no statistical evidence to suggest a significant difference between the two proportions.\nAlternative Hypothesis: “two.sided” - The test was conducted to determine if there is any difference (either way) between the two proportions.\n95 Percent Confidence Interval: Ranges from -0.3044 to 0.2418. Since this interval includes 0, it suggests that the difference in proportions might be zero, supporting the lack of statistical significance.\nSample Estimates: The estimated proportions are 0.40625 for the first group (automatic transmissions) and 0.43750 for the second group (V-shaped engines).\n\nConclusion: The test suggests no significant difference between the proportion of cars with automatic transmissions and those with V-shaped engines in the mtcars dataset. [8]\n\n\n\n\n[1] Agresti, A. (2002). Categorical data analysis (2nd ed.). Wiley.\n[2] Hair Jr, J. F., Black, W. C., Babin, B. J., & Anderson, R. E. (2010). Multivariate data analysis: A global perspective (7th ed.). Pearson Education.\nChurchill Jr, G. A., & Iacobucci, D. (2010). Marketing research: Methodological foundations (10th ed.). Cengage Learning.\nKotler, P., & Keller, K. L. (2012). Marketing management (14th ed.). Pearson Education.\nBabin, B. J., & Zikmund, W. G. (2016). Essentials of marketing research (5th ed.). Cengage Learning.\nMalhotra, N. K., & Peterson, M. (2006). Basic marketing research (3rd ed.). Prentice-Hall.\n[3] Altman, E. I. (2008). Evaluating credit risk models. Journal of Banking & Finance, 32(8), 1544-1558.\nNofsinger, J. R. (2012). The psychology of investing (5th ed.). Pearson Education.\nMitra, A., & Gilbert, T. (2012). Banking on customer knowledge: The potential of customer relationship management and analytics in retail banking. Journal of Service Research, 15(1), 59-75.\nFrench, K. R., Schwert, G. W., & Stambaugh, R. F. (1987). Expected stock returns and volatility. Journal of Financial Economics, 19(1), 3-29.\nTsay, R. S., & Chen, T. H. (2012). Financial time series analysis (2nd ed.). Wiley.\n[4] Judge, T. A., & Watanabe, S. (1994). Another look at the job satisfaction-life satisfaction relationship. Journal of Applied Psychology, 79(6), 939-948.\nRoberson, Q. M. (2019). Race and ethnicity in the workplace. Annual Review of Organizational Psychology and Organizational Behavior, 6, 413-438.\nSaks, A. M. (2006). Antecedents and consequences of employee engagement. Journal of Managerial Psychology, 21(7), 600-619.\nPhillips, J. J. (2016). Handbook of training evaluation and measurement methods (4th ed.). Routledge.\nMobley, W. H., Horner, S. O., & Hollingsworth, A. T. (1978). An evaluation of precursors of hospital employee turnover. Journal of Applied Psychology, 63(4), 408-414.\n[5] Arsham, H. (2019). Statistical thinking for management (2nd ed.). Prentice-Hall.\nHogg, R. V., McKean, J. W., & Craig, A. T. (2019). Introduction to mathematical statistics (8th ed.). Pearson Education.\nSullivan, L. M., & Artino Jr, A. R. (2013). Analyzing and interpreting data from Likert-type scales. Journal of Graduate Medical Education, 5(4), 541-542.\n[6] Arsham, H. (2019). Statistical thinking for management (2nd ed.). Prentice-Hall.\nHogg, R. V., McKean, J. W., & Craig, A. T. (2019). Introduction to mathematical statistics (8th ed.). Pearson Education.\n[7] Venables, W. N., & Ripley, B. D. (2002). Modern applied statistics with S (4th ed.). Springer.\nR Core Team. (2021). R: A language and environment for statistical computing. R Foundation for Statistical Computing. https://www.R-project.org/\n[8] Arsham, H. (2019). Statistical thinking for management (2nd ed.). Prentice-Hall.\nHogg, R. V., McKean, J. W., & Craig, A. T. (2019). Introduction to mathematical statistics (8th ed.). Pearson Education.\nKirk, R. E. (2013). Experimental design: Procedures for the behavioral sciences (4th ed.). Sage."
  },
  {
    "objectID": "02ZTests.html#overview-of-z-tests",
    "href": "02ZTests.html#overview-of-z-tests",
    "title": "Z Tests",
    "section": "",
    "text": "We will begin with the Z-test.\nThe Z-test for proportions is a statistical method used to compare observed proportions, such as the proportion of successes in two groups, to see if there is a significant difference between them. This test is particularly useful when dealing with large sample sizes.\nDepending on the research question and the type of data being analyzed, different types of Z-tests are frequently employed in statistics. Variations of the Z-test can be used to compare sample means or sample proportions. In this chapter, we will confine our attention to the Z-tests used to analyze categorical data and sample proportions.\nThe following are three prevalent types of Z-tests for Categorical data:\n\nOne-sample Z-test: This test determines whether a sample proportion significantly deviates from a known or hypothesized population mean or proportion.\nTwo-sample Z-test: This test is used to determine whether there is a statistically significant difference between the proportions of two independent groups.\nPaired Z-test for proportions: This test determines whether there is a statistically significant difference in proportions between two related groups, such as before and after a treatment or intervention. [1]"
  },
  {
    "objectID": "02ZTests.html#business-applications-of-z-tests-for-categorical-data",
    "href": "02ZTests.html#business-applications-of-z-tests-for-categorical-data",
    "title": "Z Tests",
    "section": "",
    "text": "In marketing research and analysis, Z-tests for categorical data are useful tools for testing hypotheses and determining the significance of relationships between categorical variables. Here are some possible marketing applications of Z-tests for categorical data:\n\nBrand loyalty: A marketer may wish to determine whether customers who purchase a particular brand are more loyal to the brand based on repeat purchase, than those who purchase a competing brand. To compare the proportion of loyal customers between the two brands, a two-sample Z-test for proportions could be utilized.\nCustomer satisfaction: A marketer may wish to determine whether a new product or service is more satisfying to customers than its predecessor. Before and after the introduction of the new product or service, the proportion of satisfied customers could be compared.\nMarket segmentation: A marketer may use market segmentation to determine whether there are significant differences in customer demographics or behavior across market segments. A test could be utilized to determine if there is a significant relationship between the demographic variables and market segments.\nAdvertising effectiveness: A marketer may wish to determine if one advertising campaign is more effective than another at reaching the target audience. A two-sample Z-test for proportions could be used to compare the proportion of viewers who remember the message or take action following exposure to each campaign.\nProduct preference: A marketer may wish to determine whether a particular product feature or characteristic is more preferred by customers than another. A one-sample Z-test for proportions could be utilized to determine if the proportion of customers who prefer the feature significantly deviates from the null hypothesis. [2]\n\n\n\n\nZ tests can be helpful in studying different aspects of Finance such as default rates, investment decisions, stock market analysis, risk analysis.\n\nCredit risk assessment: A financial institution assessing credit risk may wish to determine whether the proportion of delinquent loans varies significantly across risk categories. A two-sample Z-test for proportions could be utilized to compare the proportion of delinquent loans across various risk categories.\nInvestment portfolio analysis: An investor may wish to determine whether the proportion of winning trades varies significantly between different types of investments, including stocks and bonds. Using a two-sample Z-test for proportions, it is possible to compare the proportion of profitable trades for the various types of investments.\nCustomer behavior analysis: A financial institution may conduct an analysis of customer behavior to determine whether the proportion of customers who use online banking varies significantly by age group. A Chi-square test could be utilized to determine if there is a significant relationship between the demographic variable (age group) and the proportion of customers who use online banking.\nMarket analysis: An investment bank may wish to determine if there is a significant correlation between the market type (bullish or bearish) and the proportion of investors who sell or purchase stocks. A Chi-square test could be used to determine if there is a significant relationship between the type of market and the proportion of investors who sell or purchase stocks.\nFraud detection: A financial institution may wish to determine whether the proportion of fraudulent transactions varies significantly between different types of transactions, such as ATM withdrawals, online purchases, and wire transfers, for the purpose of detecting fraud. A one-sample Z-test for proportions could be used to determine if the proportion of fraudulent transactions in a specific type of transaction significantly deviates from a null hypothesis. [3]\n\n\n\n\nIn organizational behavior research, Z-tests for categorical data can be useful tools for analyzing data and making informed decisions. Here are some potential organizational behavior applications of Z-tests for categorical data:\n\nEmployee satisfaction: An organization may wish to determine whether the proportion of satisfied employees varies significantly across departments. Using a two-sample Z-test for proportions, it is possible to compare the percentage of satisfied employees across departments.\nDiversity and inclusion: An organization may wish to determine whether the proportion of employees from diverse backgrounds varies significantly across management levels. A Chi-square test could be used to determine whether there is a significant relationship between the demographic variable (race, gender, etc.) and the management level.\nEmployee engagement: An organization may wish to determine whether the proportion of engaged employees differs significantly across teams or work units. Using a two-sample Z-test for proportions, it is possible to compare the proportion of engaged employees among various teams or work units.\nTraining effectiveness: An organization may wish to determine whether the percentage of employees who pass a certification exam differs significantly between those who have completed a training program and those who have not. The proportion of employees who pass the certification exam could be compared between the two groups using a two-sample Z-test for proportions.\nEmployee turnover: An organization may wish to determine whether the proportion of employees who voluntarily leave the organization varies significantly across job functions or departments. A Chi-square test could be used to determine whether there is a significant relationship between the job function or department and the percentage of employees who leave the organization voluntarily. [4]"
  },
  {
    "objectID": "02ZTests.html#one-sample-z-test-for-proportions",
    "href": "02ZTests.html#one-sample-z-test-for-proportions",
    "title": "Z Tests",
    "section": "",
    "text": "This type of Z-test is used to examine a population proportion hypothesis.\nFor instance, a business may wish to determine whether the proportion of customers who are satisfied with their product exceeds 0.5.\nThe company could collect a sample of customers, determine the proportion of satisfied customers in the sample, and use a one-sample Z-test to determine if the proportion is significantly different from 0.5.\n\n\n\n\nThe general steps to perform a one-sample Z-test are:\n\nState the null hypothesis and alternative hypothesis: Typically, the null hypothesis states that the sample proportion matches the hypothesized population proportion, whereas the alternative hypothesis states that the sample proportion differs from the hypothesized population proportion.\nDetermine the level of significance (α): This is the maximum probability of rejecting the null hypothesis when it is true. Typical values for α are 0.05 or 0.01.\nCollect sample data: Obtain a random sample from the population and calculate the sample proportion \\((p)\\).\nCalculate the test statistic: The test statistic is calculated as follows: \\(z = (p_0 - p) / \\sqrt{(p * (1 - p) / n)}\\) , where \\(p\\) is the population proportion assumed under the null hypothesis, and \\(n\\) is the sample size.\nDetermine the critical value: The critical value is the value of \\(z\\) that corresponds to the chosen level of significance and degrees of freedom \\((df = n-1)\\).\nCompare the test statistic to the critical value: If the absolute value of the test statistic is greater than the critical value, we reject the null hypothesis. Otherwise, we fail to reject the null hypothesis.\nInterpret the results: If the null hypothesis is rejected, one can conclude that the proportion of the sample is significantly different from that of the population. If the null hypothesis is not rejected, it can be concluded that the sample proportion is not significantly different from the population proportion.\nDetermine the p-value using a Z-table or calculator, and compare the p-value to the level of significance (alpha) in order to reach a conclusion.\nDraw a conclusion. [5]\n\n\n\n\n\n\nResearch Objective:\nA researcher wants to determine if the proportion of adults in a population who own a car significantly deviates from 70%. A random sample of 200 adults shows that 150 of them own a car.\nHypotheses:\n\nNull Hypothesis (H0): p = 0.7\nAlternative Hypothesis (Ha): p ≠ 0.7\nHere, p is the population proportion.\n\nSignificance Level:\nThe significance level is set at 0.05.\nCritical Value:\nThe critical value for a two-tailed test at the 0.05 significance level is ±1.96.\nTest Statistic Formula:\nThe test statistic (Z) is calculated using the formula: \\[ Z = \\frac{p_0 - p}{\\sqrt{p \\cdot (1 - p) / n}} \\]\n\np is the hypothesized population proportion.\np_0 is the sample proportion.\nn is the sample size.\n\nCalculation:\n\nSample proportion (p_0): 0.75 (150 out of 200)\nHypothesized population proportion (p): 0.7\nSample size (n): 200\nThe Z-score is calculated as approximately 1.54.\n\n\nVerify by plugging in the values: \\[ Z = \\frac{0.75 - 0.7}{\\sqrt{0.7 \\cdot (1 - 0.7) / 200}} \\]\nThe calculated Z-score is approximately 1.54.\n\nDecision Rule:\n\nCompare the absolute value of the test statistic with the critical value. If it’s greater than 1.96, reject the null hypothesis.\n\nConclusion:\n\nSince the absolute value of the test statistic (1.54) is less than the critical value (1.96), we fail to reject the null hypothesis. There is not enough statistical evidence to conclude that the proportion of adults who own a car in the population is significantly different from 70%. [6]\n\n\n\n\n\n\nConsider the mtcars data set. Suppose we want to test whether the proportion of cars with a 6-cylinder engine (cyl=6) is significantly different from a hypothesized value of 50%.\n\n\nNull hypothesis: The proportion of 6-cylinder engine cars in the mtcars dataset is 0.5 (p = 0.5).\nAlternative hypothesis: The proportion of 6-cylinder engine cars in the mtcars dataset is not equal to 0.5 (p ≠ 0.5).\n\n\n# Load the mtcars dataset\ndata(mtcars)\n\n# Create a binary variable indicating whether a car has a 6-cylinder engine or not\nmtcars$is_cyl6 &lt;- ifelse(mtcars$cyl == 6, 1, 0)\n\n# Count the number of 6 cylinder cars\nn6 &lt;- sum(mtcars$is_cyl6)\n\n# Count the total number of cars\nn &lt;- length(mtcars$is_cyl6)\n  # alternate code\n  # n &lt;- nrow(mtcars)\n\n# Calculate the sample proportion of cars with a 6-cylinder engine\nprop_cyl6 &lt;- mean(mtcars$is_cyl6)\n\n# Conduct a one-sample Z-test for proportions using the prop.test function\nprop.test(n6, \n          n, \n          p = 0.5)\n\n\n    1-sample proportions test with continuity correction\n\ndata:  n6 out of n, null probability 0.5\nX-squared = 9.0312, df = 1, p-value = 0.002654\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.09944097 0.40441815\nsample estimates:\n      p \n0.21875 \n\n\n\nThe sample proportion is 0.21875, indicating that only 21.875% of the cars in the dataset are equipped with a six-cylinder engine.\nUsing the prop.test function, we can now perform a one-sample Z-test on proportions:\nThe prop.test function requires three arguments:\nthe number of successes (in this case, the number of cars with a 6-cylinder engine),\nthe total number of trials (in this case, the total number of cars), and\nthe population proportion hypothesis (0.5 in this case)\nThe output reveals that the test statistic is X-squared = 9.0312 with a p-value of 0.002654 and one degree of freedom.\nSince the p-value is less than the significance level (0.05), we reject the null hypothesis and conclude that the proportion of cars in the population with a 6-cylinder engine is significantly different from 50%.\nThe sample proportion estimate is 0.21875, with a 95% confidence interval encompassing this value (0.099, 0.404). [7]"
  },
  {
    "objectID": "02ZTests.html#independent-samples-z-test-for-proportions",
    "href": "02ZTests.html#independent-samples-z-test-for-proportions",
    "title": "Z Tests",
    "section": "",
    "text": "The independent samples Z-test for proportions is a statistical test used to determine if there is a statistically significant difference between two sample proportions drawn from independent populations.\nA company may wish to determine, for instance, whether the proportion of male customers who purchase their product differs significantly from the proportion of female customers who purchase their product.\nThe company could collect separate samples of male and female customers, calculate the proportion of customers in each sample who purchased the product, and use a Z-test for independent samples to determine whether the difference in proportions is statistically significant. [8]\n\n\n\nHere are the steps involved in conducting an independent samples Z-test for proportions:\n\nState the null and alternative hypotheses: The null hypothesis asserts that the two population proportions are equal, while the alternative hypothesis asserts that they are not.\nDetermine the level of significance. This is the level of risk the researcher is willing to accept when deciding on the null hypothesis. Typically, the significance level is set at 0.05.\nDetermine the test’s statistic: The test statistic for a Z-test for proportions using independent samples is calculated using the following formula.\nZ = (p1 - p2) / sqrt(p * (1 - p) * (1 / n1 + 1 / n2))\nwhere p1 and p2 are sample proportions, n1 and n2 are sample sizes, and p is the pooled sample proportion, which is calculated as follows:\np = (x1 + x2) / (n1 + n2)\nwhere x1 and x2 represent the success rate of each sample.\nThe critical value is the value of the test statistic that corresponds to the level of significance and the degrees of freedom.\nComparing the test statistic to the threshold: The null hypothesis is rejected if the absolute value of the test statistic is greater than the critical value. If not, the null hypothesis cannot be rejected.\nOn the basis of the test results, the researcher can draw a conclusion regarding whether the two population proportions differ significantly. [8]\n\n\n\n\n\nSuppose a researcher is interested in comparing the proportion of men and women who purchase a particular product. A random sample of 200 men and 300 women reveals that 100 men and 150 women bought the product.\nThe null hypothesis states that the proportion of men and women who purchase the product is equal, whereas the alternative hypothesis states that the proportion is not equal. Hence, we have:\n\nH0: p1 = p2\nHa: p1 ≠ p2\n\nwhere p1 represents the percentage of men who buy the product and p2 represents the percentage of women who buy the product.\nNext, we establish the significance level, which is typically set at 0.05.\n1.96 is the critical value for a two-tailed test with a significance level of 0.05.\nNow we can calculate the test statistic with the following formula:\nZ = (p1 - p2) / sqrt(p * (1 - p) * (1 / n1 + 1 / n2))\nwhere p is the proportion of the pooled sample calculated as:\np = (x1 + x2) / (n1 + n2)\nwhere x1 and x2 represent the success rates for each sample and n1 and n2 represent the sample sizes.\nUsing the data provided, we have:\np1 = 100 / 200 = 0.5\np2 = 150 / 300 = 0.5\nn1 = 200\np = (100 + 150) / (200 + 300) = 0.4167\nThe test statistic is thus:\nZ = (0.5 - 0.5) / sqrt(0.4167 * (1 - 0.4167) * (1 / 200 + 1 / 300)) = 0\nDue to the fact that the absolute value of the test statistic is less than the critical value (|0| &lt; 1.96), we fail to reject the null hypothesis and conclude that there is insufficient evidence to suggest that the proportion of men and women who purchase the product differs.\nTherefore, we can conclude with 95% level of confidence that the proportion of men and women who purchase the product does not differ significantly.\n\n\n\n\n\nSuppose we wish to compare the proportion of cars in the mtcars dataset with automatic transmission (am = 1) to those with V-shaped engines (vs = 1).\n\n\n# Load the mtcars dataset\ndata(mtcars)\n\n# Create two vectors representing the two samples\nsample1 &lt;- mtcars$am # Sample 1: cars with automatic transmission\nsample2 &lt;- mtcars$vs # Sample 2: cars with v-shaped engines\n\n# Conduct a two-sample test for equality of proportions\nprop.test(x = c(sum(sample1 == 1), sum(sample2 == 1)),\n          n = c(length(sample1), length(sample2)),\n          alternative = \"two.sided\")\n\n\n    2-sample test for equality of proportions with continuity correction\n\ndata:  c(sum(sample1 == 1), sum(sample2 == 1)) out of c(length(sample1), length(sample2))\nX-squared = 0, df = 1, p-value = 1\nalternative hypothesis: two.sided\n95 percent confidence interval:\n -0.3043652  0.2418652\nsample estimates:\n prop 1  prop 2 \n0.40625 0.43750 \n\n\n\nThe code generates two vectors, sample1 and sample2, representing the two samples for which the proportion of successes is to be compared.\nIn this example, sample1 represents the percentage of automobiles with automatic transmission (am = 1) and sample2 represents the percentage of automobiles with V-shaped engines (vs = 1).\nThe final code block uses the prop.test() function to conduct a two-sample test for proportional equality. x is a vector of the counts of successes (cars with automatic transmission or cars with v-shaped engines) for each sample, while n is a vector of the total number of trials (the total number of cars) for each sample.\nThe alternative argument specifies the two-sidedness of the alternative hypothesis.\nThe function prop.test() returns the test statistic, degrees of freedom, p-value, and confidence interval as its output.\nThe null hypothesis states that the proportion of automobiles with automatic transmissions and those with V-shaped engines are equal. If the p-value is less than the significance level (typically 0.05), we can reject the null hypothesis and conclude that the proportions of successes in the two samples differ significantly.\nThe output includes the test name (Two-sample test for equality of proportions with continuity correction), the sample proportions (prop 1 and prop 2), the test statistic (X-squared), the degrees of freedom (df), the p-value (p-value), the alternative hypothesis, and the 95% confidence interval (95 percent confidence interval).\nIn this instance, the p-value is less than 0.05, indicating that we can reject the null hypothesis of equal proportions and conclude that the proportions of cars with automatic transmission and V-shaped engines are significantly different.\nThe sample proportion estimates for am and vs are respectively 0.666667 and 0.1250000. The 95% confidence interval for the difference in proportions is (0.09437123, 0.31162877), which indicates that we are 95% confident that the true difference in proportions lies within this interval. [8]"
  },
  {
    "objectID": "02ZTests.html#references",
    "href": "02ZTests.html#references",
    "title": "Z Tests",
    "section": "",
    "text": "[1] Agresti, A. (2002). Categorical data analysis (2nd ed.). Wiley.\n[2] Hair Jr, J. F., Black, W. C., Babin, B. J., & Anderson, R. E. (2010). Multivariate data analysis: A global perspective (7th ed.). Pearson Education.\nChurchill Jr, G. A., & Iacobucci, D. (2010). Marketing research: Methodological foundations (10th ed.). Cengage Learning.\nKotler, P., & Keller, K. L. (2012). Marketing management (14th ed.). Pearson Education.\nBabin, B. J., & Zikmund, W. G. (2016). Essentials of marketing research (5th ed.). Cengage Learning.\nMalhotra, N. K., & Peterson, M. (2006). Basic marketing research (3rd ed.). Prentice-Hall.\n[3] Altman, E. I. (2008). Evaluating credit risk models. Journal of Banking & Finance, 32(8), 1544-1558.\nNofsinger, J. R. (2012). The psychology of investing (5th ed.). Pearson Education.\nMitra, A., & Gilbert, T. (2012). Banking on customer knowledge: The potential of customer relationship management and analytics in retail banking. Journal of Service Research, 15(1), 59-75.\nFrench, K. R., Schwert, G. W., & Stambaugh, R. F. (1987). Expected stock returns and volatility. Journal of Financial Economics, 19(1), 3-29.\nTsay, R. S., & Chen, T. H. (2012). Financial time series analysis (2nd ed.). Wiley.\n[4] Judge, T. A., & Watanabe, S. (1994). Another look at the job satisfaction-life satisfaction relationship. Journal of Applied Psychology, 79(6), 939-948.\nRoberson, Q. M. (2019). Race and ethnicity in the workplace. Annual Review of Organizational Psychology and Organizational Behavior, 6, 413-438.\nSaks, A. M. (2006). Antecedents and consequences of employee engagement. Journal of Managerial Psychology, 21(7), 600-619.\nPhillips, J. J. (2016). Handbook of training evaluation and measurement methods (4th ed.). Routledge.\nMobley, W. H., Horner, S. O., & Hollingsworth, A. T. (1978). An evaluation of precursors of hospital employee turnover. Journal of Applied Psychology, 63(4), 408-414.\n[5] Arsham, H. (2019). Statistical thinking for management (2nd ed.). Prentice-Hall.\nHogg, R. V., McKean, J. W., & Craig, A. T. (2019). Introduction to mathematical statistics (8th ed.). Pearson Education.\nSullivan, L. M., & Artino Jr, A. R. (2013). Analyzing and interpreting data from Likert-type scales. Journal of Graduate Medical Education, 5(4), 541-542.\n[6] Arsham, H. (2019). Statistical thinking for management (2nd ed.). Prentice-Hall.\nHogg, R. V., McKean, J. W., & Craig, A. T. (2019). Introduction to mathematical statistics (8th ed.). Pearson Education.\n[7] Venables, W. N., & Ripley, B. D. (2002). Modern applied statistics with S (4th ed.). Springer.\nR Core Team. (2021). R: A language and environment for statistical computing. R Foundation for Statistical Computing. https://www.R-project.org/\n[8] Arsham, H. (2019). Statistical thinking for management (2nd ed.). Prentice-Hall.\nHogg, R. V., McKean, J. W., & Craig, A. T. (2019). Introduction to mathematical statistics (8th ed.). Pearson Education.\nKirk, R. E. (2013). Experimental design: Procedures for the behavioral sciences (4th ed.). Sage."
  },
  {
    "objectID": "07LinearRegression.html",
    "href": "07LinearRegression.html",
    "title": "Linear Regression.",
    "section": "",
    "text": "July 26, 2023\n\nLinear regression is a statistical method for determining the relationship between one or more independent variables and a dependent variable. It is a simple and powerful data modeling and analysis tool that can assist in making predictions or identifying trends.\nThe primary goal of linear regression is to identify the best-fitting line or plane that describes the relationship between the independent and dependent variables. The regression line or plane represents the predicted value of the dependent variable given a particular value of the independent variable.\nLinear regression, in its most basic form, involves fitting a straight line to a set of data points. When there is only one independent variable, this is known as Simple Linear Regression. When there are two or more independent variables, it is known as Multiple Linear Regression.\nLinear regression makes several data assumptions, including that the relationship between the independent and dependent variables is linear, the errors or residuals are normally distributed, and there is no multicollinearity (high correlation) among the independent variables.\nLinear regression can be used for a variety of purposes, including forecasting sales, predicting future trends, and estimating the impact of various variables on a specific outcome. It is widely used in a variety of fields, including economics, marketing, finance and social sciences. [1]\n\n\n\n\n\n\nSales forecasting: Using historical data and other factors such as pricing, advertising, and promotional activities, linear regression can be used to forecast future sales. This can assist marketers in optimizing their marketing mix and increasing sales performance.\nCustomer segmentation: Linear regression can be used to analyze customer groups based on demographic, psychographic, and behavioral characteristics. Marketers can use this information to tailor their marketing messages and offerings to specific customer segments.\nProduct development: Linear regression can be used to characterize the product features and attributes that are most important to customers and then optimize the product design and development process accordingly.\nAdvertising effectiveness: Multiple linear regression can be used to measure the effectiveness of advertising campaigns and to identify the most effective advertising messages and media channels for reaching and persuading target customers. [2]\n\n\n\n\n\nPortfolio management: Linear regression can be used to examine the relationship between various variables such as interest rates, inflation, and market volatility, as well as to optimize portfolio allocation and risk management strategies.\nCredit scoring: Using Linear regression, credit scoring models can be created that predict the likelihood of default based on factors such as credit history, income, and employment status.\nAsset pricing: Linear regression can be used to estimate the fair value of assets such as stocks, bonds, and options by analyzing the relationship between various factors such as earnings, dividends, and market trends.\nFinancial forecasting: Using historical data and other factors such as macroeconomic indicators and industry trends, Linear regression can be used to forecast financial outcomes such as revenue, earnings, and cash flow.\nRisk management: Linear regression can be used to analyze the relationship between different risk factors, such as interest rates, exchange rates, and commodity prices, and to develop risk management strategies that reduce exposure to these risks. [3]\n\n\n\n\nLinear Regression can be used to investigate various aspects of organizational behavior, such as employee performance, workforce diversity, leadership effectiveness, organizational culture, and employee well-being.\n\nEmployee performance: Linear regression can be used to investigate the relationship between various factors like job satisfaction, motivation, and training and employee performance outcomes like productivity, job satisfaction, and turnover.\nWorkforce diversity: Linear regression can be used to examine the relationship between demographic diversity, cultural diversity, and social diversity and organizational outcomes such as creativity, innovation, and problem-solving.\nLeadership effectiveness: Linear regression can be used to examine the relationship between various factors like leadership style, communication, and decision-making and leadership outcomes like employee motivation and job satisfaction.\nOrganizational culture: Linear regression can be used to investigate the relationship between organizational values, norms, and beliefs and organizational outcomes such as employee engagement, retention, and performance.\nEmployee well-being: Multiple linear regression can be used to examine the relationship between work-life balance, social support, and job demands and employee well-being outcomes such as stress, burnout, and health. [4]"
  },
  {
    "objectID": "07LinearRegression.html#business-applications-of-linear-regression",
    "href": "07LinearRegression.html#business-applications-of-linear-regression",
    "title": "Linear Regression.",
    "section": "",
    "text": "Sales forecasting: Using historical data and other factors such as pricing, advertising, and promotional activities, linear regression can be used to forecast future sales. This can assist marketers in optimizing their marketing mix and increasing sales performance.\nCustomer segmentation: Linear regression can be used to analyze customer groups based on demographic, psychographic, and behavioral characteristics. Marketers can use this information to tailor their marketing messages and offerings to specific customer segments.\nProduct development: Linear regression can be used to characterize the product features and attributes that are most important to customers and then optimize the product design and development process accordingly.\nAdvertising effectiveness: Multiple linear regression can be used to measure the effectiveness of advertising campaigns and to identify the most effective advertising messages and media channels for reaching and persuading target customers. [2]\n\n\n\n\n\nPortfolio management: Linear regression can be used to examine the relationship between various variables such as interest rates, inflation, and market volatility, as well as to optimize portfolio allocation and risk management strategies.\nCredit scoring: Using Linear regression, credit scoring models can be created that predict the likelihood of default based on factors such as credit history, income, and employment status.\nAsset pricing: Linear regression can be used to estimate the fair value of assets such as stocks, bonds, and options by analyzing the relationship between various factors such as earnings, dividends, and market trends.\nFinancial forecasting: Using historical data and other factors such as macroeconomic indicators and industry trends, Linear regression can be used to forecast financial outcomes such as revenue, earnings, and cash flow.\nRisk management: Linear regression can be used to analyze the relationship between different risk factors, such as interest rates, exchange rates, and commodity prices, and to develop risk management strategies that reduce exposure to these risks. [3]\n\n\n\n\nLinear Regression can be used to investigate various aspects of organizational behavior, such as employee performance, workforce diversity, leadership effectiveness, organizational culture, and employee well-being.\n\nEmployee performance: Linear regression can be used to investigate the relationship between various factors like job satisfaction, motivation, and training and employee performance outcomes like productivity, job satisfaction, and turnover.\nWorkforce diversity: Linear regression can be used to examine the relationship between demographic diversity, cultural diversity, and social diversity and organizational outcomes such as creativity, innovation, and problem-solving.\nLeadership effectiveness: Linear regression can be used to examine the relationship between various factors like leadership style, communication, and decision-making and leadership outcomes like employee motivation and job satisfaction.\nOrganizational culture: Linear regression can be used to investigate the relationship between organizational values, norms, and beliefs and organizational outcomes such as employee engagement, retention, and performance.\nEmployee well-being: Multiple linear regression can be used to examine the relationship between work-life balance, social support, and job demands and employee well-being outcomes such as stress, burnout, and health. [4]"
  },
  {
    "objectID": "07LinearRegression.html#overview",
    "href": "07LinearRegression.html#overview",
    "title": "Linear Regression.",
    "section": "Overview",
    "text": "Overview\n\nSimple linear regression is used to model the relationship between a dependent variable and a single independent variable.\nThe goal of simple linear regression is to find the best fit line.\nThe best fit line is determined by minimizing the sum of the squared distances between the actual and predicted values of the dependent variable based on the independent variable.\nThe independent variable is plotted on the x-axis, and the dependent variable is plotted on the y-axis, to create a scatter plot and visualize the best fit line. [5]"
  },
  {
    "objectID": "07LinearRegression.html#model-of-simple-linear-regression",
    "href": "07LinearRegression.html#model-of-simple-linear-regression",
    "title": "Linear Regression.",
    "section": "Model of Simple Linear Regression",
    "text": "Model of Simple Linear Regression\n\nThe model equation for a simple linear regression model is:\n\n\\[\\begin{equation}\n\\label{eq:slr}\ny = β_0 + β_1x + ε\n\\end{equation}\\]\n\n\\(y\\) is the dependent variable\n\\(x\\) is the independent variable\n\\(β_0\\) is the intercept (the value of \\(y\\) when \\(x = 0\\))\n\\(β_1\\) is the slope (the change in y for a one-unit change in x)\n\\(ε\\) is the error term (represents the random variability in the data)\n\n\nThe goal of the simple linear regression model shown in (\\(\\ref{eq:slr}\\)), is to estimate the values of \\(β_0\\) and \\(β_1\\) given that minimize the sum of squared errors (SSE) between the observed values of \\(y\\) and the predicted values of \\(y\\).\nThere are several methods for estimating the values of \\(β_0\\) and \\(β_1\\), the most common of which is the called Ordinary Least Squares (OLS).\nThe Ordinary Least Squares (OLS) method calculates the values of\n\\(β_0\\) and \\(β_1\\) that minimize the sum of squared errors (SSE) between the observed and predicted values of \\(y\\).\nAfter estimating the values of \\(β_0\\) and \\(β_1\\), the model can be used to predict the value of \\(y\\), for any given value of \\(x\\). [6]"
  },
  {
    "objectID": "07LinearRegression.html#running-simple-linear-regression-in-r",
    "href": "07LinearRegression.html#running-simple-linear-regression-in-r",
    "title": "Linear Regression.",
    "section": "Running Simple Linear Regression in R",
    "text": "Running Simple Linear Regression in R\n\nGoal\n\nConsider the mtcars data. Suppose we need to fit a simple linear regression model to predict mpg (miles per gallon) based on the predictor variable: wt (weight).\nWe would like to estimate it using Ordinary Least Squares.\n\n\n\nSteps\n\n# load mtcars dataset\ndata(mtcars)\n\n# fit a simple linear regression model\nmodel0 &lt;- lm(mpg ~ wt, data = mtcars)\n\n# print the model summary\nsummary(model0)\n\n\nCall:\nlm(formula = mpg ~ wt, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.5432 -2.3647 -0.1252  1.4096  6.8727 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  37.2851     1.8776  19.858  &lt; 2e-16 ***\nwt           -5.3445     0.5591  -9.559 1.29e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.046 on 30 degrees of freedom\nMultiple R-squared:  0.7528,    Adjusted R-squared:  0.7446 \nF-statistic: 91.38 on 1 and 30 DF,  p-value: 1.294e-10\n\n\n\nUsing the lm() function in R, we can fit a simple linear regression model in which the dependent variable (mpg) is regressed on the independent variable (wt).\nThe data argument specifies the dataset, and mpg ~ wt specifies the linear regression model formula, where mpg is the dependent variable and wt is the independent variable.\nThe summary() function is called to print the model’s results. The output gives the estimated coefficients, standard errors, t-statistics, and p-values for each variable in the model, as well as the overall R-squared value and other statistics related to model fit.\nSubstituting the values of \\(\\beta\\) coefficients from the above regression output, we can write the model as follows.\n\n\\[\\begin{equation}\n\\label{eq:slr2}\nmpg = 37.2851 -5.3445 wt + ε\n\\end{equation}\\]"
  },
  {
    "objectID": "07LinearRegression.html#visualizing-simple-linear-regression-in-r",
    "href": "07LinearRegression.html#visualizing-simple-linear-regression-in-r",
    "title": "Linear Regression.",
    "section": "Visualizing Simple Linear Regression in R",
    "text": "Visualizing Simple Linear Regression in R\n\nGoal\n\nSuppose we need to visualize the above simple linear regression model to predict mpg (miles per gallon) based on the predictor variable: wt (weight).\nOne way of doing this is to create a scatter plot.\n\n\n\nSteps\n\n# plot the data and the regression line\nplot(mtcars$wt, \n     mtcars$mpg, \n     main = \"Simple Linear Regression\", \n     xlab = \"Weight (in 1000 lbs)\", \n     ylab = \"Miles per Gallon\")\nabline(model0, col = \"red\")\n\n\n\n\n\nWe create a scatter plot of the data using the plot() function, where the x-axis represents the weight of the car in thousands of lbs, and the y-axis represents the miles per gallon.\nWe also use the main and xlab, ylab arguments to add a title and axes labels.\nFinally, we use the abline() function to add the regression line to the plot, which takes the model object as an argument and plots the regression line. Using the col argument, we change the color of the line.\nThis code generates a scatter plot of the data with the regression line overlaid on top, allowing us to easily visualize the relationship between the dependent and independent variables. [7]"
  },
  {
    "objectID": "07LinearRegression.html#overview-1",
    "href": "07LinearRegression.html#overview-1",
    "title": "Linear Regression.",
    "section": "Overview",
    "text": "Overview\n\nMultiple linear regression is a statistical technique used to model the relationship between a dependent variable and multiple independent variables.\nThe goal of multiple linear regression is to find the linear equation that best explains the relationship between the dependent variable and the independent variables. [8]"
  },
  {
    "objectID": "07LinearRegression.html#model-of-multiple-linear-regression",
    "href": "07LinearRegression.html#model-of-multiple-linear-regression",
    "title": "Linear Regression.",
    "section": "Model of Multiple Linear Regression",
    "text": "Model of Multiple Linear Regression\n\nThe model equation for a multiple linear regression model with p independent variables is:\n\n\\[\\begin{equation}\n\\label{eq:mlr}\ny = β_0 + β_1x_1 + β_2x_2 + … + β_px_p + ε\n\\end{equation}\\]\n\n\\(y\\) is the dependent variable\n\\(x_1\\), \\(x_2\\), …, \\(x_p\\) are the independent variables\n\\(β_0\\) is the intercept (the value of \\(y\\) when all independent variables are 0)\n\\(β_1\\), \\(β_2\\), …, \\(β_p\\) are the coefficients that represent the change in \\(y\\) for a one-unit change in each independent variable, holding all other variables constant\n\\(ε\\) is the error term (represents the random variability in the data)\n\nThe model given in (\\(\\ref{eq:mlr}\\)) is the most popular regression model."
  },
  {
    "objectID": "07LinearRegression.html#ordinary-least-squares-ols-estimation",
    "href": "07LinearRegression.html#ordinary-least-squares-ols-estimation",
    "title": "Linear Regression.",
    "section": "Ordinary Least Squares (OLS) Estimation",
    "text": "Ordinary Least Squares (OLS) Estimation\n\nOrdinary Least Squares (OLS) regression is the most widely used statistical estimation method for multiple linear regression.\nOLS estimates the values of \\(β_0\\), \\(β_1\\), \\(β_2\\), …, \\(β_p\\) that minimize the sum of squared errors (SSE) between the observed values of \\(y\\) and the predicted values of \\(y\\).\nOnce the values of \\(β_0\\), \\(β_1\\), \\(β_2\\), …, \\(β_p\\) have been estimated, the model can be used to predict the value of \\(y\\) for any given values of \\(x_1\\), \\(x_2\\), …, \\(x_p\\). [8]"
  },
  {
    "objectID": "07LinearRegression.html#running-multiple-linear-regression-using-ols-in-r",
    "href": "07LinearRegression.html#running-multiple-linear-regression-using-ols-in-r",
    "title": "Linear Regression.",
    "section": "Running Multiple Linear Regression using OLS in R",
    "text": "Running Multiple Linear Regression using OLS in R\n\nGoal\n\nConsider the mtcars data. Suppose we need to fit a multiple linear regression model to predict mpg (miles per gallon) based on the predictor variables: disp (displacement), hp (horsepower), and wt (weight) and cyl (number of cylinders).\nWe would like to estimate it using Ordinary Least Squares.\n\n\n\nSteps\n\nAt the outset, we need to ensure that the data types are correct\n\n\n# Load the mtcars dataset\ndata(mtcars)\n\n# `cyl` (number of cylinders) needs to set as a factor\nmtcars$cyl &lt;- as.factor(mtcars$cyl)\n\n# verify that `cyl` is a factor\nstr(mtcars$cyl)\n\n Factor w/ 3 levels \"4\",\"6\",\"8\": 2 2 1 2 3 2 3 1 1 2 ...\n\n\n\n# verify that `mpg`, `disp`, `hp`, `wt` are numeric\nstr(mtcars$mpg)\n\n num [1:32] 21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ...\n\nstr(mtcars$disp)\n\n num [1:32] 160 160 108 258 360 ...\n\nstr(mtcars$hp)\n\n num [1:32] 110 110 93 110 175 105 245 62 95 123 ...\n\nstr(mtcars$wt)\n\n num [1:32] 2.62 2.88 2.32 3.21 3.44 ...\n\n\n\nNow, we can run the multiple linear regression.\n\n\n# Fit a multiple linear regression model\nmodel &lt;- lm(mpg ~ disp + hp + wt + cyl, \n            data = mtcars)\n\n# View the summary of the model\nsummary(model)\n\n\nCall:\nlm(formula = mpg ~ disp + hp + wt + cyl, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.2740 -1.0349 -0.3831  0.9810  5.4192 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 36.002405   2.130726  16.897 1.54e-15 ***\ndisp         0.004199   0.012917   0.325  0.74774    \nhp          -0.023517   0.012216  -1.925  0.06523 .  \nwt          -3.428626   1.055455  -3.248  0.00319 ** \ncyl6        -3.466011   1.462979  -2.369  0.02554 *  \ncyl8        -3.753227   2.813996  -1.334  0.19385    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.482 on 26 degrees of freedom\nMultiple R-squared:  0.8578,    Adjusted R-squared:  0.8305 \nF-statistic: 31.37 on 5 and 26 DF,  p-value: 3.18e-10\n\n\n\nThe lm() function is used to fit the model, and the data argument specifies the mtcars dataset.\nThe model formula specifies that mpg is the response variable and cyl, disp, hp, and wt are the predictor variables. The data argument specifies the dataset that the variables are taken from, which in this case is the mtcars dataset.\nRecall that cyl is a categorical, factor variable that takes three levels \\(cyl = {4, 6, 8}\\) corresponding to four, six and eight cylinder cars.\nR sets the level \\(cyl = 4\\) to be the base level by default. It creates two dummy variables cyl6 and cyl8 corresponding to six and eight cylinder cars. We have, cyl6=1 for six cylinder cars, cyl6=0 otherwise. We have, cyl8=1 for eight cylinder cars, cyl8=0 otherwise.\nThe beta coefficient estimate for cyl6 represents the expected difference in mpg between cars with six cylinders and cars with four cylinders.\nThe equation for the multiple linear regression model to predict mpg (miles per gallon) based on the predictor variables: disp (displacement), hp (horsepower), and wt (weight) and cyl (number of cylinders) is given as follows:\n\n\\[\\begin{equation}\n\\label{eq:mlr2}\nmpg = 36.0024 + 0.0042disp - 0.0235hp - 3.4286 wt - 3.4660 cyl_6 - 3.7532 cyl_8 + ε\n\\end{equation}\\]\n\nThe summary() function is used to view a summary of the model, which includes information such as the coefficients for each predictor variable, the standard errors, t-values, and p-values for each coefficient, the R-squared value, and more. This summary output can be used to interpret the results of the multiple linear regression model. [9]"
  },
  {
    "objectID": "07LinearRegression.html#output-of-multiple-linear-regression-using-ols-in-r",
    "href": "07LinearRegression.html#output-of-multiple-linear-regression-using-ols-in-r",
    "title": "Linear Regression.",
    "section": "Output of Multiple Linear Regression using OLS in R",
    "text": "Output of Multiple Linear Regression using OLS in R\n\nBeta Coefficients Estimates\n\nThe beta coefficient estimate for a predictor variable represents the change in the response variable that is associated with a one-unit increase in that predictor variable, while holding all other predictor variables constant. In other words, it represents the effect that each predictor variable has on the response variable, after controlling for the effects of all other predictor variables in the model.\nIn the output of the multiple linear regression model, the “Coefficients” table shows the estimates of the beta coefficients (also known as regression coefficients or slope coefficients) for each predictor variable in the model. [9]\nWe can use the following code to extract the beta coefficients.\n\n\nbetas = model$coefficients\nround(betas,4)\n\n(Intercept)        disp          hp          wt        cyl6        cyl8 \n    36.0024      0.0042     -0.0235     -3.4286     -3.4660     -3.7532 \n\n\n\nWe can also visualize the beta coefficients. We use coefplot() in the arm package to plot the coefficients and their 95% credible intervals.\n\n\n# Load the arm package\nlibrary(arm)\n\nLoading required package: MASS\n\n\nLoading required package: Matrix\n\n\nLoading required package: lme4\n\n\n\narm (Version 1.13-1, built: 2022-8-25)\n\n\nWorking directory is /cloud/project\n\n# Use coefplot() to plot the coefficients of the model\ncoefplot(model)\n\n\n\n\n\n\nInterpretation of beta coefficient estimates [9]\nExample 1:\n\nThe beta coefficient estimate for disp represents the estimated change in the “mpg” response variable associated with a one-unit increase in the disp predictor variable, while holding all other predictors constant.\nThe beta coefficient estimate for disp is 0.004199. This means that, on average, as the disp variable (engine displacement, in cubic inches) increases by one unit, the predicted mpg value for the car increases by 0.004199 units, while holding all other predictor variables constant.\nHowever, as the p-value for disp is greater than the chosen threshold of 0.05, there is no evidence to suggest that this change is statistically significant at the 5% level, after controlling for the effects of the other predictor variables in the model. Therefore, it may not be appropriate to make any conclusive statements about the relationship between disp and mpg based solely on the beta coefficient estimate for disp in this model.\n\nExample 2:\n\nThe the beta coefficient estimate for wt represents the estimated change in the mpg response variable associated with a one-unit increase in the wt predictor variable, while holding all other predictors constant.\nThe beta coefficient estimate for wt is -3.428626. This means that, on average, as the wt variable (weight of the car in thousands of pounds) increases by one unit, the predicted mpg value for the car decreases by 3.428626 units, while holding all other predictor variables constant.\n\nExample 3:\n\nThe beta coefficient estimate for cyl6 is -3.466011. This means that, on average, cars with six cylinders have a predicted “mpg” value that is 3.466011 lower than cars with four cylinders, after accounting for the effects of the other predictor variables in the model.\nSimilarly, the beta coefficient estimate for cyl8 is -3.753227. This means that, on average, cars with eight cylinders have a predicted mpg value that is 3.753227 lower than cars with four cylinders, after accounting for the effects of the other predictor variables in the model.\n\n\n\nStandard Errors\n\nIn multiple linear regression, the standard error is a measure of the variability of the estimated coefficients of the predictor variables in the model.\nIt reflects the variability of the estimated coefficients due to the random error in the data. It represents the average amount that the estimated coefficients would vary if we fit the same model to different samples of data from the population.\nThe standard error is estimated from the residual standard error of the model, which is a measure of the variability of the residuals or the differences between the predicted and observed values of the response variable.\nA smaller standard error means a more precise estimate of the coefficient, and more likely to accurately reflect the true relationship between the independent variable and the dependent variable. [9]\n\nExample 1:\n\nIn the output of the linear regression model, “Std. Error” of wt refers to the standard error of the estimate for the coefficient of the independent variable wt (weight of the car).\nThe “Std. Error” of wt is given as 1.0554. This means that the estimated coefficient for wt is expected to vary by about 1.0554 units across different samples of data that could have been collected. [9]\n\n\n\nt-values and p-values\n\nThe t-value measures the number of standard errors the estimated coefficient is away from zero. It is calculated by dividing the estimated coefficient by its standard error.\nA higher absolute t-value suggests that the estimated coefficient is more significant and more likely to accurately reflect the true relationship between the independent variable and the dependent variable.\nIt’s important to note that the t-value is used to calculate the p-value for the coefficient estimate. A small p-value indicates that the coefficient is statistically significant and not likely to have occurred by chance.\nThe p-value is the probability of observing a t-value as extreme or more extreme than the observed t-value, assuming that the true coefficient is zero.\nA small p-value indicates that it is unlikely that the observed t-value occurred by chance, and suggests that the true coefficient is not zero (i.e., there is a statistically significant relationship between wt and mpg). [9]\n\nExample 1:\n\nIn the output, the “t-value” of wt is given as -3.248. This means that the estimated coefficient for wt is -3.248 standard errors away from zero.\nA negative t-value indicates that there is an inverse relationship between the independent variable and the dependent variable. Since the t-value is relatively large (in absolute value) and negative, it suggests that the estimated coefficient for wt is statistically significant and has a significant impact on the dependent variable (mpg).\nIn this case, the p-value for wt is 0.00319, indicating that the estimated coefficient for wt is statistically significant and has a significant impact on the dependent variable.\nIn the output, the “t-value” of wt is -3.248, and the degrees of freedom for the t-distribution is 26 (which is the sample size minus the number of parameters estimated in the model).\n\n[9]\n\n\nInterpretation of p-value, Pr(&gt;|t|)\nExample 2:\n\nIn the output, the beta coefficient estimate for disp is 0.004199 with a standard error of 0.012917, a t-value of 0.325, and a p-value of 0.74774.\nThe p-value for disp is greater than the chosen threshold of 0.05, indicating that the beta coefficient estimate for “disp” is not statistically significant at the 5% level. This means that, after controlling for the effects of the other predictor variables in the model, there is no evidence to suggest that the disp variable has a significant linear relationship with the mpg response variable.\nTherefore, the disp variable is not a significant predictor of the mpg response variable in the model. [9]\n\nExample 3:\n\nIn the output, the beta coefficient estimate for wt is -3.428626 with a standard error of 1.055455, a t-value of -4.223, and a p-value of 0.0002.\nThe p-value for wt is less than the chosen threshold of 0.05, indicating that the beta coefficient estimate for wt is statistically significant at the 5% level.\nThis means that, after controlling for the effects of the other predictor variables in the model, there is evidence to suggest that the wt variable has a significant linear relationship with the mpg response variable. [9]\n\nExample 4:\n\nIn the output, the beta coefficient estimate for cyl_6 is -3.466011 with a standard error of 1.462979, a t-value of -2.369, and a p-value of 0.02554. Similarly, the beta coefficient estimate for cyl_8 is -3.753227 with a standard error of 2.813996, a t-value of -1.334, and a p-value of 0.19385.\nThe p-value for cyl_6 is greater than the chosen threshold of 0.05, indicating that the beta coefficient estimate for cyl_6 is not statistically significant at the 5% level.\nThis means that, after controlling for the effects of the other predictor variables in the model, there is no evidence to suggest that the average mpg for cars with six cylinders is significantly different from that for cars with four cylinders.\nOn the other hand, the p-value for cyl_8 is less than 0.05, indicating that the beta coefficient estimate for cyl_8 is statistically significant at the 5% level.\nThis means that, after controlling for the effects of the other predictor variables in the model, there is evidence to suggest that the average mpg for cars with eight cylinders is significantly different from that for cars with four cylinders.\nAlso, the negative beta coefficient estimate for cyl_8 indicates that, on average, cars with eight cylinders have a lower predicted mpg value than cars with four cylinders, while holding all other predictor variables constant. [9]\n\n\n\nConfidence Intervals\n\nIn multiple linear regression, the confidence interval can be interpreted as a range of values that we are x% confident contains the true population coefficient, where x is the chosen level of confidence.\nThe most common level of confidence is 95%, but other levels of confidence, such as 90% or 99%, can also be used.\nThe confidence interval is based on the standard error and the t-distribution, which takes into account the sample size and the degrees of freedom of the model.\nIn the output of the linear regression model, we can calculate the confidence interval for the beta coefficient of wt (weight of the car) at a desired confidence level (e.g., 95%) using the t-distribution.\nThe following code will output the confidence intervals\n\n\n# View the 95% confidence intervals of the estimated coefficients\nconfint(model, level = 0.95)\n\n                  2.5 %       97.5 %\n(Intercept) 31.62263411 40.382174889\ndisp        -0.02235276  0.030750496\nhp          -0.04862749  0.001594483\nwt          -5.59814537 -1.259106203\ncyl6        -6.47320809 -0.458813851\ncyl8        -9.53747788  2.031023933\n\n\n\nIn the output, the confidence intervals of the estimated coefficients are displayed in a table with two columns, “2.5 %” and “97.5 %”.\nFor example, the confidence interval for the estimated coefficient of wt is (-5.985, -1.617), which means that we are 95% confident that the true population coefficient for wt lies within this range.\nIf the confidence interval for a coefficient does not include 0, we can conclude that the corresponding predictor variable is likely to be a significant predictor of the response variable at the 5% level of significance.\nThe formula for the confidence interval in multiple linear regression is: \\(CI = \\beta ± t * SE\\), where $t” represents the t-value and \\(SE\\) represents the Standard Error.\n\n\nthe coefficient estimate is the point estimate of the coefficient,\nthe t-value is the critical value from the t-distribution with \\(n-p-1\\) degrees of freedom, where\n\\(n\\) is the sample size and \\(p\\) is the number of predictor variables in the model,\nthe standard error \\(SE\\) is a measure of the variability of the estimated coefficient\nThe critical value from the t-distribution depends on the level of confidence and the degrees of freedom. For example, for a 95% confidence interval with 25 observations and 3 predictor variables, the critical value is approximately 2.306. [10]"
  },
  {
    "objectID": "07LinearRegression.html#visualizing-multiple-linear-regression-in-r",
    "href": "07LinearRegression.html#visualizing-multiple-linear-regression-in-r",
    "title": "Linear Regression.",
    "section": "Visualizing Multiple Linear Regression in R",
    "text": "Visualizing Multiple Linear Regression in R\n\nGoal\n\nSuppose we need to visualize the above multiple linear regression model to predict mpg (miles per gallon) based on multiple predictor variables.\nThere is no good way of doing this. One limited way of doing this is to create a scatter plot matrix.\n\n\n\nSteps\n\n# Create a scatterplot matrix of the variables\nlibrary(car)\n\nLoading required package: carData\n\n\n\nAttaching package: 'car'\n\n\nThe following object is masked from 'package:arm':\n\n    logit\n\nscatterplotMatrix(~mpg + wt + hp + disp, \n                  data = mtcars)\n\n# Add the regression line to the scatterplot matrix\nabline(model, col = \"blue\")\n\nWarning in abline(model, col = \"blue\"): only using the first two of 6\nregression coefficients\n\n\n\n\n\n\nThis code will create a scatterplot matrix of the variables (mpg, wt, hp, disp) in the mtcars data set, and will add a regression line to the plot to visualize the relationship between mpg, wt, hp,`disp.\nThe scatterplot matrix will show the scatterplots of all three variables, and the regression line will show the predicted relationship between mpg, wt, hp, disp.\nNote that we will need to have the car package installed in order to create the scatterplot matrix. If we don’t have the package installed, we can install it by running install.packages(\"car\")."
  },
  {
    "objectID": "07LinearRegression.html#residuals-and-residual-standard-error",
    "href": "07LinearRegression.html#residuals-and-residual-standard-error",
    "title": "Linear Regression.",
    "section": "Residuals and Residual Standard Error",
    "text": "Residuals and Residual Standard Error\n\nResiduals\n\nIn the output of the linear regression model, “Residuals” refer to the differences between the predicted values of the dependent variable (mpg) and the actual observed values of the dependent variable in the dataset (mtcars).\nThe residuals are important to examine because they can provide information about the accuracy of the model. Ideally, the residuals should be normally distributed around zero, indicating that the model is accurately predicting the values of the dependent variable. If the residuals are not normally distributed or have a pattern, this could indicate that the model is not accurately predicting the values of the dependent variable and may require further refinement.\nIn the above example, the residuals are the differences between the predicted values and actual values of the dependent variable cyl, which cannot be explained by the independent variables (cyl, disp, hp, and wt) included in the model.\nAnalyzing the residuals can help us check whether the assumptions of the multiple linear regression model are met, and can also help us identify outliers or influential observations that may be affecting the model fit.\n\n\n# Review the Residuals\nsummary(model)\n\n\nCall:\nlm(formula = mpg ~ disp + hp + wt + cyl, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.2740 -1.0349 -0.3831  0.9810  5.4192 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 36.002405   2.130726  16.897 1.54e-15 ***\ndisp         0.004199   0.012917   0.325  0.74774    \nhp          -0.023517   0.012216  -1.925  0.06523 .  \nwt          -3.428626   1.055455  -3.248  0.00319 ** \ncyl6        -3.466011   1.462979  -2.369  0.02554 *  \ncyl8        -3.753227   2.813996  -1.334  0.19385    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.482 on 26 degrees of freedom\nMultiple R-squared:  0.8578,    Adjusted R-squared:  0.8305 \nF-statistic: 31.37 on 5 and 26 DF,  p-value: 3.18e-10\n\n\n\nIn the output, the “Residuals” section includes information about the minimum and maximum residuals, as well as the first and third quartiles, the median of the residuals. This information can be used to assess the distribution and potential patterns in the residuals.\nTo extract the residuals from a multiple linear regression model in R, we can use the resid() function on the model object.\n\n\n# Extract the residuals\nresiduals &lt;- resid(model)\n\n\nWe can also create a scatter plot of the residuals against the fitted values (predicted values) to visually inspect whether the assumptions of the model are met:\n\n\n# Plot the residuals against the fitted values\nplot(fitted(model), residuals,\n     xlab = \"Fitted Values\",\n     ylab = \"Residuals\",\n     main = \"Residual Plot\")\n\n\n\n\n\nThis will create a scatter plot of the residuals against the fitted values In this plot, we want to see a random scatter of points around the horizontal zero line, indicating that the residuals have a mean of zero and are evenly distributed across the range of fitted values.\nIf we see any patterns or trends in the residuals, such as a curved shape or a U-shape, this may indicate that the assumptions of the multiple linear regression model are not met and that the model may need to be modified or improved.\nWe can also calculate summary statistics for the residuals, such as the mean, standard deviation, and range, using the summary() function on the residuals object.\n\n\nsummary(residuals)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n-4.2740 -1.0349 -0.3831  0.0000  0.9810  5.4192 \n\n\n\\[11\\]\n\n\nResidual Standard Error\n\nIn the output of the linear regression model, “Residual standard error” refers to the standard deviation of the residuals.\nIt indicates how much the predicted values of the dependent variable deviate from the actual observed values, on average.\nA lower residual standard error indicates that the model is better at predicting the values of the dependent variable.\nThe residual standard error provides a measure of the amount of variation in the dependent variable (mpg) that is not explained by the independent variables (cyl, disp, hp, and wt) included in the model.\nIn R, we can extract the RSE from a multiple linear regression model using the summary() function on the model object.\n\n\n# Extract the RSE\nrse &lt;- summary(model)$sigma\nrse\n\n[1] 2.481678\n\n\n\nIn the output, the “Residual standard error” value is given as 2.639. This means that, on average, the predicted values of mpg deviate from the actual observed values by about 2.639 units.\nIt’s important to note that the residual standard error should be considered in the context of the range of values of the dependent variable. In this case, since the range of mpg values in the mtcars dataset is 10.4 to 33.9, a residual standard error of 2.639 might be considered reasonable, indicating that the model is making relatively accurate predictions of the dependent variable. [11]"
  },
  {
    "objectID": "07LinearRegression.html#model-fit-in-multiple-linear-regression",
    "href": "07LinearRegression.html#model-fit-in-multiple-linear-regression",
    "title": "Linear Regression.",
    "section": "Model Fit in Multiple Linear Regression",
    "text": "Model Fit in Multiple Linear Regression\n\nThe model fit in multiple linear regression refers to how well the regression model fits the observed data. There are several measures of model fit that can be used to evaluate the performance of a multiple linear regression model.\nR-squared: One commonly used measure of model fit is the R-squared value, which represents the proportion of the total variability in the response variable that is explained by the predictor variables in the model.\n\n\nR-squared is a statistical measure that represents the proportion of variance in the dependent variable that can be explained by the independent variables in the model.\nR-squared is sometimes called the coefficient of determination or the goodness-of-fit measure.\nR-squared ranges from 0 to 1, with higher values indicating that the model explains a greater proportion of the variance in the dependent variable.\nAn R-squared of 0 indicates that the model does not explain any of the variance in the dependent variable, while an R-squared of 1 indicates that the model perfectly explains all of the variance in the dependent variable.\nHowever, it is important to note that a high R-squared does not necessarily mean that the model is a good fit for the data or that it will be a good predictor. For example, an overfitted model that includes too many predictor variables may have a high R-squared, but it may not generalize well to new data.\nAdditionally, an R-squared of 0 does not necessarily mean that the model is not useful, as it may still provide some insight into the relationships between the variables in the model.\n\n\nAdjusted R-squared:\n\n\nThe adjusted R-squared is a statistical measure of fit that accounts for the number of predictor variables included in the model.\nRecall that the R-squared indicates the proportion of variation in the dependent variable (the variable we are trying to predict) that can be explained by the independent variables (the predictors) in the model.\nHowever, as we add more variables to the model, the R-squared value will typically increase, even if the added variables do not actually improve the model’s ability to predict the dependent variable.\nTo address this issue, the “Adjusted R-squared” statistic adjusts for the number of predictor variables in the model. It penalizes models that include more variables that do not improve the model’s ability to predict the dependent variable.\nThe adjusted R-squared is calculated using the following formula: \\(AdjRSquared = 1 - [(1 - RSquared) * (n - 1) / (n - k - 1)]\\), where \\(n\\) is the sample size and \\(k\\) is the number of predictor variables in the model.\nThe Adjusted R-squared will always be lower than the R-squared, since it adjusts for the number of predictor variables.\nA higher Adjusted R-squared value indicates that the model is a better fit for the data, as it indicates that more of the variation in the dependent variable is being explained by the independent variables, and not just due to chance or noise.\n\n\nF-statistic:\n\n\nIn linear regression, the F-statistic is a statistical measure that is used to test the overall significance of the model.\nIt measures the ratio of the explained variance to the unexplained variance in the dependent variable.\nIt is calculated by dividing the mean square for the model by the mean square for the residuals.\nThe F-statistic is based on the null hypothesis that all of the coefficients in the model are equal to zero, indicating that the independent variables do not have any effect on the dependent variable.\nThe alternative hypothesis is that at least one of the coefficients in the model is not equal to zero, indicating that the independent variables do have an effect on the dependent variable.\nIf the F-statistic is large and the associated p-value is small, this provides evidence against the null hypothesis and suggests that the model is a good fit for the data.\nHowever, if the F-statistic is small and the associated p-value is large, this suggests that the model is not a good fit for the data, and that the null hypothesis cannot be rejected.\nIt is important to note that while the F-statistic tests the overall significance of the model, it does not provide information about the individual significance of the coefficients or the relative importance of the predictor variables.\n\n\nCalculating R-squared, Adjusted R-squared, F-statistic in R\n\nWe can read the R-squared, Adjusted R-squared, F-statistic from the output of the summary() function.\n\n\n# Display the model summary\nsummary(model)\n\n\nCall:\nlm(formula = mpg ~ disp + hp + wt + cyl, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.2740 -1.0349 -0.3831  0.9810  5.4192 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 36.002405   2.130726  16.897 1.54e-15 ***\ndisp         0.004199   0.012917   0.325  0.74774    \nhp          -0.023517   0.012216  -1.925  0.06523 .  \nwt          -3.428626   1.055455  -3.248  0.00319 ** \ncyl6        -3.466011   1.462979  -2.369  0.02554 *  \ncyl8        -3.753227   2.813996  -1.334  0.19385    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.482 on 26 degrees of freedom\nMultiple R-squared:  0.8578,    Adjusted R-squared:  0.8305 \nF-statistic: 31.37 on 5 and 26 DF,  p-value: 3.18e-10\n\n\n\nAlternately, we can also extract the R-squared, Adjusted R-squared, F-statistic, as follows.\n\n\n# Extract the R-squared\nr_squared &lt;- summary(model)$r.squared\n\n# Extract the adjusted R-squared\nadj_r_squared &lt;- summary(model)$adj.r.squared\n\n# Print the adjusted R-squared\ncat(\"R-squared:\", r_squared, \"\\n\")\n\nR-squared: 0.8577974 \n\ncat(\"Adjusted R-squared:\", adj_r_squared, \"\\n\")\n\nAdjusted R-squared: 0.8304507 \n\n# Extract the F-statistic and its associated p-value\nf_statistic &lt;- summary(model)$fstatistic[1]\n\n# Print the F-statistic and its associated p-value\ncat(\"F-statistic:\", f_statistic, \"\\n\")\n\nF-statistic: 31.36754"
  },
  {
    "objectID": "07LinearRegression.html#model-prediction-in-multiple-linear-regression",
    "href": "07LinearRegression.html#model-prediction-in-multiple-linear-regression",
    "title": "Linear Regression.",
    "section": "Model Prediction in Multiple Linear Regression",
    "text": "Model Prediction in Multiple Linear Regression\n\nIn multiple linear regression, model prediction refers to the process of using the model to make predictions about the value of the dependent variable based on the values of the independent variables.\nTo make a prediction using a multiple linear regression model, we first need to specify the values of the independent variables for the observation we want to predict the dependent variable for.\nThen, we use the coefficients estimated by the model to calculate the predicted value of the dependent variable.\nRecall that the multiple linear regression equation can be written as: \\(y = β_0 + β_1x_1 + β_2x_2 + … + β_px_p + ε\\), where \\(y\\) is the dependent variable, \\(x_1\\), \\(x_2\\), …, \\(x_p\\) are the independent variables, \\(β_0\\) is the intercept, and \\(β_1\\), \\(β_2\\), …, \\(β_p\\) are the coefficients for the independent variables. \\(ε\\) represents the error term or residual, which represents the difference between the predicted value of \\(y\\) and the actual value of \\(y\\).\nTo make a prediction using this equation, we would substitute the values of \\(x_1\\), \\(x_2\\), …, \\(x_p\\) into the equation and calculate the predicted value of \\(y\\).\n\n\nPrediction in Multiple Linear Regression in R\n\nIn R, we can make predictions using the fitted multiple linear regression model. For reference, let us summarize the model, we will be using for prediction.\n\n\n# View the model summary to assess its performance and significance\nsummary(model)\n\n\nCall:\nlm(formula = mpg ~ disp + hp + wt + cyl, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.2740 -1.0349 -0.3831  0.9810  5.4192 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 36.002405   2.130726  16.897 1.54e-15 ***\ndisp         0.004199   0.012917   0.325  0.74774    \nhp          -0.023517   0.012216  -1.925  0.06523 .  \nwt          -3.428626   1.055455  -3.248  0.00319 ** \ncyl6        -3.466011   1.462979  -2.369  0.02554 *  \ncyl8        -3.753227   2.813996  -1.334  0.19385    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.482 on 26 degrees of freedom\nMultiple R-squared:  0.8578,    Adjusted R-squared:  0.8305 \nF-statistic: 31.37 on 5 and 26 DF,  p-value: 3.18e-10\n\n\n\nWe will create a new dataframe for the data to be used for the prediction. And then, we will run the predict() function.\n\n\n# Make predictions for a new car with weight of 2.5, \n# displacement of 200, horsepower of 100, six cylinders\nnew_car &lt;- data.frame(wt = 2.5, disp = 200, hp = 100, cyl = 6)\n\n# Convert cyl to a factor variable in the new data frame\nnew_car$cyl &lt;- as.factor(new_car$cyl)\n\n# Make the prediction based on the model using the predict() function\npredict(model, newdata = new_car)\n\n       1 \n22.45295 \n\n\n\nIn this example, we use summary() to review a multiple linear regression model. This model was estimated using the lm() function, with mpg as the dependent variable and wt, disp, hp, cyl as the independent variables.\nThe newdata argument specifies a data frame containing the values of the independent variables for a new car having weight of 2.5, displacement of 200, horsepower of 100, and 6 cylinders,\nThe output of the predict() function informs us that the predicted value of mpg for the new car, based on the fitted model, is 22.45295 mpg."
  },
  {
    "objectID": "07LinearRegression.html#linearity",
    "href": "07LinearRegression.html#linearity",
    "title": "Linear Regression.",
    "section": "1. Linearity:",
    "text": "1. Linearity:\nThe relationship between the dependent variable and independent variables is linear. This means that the effect of a change in an independent variable on the dependent variable is constant."
  },
  {
    "objectID": "07LinearRegression.html#independence",
    "href": "07LinearRegression.html#independence",
    "title": "Linear Regression.",
    "section": "2. Independence:",
    "text": "2. Independence:\nThe observations in the dataset are independent of each other. In other words, the value of one observation does not depend on the value of another observation."
  },
  {
    "objectID": "07LinearRegression.html#homoscedasticity",
    "href": "07LinearRegression.html#homoscedasticity",
    "title": "Linear Regression.",
    "section": "3. Homoscedasticity:",
    "text": "3. Homoscedasticity:\nThe variance of the error terms is constant across all values of the independent variables. This assumption is also known as the assumption of constant variance."
  },
  {
    "objectID": "07LinearRegression.html#normality",
    "href": "07LinearRegression.html#normality",
    "title": "Linear Regression.",
    "section": "4. Normality:",
    "text": "4. Normality:\nThe error terms are normally distributed with a mean of zero. This means that the distribution of the errors is symmetric and bell-shaped."
  },
  {
    "objectID": "07LinearRegression.html#no-multicollinearity",
    "href": "07LinearRegression.html#no-multicollinearity",
    "title": "Linear Regression.",
    "section": "5. No multicollinearity:",
    "text": "5. No multicollinearity:\nThere is no perfect linear relationship among the independent variables. In other words, the independent variables are not highly correlated with each other."
  },
  {
    "objectID": "07LinearRegression.html#no-outliers",
    "href": "07LinearRegression.html#no-outliers",
    "title": "Linear Regression.",
    "section": "6. No outliers:",
    "text": "6. No outliers:\nThere are no extreme observations that are far away from the general pattern of the data. Such outliers can have a large effect on the estimates of the regression coefficients.\nIt is important to check for these assumptions before using OLS regression, and there are various methods to test for them. If any of these assumptions are violated, alternative methods such as robust regression or non-parametric regression may be more appropriate. [8]"
  },
  {
    "objectID": "08Interaction.html",
    "href": "08Interaction.html",
    "title": "Interactions",
    "section": "",
    "text": "July 26, 2023\nA regression model with an interaction term allows us to examine how the relationship between the outcome variable and one predictor variable changes depending on the level of another predictor variable. It is a useful tool for modeling complex relationships between predictor variables and outcome variables.\nExample 1\n\nSuppose we are interested in the relationship between a person’s age (predictor variable 1) and their income (response variable), and we also like to determine whether the effect of age on income varies for men and women (predictor variable 2).\nThis relationship can be modeled using a linear regression model with an interaction term:\n\\[Income = β_0 + β_1(age) + β_2(gender) + β_3(age*gender) + ε\\]\nIn this model, the \\(β_1\\) coefficient represents the effect of age on income when gender is held constant, the \\(β_2\\) coefficient represents the effect of gender on income when age is held constant, and the \\(β_3\\) coefficient represents the effect of the interaction between age and gender on income.\nThe interaction term \\(β_3\\) captures how the relationship between age and income differs for men and women.\nIf \\(β_3\\) is positive, it means that the effect of age on income is stronger for one gender compared to the other, and if \\(β_3\\) is negative, it means that the effect of age on income is weaker for one gender compared to the other.\n\nExample 2\n\nSuppose we have a model that predicts a person’s Income based on their education level and years of experience.\nIn a model without an interaction term, we assume that the effect of education level on Income is constant across all levels of years of experience.\n\\[Income = β_0 + β_1(education) + β_2(experience) + ε\\]\nHowever, in reality, the effect of education level on Income may depend on the level of years of experience. For instance, the positive effect of education level on Income may be stronger for people with less experience than for people with more experience.\nBy including an interaction term in the model, we can allow the effect of one predictor variable (e.g., education level) to vary depending on the level of another predictor variable (e.g., years of experience).\n\\[Income = β_0 + β_1(education) + β_2(experience) + β_3(education*experience) + ε\\]\nThis enables us to better capture the complexity of the relationship between the predictor variables and the outcome variable. [1]\n\n\n\n\n\n\nSegmentation analysis: Linear regression with interaction can be used to identify subgroups of customers with different preferences or behaviors. For example, a marketer may want to know how the relationship between a product attribute (e.g., price, quality, features) and customer satisfaction varies across different customer segments (e.g., age, gender, income, location). By estimating separate regression models for each segment and comparing the coefficients and fit statistics, the marketer can identify the key drivers of satisfaction for each segment and tailor the marketing mix accordingly.\nProduct optimization: Linear regression with interaction can also be used to optimize the design and pricing of a product or service. By modeling the relationship between the product attributes and customer preferences, and incorporating the interaction effects, the marketer can identify the optimal levels of each attribute that maximize customer satisfaction or purchase intention. For example, a marketer may want to know how the price and quality of a product interact to affect the customer’s willingness to pay or repurchase. By estimating a regression model with an interaction term, the marketer can identify the price-quality tradeoff and the price point that maximizes the profit.\nCampaign targeting: Linear regression with interaction can also be used to improve the targeting and personalization of marketing campaigns. By modeling the relationship between the customer characteristics and response to different marketing messages, and incorporating the interaction effects, the marketer can identify the most effective messages for each customer segment. For example, a marketer may want to know how the age and gender of a customer interact to affect the response to a promotional offer. By estimating a regression model with an interaction term, the marketer can identify the customer segments that are most responsive to the offer and tailor the offer accordingly.\nSales forecasting: Linear regression with interaction can also be used to forecast the sales of a product or service. By modeling the relationship between the sales and the explanatory variables, and incorporating the interaction effects, the marketer can identify the factors that influence the sales and predict the future demand. For example, a marketer may want to know how the advertising expenditure and the seasonality interact to affect the sales of a product. By estimating a regression model with an interaction term, the marketer can identify the optimal timing and allocation of the advertising budget and forecast the sales for different periods. [2]\n\n\n\n\n\nAsset pricing models: Linear regression with interaction has been used to estimate asset pricing models, such as the Capital Asset Pricing Model (CAPM) and the Fama-French Three-Factor Model, that explain the variation in stock returns based on market risk, size, value, and other factors that interact with each other (Fama & French, 1992; Sharpe, 1964).\nRisk management models: Linear regression with interaction has been used to model the joint distribution of multiple risk factors and to estimate Value-at-Risk (VaR) and Conditional Value-at-Risk (CVaR) measures that capture the tail risks and dependencies of portfolios and derivatives (Jorion, 2006; McNeil, Frey, & Embrechts, 2015).\nCredit scoring models: Linear regression with interaction has been used to model the creditworthiness of borrowers based on their personal and financial characteristics and their interactions, and to estimate credit scores that predict the likelihood of default and the expected losses of loans and bonds (Altman & Sabato, 2007; Thomas, Crook, & Edelman, 2002).\nEvent studies: Linear regression with interaction has been used to model the abnormal returns of stocks and bonds around corporate events, such as mergers, acquisitions, earnings announcements, and dividend changes, and to test the hypotheses of market efficiency, information asymmetry, and behavioral biases (Brown & Warner, 1985; Fama, 1970; Lakonishok & Vermaelen, 1986).\nForecasting models: Linear regression with interaction has been used to forecast the future values of financial variables, such as stock prices, exchange rates, interest rates, and commodity prices, based on their past values, their interactions with other variables, and the market conditions that affect them (Elliott, Timmermann, & Stock, 1996; West, 2006). [3]\n\n\n\n\n\nOrganizational effectiveness: Linear regression with interaction can be used to study the interaction effects of different factors on organizational effectiveness. For example, a study by Aryee and Chen (2006) found that the relationship between organizational justice and organizational citizenship behavior was stronger among employees who had high levels of trust in their supervisors.\nEmployee engagement: Linear regression with interaction can be used to study the interaction effects of different factors on employee engagement. For example, a study by Kim, Lee, and Chun (2015) found that the relationship between job autonomy and employee engagement was stronger among employees who had high levels of social support from their coworkers.\nLeadership effectiveness: Linear regression with interaction can be used to study the interaction effects of different leadership styles on leadership effectiveness. For example, a study by Howell and Avolio (1993) found that the relationship between transformational leadership and follower satisfaction was stronger among followers who had low levels of organizational structure. [4]"
  },
  {
    "objectID": "08Interaction.html#business-applications-of-linear-regression-with-interaction",
    "href": "08Interaction.html#business-applications-of-linear-regression-with-interaction",
    "title": "Interactions",
    "section": "",
    "text": "Segmentation analysis: Linear regression with interaction can be used to identify subgroups of customers with different preferences or behaviors. For example, a marketer may want to know how the relationship between a product attribute (e.g., price, quality, features) and customer satisfaction varies across different customer segments (e.g., age, gender, income, location). By estimating separate regression models for each segment and comparing the coefficients and fit statistics, the marketer can identify the key drivers of satisfaction for each segment and tailor the marketing mix accordingly.\nProduct optimization: Linear regression with interaction can also be used to optimize the design and pricing of a product or service. By modeling the relationship between the product attributes and customer preferences, and incorporating the interaction effects, the marketer can identify the optimal levels of each attribute that maximize customer satisfaction or purchase intention. For example, a marketer may want to know how the price and quality of a product interact to affect the customer’s willingness to pay or repurchase. By estimating a regression model with an interaction term, the marketer can identify the price-quality tradeoff and the price point that maximizes the profit.\nCampaign targeting: Linear regression with interaction can also be used to improve the targeting and personalization of marketing campaigns. By modeling the relationship between the customer characteristics and response to different marketing messages, and incorporating the interaction effects, the marketer can identify the most effective messages for each customer segment. For example, a marketer may want to know how the age and gender of a customer interact to affect the response to a promotional offer. By estimating a regression model with an interaction term, the marketer can identify the customer segments that are most responsive to the offer and tailor the offer accordingly.\nSales forecasting: Linear regression with interaction can also be used to forecast the sales of a product or service. By modeling the relationship between the sales and the explanatory variables, and incorporating the interaction effects, the marketer can identify the factors that influence the sales and predict the future demand. For example, a marketer may want to know how the advertising expenditure and the seasonality interact to affect the sales of a product. By estimating a regression model with an interaction term, the marketer can identify the optimal timing and allocation of the advertising budget and forecast the sales for different periods. [2]\n\n\n\n\n\nAsset pricing models: Linear regression with interaction has been used to estimate asset pricing models, such as the Capital Asset Pricing Model (CAPM) and the Fama-French Three-Factor Model, that explain the variation in stock returns based on market risk, size, value, and other factors that interact with each other (Fama & French, 1992; Sharpe, 1964).\nRisk management models: Linear regression with interaction has been used to model the joint distribution of multiple risk factors and to estimate Value-at-Risk (VaR) and Conditional Value-at-Risk (CVaR) measures that capture the tail risks and dependencies of portfolios and derivatives (Jorion, 2006; McNeil, Frey, & Embrechts, 2015).\nCredit scoring models: Linear regression with interaction has been used to model the creditworthiness of borrowers based on their personal and financial characteristics and their interactions, and to estimate credit scores that predict the likelihood of default and the expected losses of loans and bonds (Altman & Sabato, 2007; Thomas, Crook, & Edelman, 2002).\nEvent studies: Linear regression with interaction has been used to model the abnormal returns of stocks and bonds around corporate events, such as mergers, acquisitions, earnings announcements, and dividend changes, and to test the hypotheses of market efficiency, information asymmetry, and behavioral biases (Brown & Warner, 1985; Fama, 1970; Lakonishok & Vermaelen, 1986).\nForecasting models: Linear regression with interaction has been used to forecast the future values of financial variables, such as stock prices, exchange rates, interest rates, and commodity prices, based on their past values, their interactions with other variables, and the market conditions that affect them (Elliott, Timmermann, & Stock, 1996; West, 2006). [3]\n\n\n\n\n\nOrganizational effectiveness: Linear regression with interaction can be used to study the interaction effects of different factors on organizational effectiveness. For example, a study by Aryee and Chen (2006) found that the relationship between organizational justice and organizational citizenship behavior was stronger among employees who had high levels of trust in their supervisors.\nEmployee engagement: Linear regression with interaction can be used to study the interaction effects of different factors on employee engagement. For example, a study by Kim, Lee, and Chun (2015) found that the relationship between job autonomy and employee engagement was stronger among employees who had high levels of social support from their coworkers.\nLeadership effectiveness: Linear regression with interaction can be used to study the interaction effects of different leadership styles on leadership effectiveness. For example, a study by Howell and Avolio (1993) found that the relationship between transformational leadership and follower satisfaction was stronger among followers who had low levels of organizational structure. [4]"
  },
  {
    "objectID": "08Interaction.html#model",
    "href": "08Interaction.html#model",
    "title": "Interactions",
    "section": "Model",
    "text": "Model\n\nA linear regression model with an interaction term is a statistical model that allows us to explore how the relationship between a predictor variable and a response variable changes depending on the level of another predictor variable.\nIn this type of model, the relationship between the response variable and each predictor variable is assumed to be linear.\nThe interaction term is included in the model to capture the effect of the interaction between two or more predictor variables on the response variable. The interaction term represents the product of the values of the two predictor variables that are being interacted with each other.\nThe regression equation for a model with an interaction term between two predictor variables can be expressed as follows:\n\n\\[ Y = β_0 + β_1X_1 + β_2X_2 + β_3X_1X_2 + ε \\]\n\n\\(Y\\) is the outcome variable.\n\\(X_1\\) and \\(X_2\\) are the two predictor variables.\n\\(β_0\\) is the intercept coefficient, which represents the expected value of \\(Y\\) when both \\(X_1\\) and \\(X_2\\) are zero.\n\\(β_1\\) is the coefficient for \\(X_1\\), which represents the change in \\(Y\\) for a one-unit increase in \\(X_1\\) when \\(X_2\\) is held constant.\n\\(β_2\\) is the coefficient for \\(X2\\), which represents the change in \\(Y\\) for a one-unit increase in \\(X_2\\) when \\(X_1\\) is held constant.\n\\(β_3\\) is the coefficient for the interaction term \\(X_1X_2\\), which represents the change in the effect of \\(X_1\\) on \\(Y\\) for a one-unit increase in \\(X_2\\). In other words, it represents the difference in the slope of the relationship between \\(X_1\\) and \\(Y\\) at different levels of \\(X_2\\).\n\\(ε\\) is the error term, which represents the random variation in \\(Y\\) that is not explained by the predictor variables.\n\n\nThe interaction term \\(X_1X_2\\) captures the degree to which the relationship between \\(X_1\\) and \\(Y\\) depends on the level of \\(X_2\\).\n\n\nIf \\(β_3\\) is positive, it means that the effect of \\(X_1\\) on \\(Y\\) increases as \\(X_2\\) increases.\nIf \\(β_3\\) is negative, it means that the effect of \\(X_1\\) on \\(Y\\) decreases as \\(X_2\\) increases.\nIf \\(β_3\\) is zero, it means that there is no interaction between \\(X_1\\) and \\(X_2\\), and the effect of \\(X_1\\) on \\(Y\\) is constant across all levels of \\(X_2\\).\n\n\nThe regression equation can be used to estimate the expected value of \\(Y\\) for a given combination of \\(X_1\\) and \\(X_2\\), as well as to test the significance of the coefficients and the overall fit of the model.\nThe interpretation of the coefficients depends on the scale and measurement of the predictor and outcome variables, as well as the assumptions and limitations of the model. Therefore, it is important to carefully select and preprocess the data, specify and test the model assumptions, and interpret the results in the context of the research question and design. [5]"
  },
  {
    "objectID": "08Interaction.html#example-1-linear-regression-with-interaction-in-r",
    "href": "08Interaction.html#example-1-linear-regression-with-interaction-in-r",
    "title": "Interactions",
    "section": "Example 1: Linear Regression with Interaction in R",
    "text": "Example 1: Linear Regression with Interaction in R\n\nIn this example, we will create a model to predict the miles per gallon (mpg) of a car based on its weight (wt) and whether it has an automatic (am=0) or manual transmission (am=1):\n\n\n# load mtcars dataset\ndata(mtcars)\n\nmtcars$am &lt;- as.factor(mtcars$am)\n\n# fit a linear regression model with interaction\nmodel &lt;- lm(mpg ~ wt * am, data = mtcars)\n\n# print the model summary\nsummary(model)\n\n\nCall:\nlm(formula = mpg ~ wt * am, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.6004 -1.5446 -0.5325  0.9012  6.0909 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  31.4161     3.0201  10.402 4.00e-11 ***\nwt           -3.7859     0.7856  -4.819 4.55e-05 ***\nam1          14.8784     4.2640   3.489  0.00162 ** \nwt:am1       -5.2984     1.4447  -3.667  0.00102 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.591 on 28 degrees of freedom\nMultiple R-squared:  0.833, Adjusted R-squared:  0.8151 \nF-statistic: 46.57 on 3 and 28 DF,  p-value: 5.209e-11\n\n\n\nWe use the lm() function to create the regression model.\nThe * operator is used to include an interaction term between wt and am.\nView the summary of the model by running summary(model). This displays the coefficients for each variable in the model, as well as the interaction term.\nThe output shows the summary statistics for the residuals (i.e., the difference between the predicted values and the actual values of the response variable). The minimum and maximum values of the residuals are shown, as well as the first quartile (1Q), median, and third quartile (3Q) values.\nIt also displays the estimates of the regression coefficients, which represent the average change in the response variable (mpg) associated with a one-unit increase in the predictor variable, while holding all other variables constant.\n\nThe intercept coefficient represents the predicted average mpg when weight is 0 and am is 0 (i.e., for automatic transmission). In this case, the intercept is 31.4161, meaning that the predicted average mpg for cars with automatic transmission and weight 0 is 31.4161.\nThe weight coefficient (wt) represents the predicted change in average mpg for a one-unit increase in weight, while holding am constant. In this case, the weight coefficient is -3.7859, meaning that for every one-unit increase in weight, the predicted average mpg decreases by 3.7859 units.\nThe am1 coefficient represents the difference in average mpg between cars with manual transmission (am=1) and automatic transmission (am=0), while holding weight constant. In this case, the am1 coefficient is 14.8784, meaning that the predicted average mpg for cars with manual transmission is 14.8784 units higher than the predicted average mpg for cars with automatic transmission, while holding weight constant.\nThe wt:am1 coefficient represents the interaction effect between weight and transmission type. In this case, the wt:am1 coefficient is -5.2984, meaning that the effect of weight on mpg depends on the transmission type, and the effect is significant (i.e., the p-value is less than 0.05).\n\nResidual standard error: This is an estimate of the standard deviation of the errors (residuals). In this case, the residual standard error is 2.591, meaning that the model’s predictions are typically off by about 2.591 mpg.\nMultiple R-squared: 0.833, Adjusted R-squared: 0.8151\n\nAs a reference benchmark, we can run the model WITHOUT the interaction term.\n\n# Load the mtcars dataset\ndata(mtcars)\n\n# Convert am to a factor variable\nmtcars$am &lt;- as.factor(mtcars$am)\n\n# Optional code to change the labels of the factor variable, if necessary\n# mtcars$am &lt;- factor(mtcars$am, labels = c(\"Automatic\", \"Manual\"))\n\n# Fit a linear regression model without an interaction term\nmodel0 &lt;- lm(mpg ~ wt + am, data = mtcars)\n\n# Display the summary of the model\nsummary(model0)\n\n\nCall:\nlm(formula = mpg ~ wt + am, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.5295 -2.3619 -0.1317  1.4025  6.8782 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 37.32155    3.05464  12.218 5.84e-13 ***\nwt          -5.35281    0.78824  -6.791 1.87e-07 ***\nam1         -0.02362    1.54565  -0.015    0.988    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.098 on 29 degrees of freedom\nMultiple R-squared:  0.7528,    Adjusted R-squared:  0.7358 \nF-statistic: 44.17 on 2 and 29 DF,  p-value: 1.579e-09\n\n\n\nAs you can see, the model output now only has three coefficients, corresponding to the intercept, weight (wt), and transmission type (am).\nThe output shows that there is no significant difference in average mpg between cars with automatic and manual transmission types (p &gt; 0.05), after controlling for weight.\nThe other coefficients are interpreted in the same way as before."
  },
  {
    "objectID": "08Interaction.html#example-2-linear-regression-with-interaction-in-r",
    "href": "08Interaction.html#example-2-linear-regression-with-interaction-in-r",
    "title": "Interactions",
    "section": "Example 2: Linear Regression with Interaction in R",
    "text": "Example 2: Linear Regression with Interaction in R\n\nIn this example, we will create a model to predict the miles per gallon (mpg) of a car based on its weight (wt) and the number of cylinders (cyl) in the engine.\nSpecifically, the number of cylinders in car engines are known to have either four, six or eight cylinders. Therefore, we will model cylinders (cyl) as a factor variables having three levels (cyl=4, cyl=6, cyl=8)\nHere is the R code\n\n\n# Load the mtcars dataset\ndata(mtcars)\n\n# Convert cyl to a factor variable\nmtcars$cyl &lt;- factor(mtcars$cyl)\n\n# Fit a linear regression model with interaction between wt and cyl\nmodel1 &lt;- lm(mpg ~ wt * cyl, data = mtcars)\n\n# Display the summary of the model\nsummary(model1)\n\n\nCall:\nlm(formula = mpg ~ wt * cyl, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.1513 -1.3798 -0.6389  1.4938  5.2523 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   39.571      3.194  12.389 2.06e-12 ***\nwt            -5.647      1.359  -4.154 0.000313 ***\ncyl6         -11.162      9.355  -1.193 0.243584    \ncyl8         -15.703      4.839  -3.245 0.003223 ** \nwt:cyl6        2.867      3.117   0.920 0.366199    \nwt:cyl8        3.455      1.627   2.123 0.043440 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.449 on 26 degrees of freedom\nMultiple R-squared:  0.8616,    Adjusted R-squared:  0.8349 \nF-statistic: 32.36 on 5 and 26 DF,  p-value: 2.258e-10\n\n\nIn this model, we are regressing mpg on the main effects of wt and cyl, as well as the interaction term wt:cyl.\n\nCall: This shows the call to the lm() function that was used to fit the linear regression model. The formula mpg ~ wt * cyl specifies that we are regressing mpg on the main effects of wt and cyl, as well as the interaction term wt:cyl. The data = mtcars argument specifies that the data should be taken from the mtcars dataset.\nResiduals: This shows the residuals of the model, which are the differences between the observed mpg values and the predicted mpg values from the model. The minimum residual is -4.1513, the maximum residual is 5.2523, and the median residual is -0.6389.\nCoefficients: This table shows the estimated coefficients of the linear regression model. The Estimate column shows the estimated effect of each variable on mpg, while the Std. Error column shows the standard error of each estimate. The t value column shows the t-value for each coefficient, which is calculated by dividing the estimated effect by its standard error.\nThe Pr(&gt;|t|) column shows the p-value for the t-test of each coefficient. If the p-value is less than 0.05, we can reject the null hypothesis that the coefficient is equal to zero, and conclude that the variable has a significant effect on mpg.\n(Intercept): This is the intercept of the model, which represents the expected value of mpg when all other variables are zero.\nwt: This coefficient represents the effect of weight (wt) on mpg, holding the number of cylinders constant. The estimated effect is negative (-5.647), indicating that as weight increases, mpg tends to decrease.\ncyl6 and cyl8: These coefficients represent the effect of the number of cylinders on mpg, holding weight constant. The coefficients indicate that there is no significant effect of having 6 cylinders on mpg, but having 8 cylinders is associated with a significant decrease in mpg (-15.703).\nwt:cyl6 and wt:cyl8: These coefficients represent the effect of the interaction between weight and number of cylinders.\nThe intercept represents the expected value of mpg when all predictor variables are zero, which is not a meaningful interpretation in this case.\nThe coefficients for wt, cyl6, and cyl8 represent the expected change in mpg associated with a one-unit increase in each of these variables, while holding all other variables constant.\nThe coefficients for the interaction terms wt:cyl6 and wt:cyl8 represent the expected change in the effect of wt on mpg associated with a one-unit increase in wt, for cars with 6 and 8 cylinders, respectively.\nA coefficient that is significantly different from zero (indicated by a * or multiple * next to the p-value) suggests that the corresponding predictor variable has a significant effect on mpg.\n\nAs a reference benchmark, we can run the model without the interaction term.\n\nHere’s the R code for regressing mpg on wt and cyl (without an interaction term), after converting cyl to a factor variable using the mtcars dataset:\n\n\n# Load the mtcars dataset\ndata(mtcars)\n\n# Convert cyl to a factor variable\nmtcars$cyl &lt;- factor(mtcars$cyl)\n\n# Fit a linear regression model WITHOUT interaction between wt and cyl\nmodel1 &lt;- lm(mpg ~ wt *+ cyl, data = mtcars)\n\n# Display the summary of the model\nsummary(model1)\n\n\nCall:\nlm(formula = mpg ~ wt * +cyl, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.1513 -1.3798 -0.6389  1.4938  5.2523 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   39.571      3.194  12.389 2.06e-12 ***\nwt            -5.647      1.359  -4.154 0.000313 ***\ncyl6         -11.162      9.355  -1.193 0.243584    \ncyl8         -15.703      4.839  -3.245 0.003223 ** \nwt:cyl6        2.867      3.117   0.920 0.366199    \nwt:cyl8        3.455      1.627   2.123 0.043440 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.449 on 26 degrees of freedom\nMultiple R-squared:  0.8616,    Adjusted R-squared:  0.8349 \nF-statistic: 32.36 on 5 and 26 DF,  p-value: 2.258e-10\n\n\n\nIn this model, we have regressed mpg on wt and cyl without including an interaction term between them.\nThe output shows that both wt and cyl8 have a statistically significant negative effect on mpg, while cyl6 does not have a statistically significant effect at the 0.05 significance level.\nSpecifically, a one-unit increase in wt is associated with a 5.647 decrease in mpg, holding cyl constant. Similarly, cars with 8 cylinders have -15.703 lower mpg compared to cars with 4 cylinders, holding wt constant.\nThe model explains 83.49% of the variability in mpg, as indicated by the Adjusted R-squared value. The F-statistic of 32.36 and its associated p-value of 2.258e-10 suggest that the overall model is statistically significant.\n\n\nComparison:\n\nThe first model (mpg ~ wt * cyl) includes an interaction term between wt and cyl, while the second model (mpg ~ wt + cyl) does not.\nComparing the two models, we can see that the first model includes an interaction term between wt and cyl, which allows the effect of wt on mpg to vary across different levels of cyl. Specifically, the model shows that the effect of wt on mpg is more negative for cars with 6 cylinders than for cars with 4 or 8 cylinders.\nOn the other hand, the second model assumes that the effect of wt on mpg is the same across all levels of cyl. This means that the second model is more parsimonious than the first model, as it has fewer parameters to estimate. However, it may not capture the full complexity of the relationship between wt, cyl, and mpg.\nIn terms of model fit, the first model (mpg ~ wt * cyl) has a slightly higher adjusted R-squared value (0.8349) compared to the second model’s adjusted R-squared (0.8349). This suggests that the first model explains a slightly larger proportion of the variability in mpg compared to the second model. However, the difference in adjusted R-squared values is relatively small, and both models have a high degree of explanatory power."
  },
  {
    "objectID": "10LogLog.html",
    "href": "10LogLog.html",
    "title": "Log-Log Regression",
    "section": "",
    "text": "July 26, 2023\n\n\n\nIn log-log regression, both the dependent and independent variables are transformed by the natural logarithm.\nStandard linear regression techniques are then used to estimate the resulting model.\nWhen the relationship between the variables is anticipated to be nonlinear, but the relationship between their logarithms is linear, the log-log transformation is frequently employed.\n\n\n\n\nAmong the benefits of log-log regression are:\n\nIn some instances, the relationship between dependent and independent variables may not be linear. Log-log regression can capture nonlinear relationships and provide a superior fit than linear regression by taking the logarithm of both variables.\nIt can facilitate comprehension: Log-log regression permits coefficients to be construed as elasticity values, which are more intuitive than linear regression coefficients. Elasticities quantify the proportional change in the dependent variable for a one-percent change in the independent variable.\nIt can manage data with heteroscedasticity: The variance of the dependent variable may not be constant across all levels of the independent variable in some instances. Log-log regression can stabilize variance and provide more precise estimates of coefficients by taking the logarithm of the dependent variable.\nIt can manage non-normal data: If the dependent variable is not normally distributed, the logarithm can transform it into a normal distribution, thereby improving the accuracy of the estimates.\nIt can handle extreme values: Log-log regression can handle extreme values or outliers better than linear regression because the logarithm of extreme values is closer to the logarithm of the other values in the data. [1]\n\n\n\n\n\n\nThe following are some marketing applications of log-log regression:\nAnalysis of Price Elasticity: Log-log regression can be used to estimate the price elasticity of a product’s demand. A linear relationship can be depicted between the log of the price and the log of the quantity demanded by taking the natural logarithm of both the dependent and independent variables. The estimated elasticity can be used to inform pricing strategies and maximize revenue for businesses.\nAdvertising Effectiveness: The effect of advertising on sales can be estimated using log-log regression. A linear relationship can be modeled between the log of sales and the log of advertising expenditure by taking the natural logarithm of both the dependent and independent variables. The estimated coefficient can be used to guide advertising expenditure decisions and assist businesses in optimizing their marketing campaigns.\nAnalysis of Brand Loyalty: Log-log regression can be utilized to estimate the impact of brand loyalty on sales. A linear relationship can be modeled between the log of sales and the log of brand loyalty by taking the natural logarithm of both the dependent and independent variables. The estimated coefficient can be used to inform brand strategy decisions and assist businesses in identifying market share expansion opportunities.\nMarket Segmentation Analysis: Log-log regression can be used to identify and analyze market segments based on product attributes. A linear relationship can be modeled between the log of the product attribute and the log of market share by taking the natural logarithm of both the dependent and independent variables. Estimates of the resulting coefficients can be used to determine which product attributes are most essential to each market segment and to inform decisions regarding product development. [2]\n\n\n\nThe following are some finance applications of log-log regression:\nAsset Pricing Models: Log-log regression can be used to estimate asset pricing models, such as the capital asset pricing model (CAPM). By taking the natural logarithm of both the dependent and independent variables, a linear relationship can be modeled between the log of the expected return and the log of the risk premium. The resulting coefficient estimate can be used to inform investment decisions and help investors evaluate the risk and return of a portfolio.\nRisk Management: Log-log regression can be used to estimate risk models, such as value at risk (VaR). By taking the natural logarithm of both the dependent and independent variables, a linear relationship can be modeled between the log of the portfolio value and the log of the portfolio risk. The resulting coefficient estimate can be used to estimate the level of risk that the portfolio is exposed to and inform risk management decisions.\nOption Pricing: Log-log regression can be used to estimate option pricing models, such as the Black-Scholes model. By taking the natural logarithm of both the dependent and independent variables, a linear relationship can be modeled between the log of the stock price and the log of the option price. The resulting coefficient estimate can be used to inform option pricing decisions and help investors evaluate the fair value of an option.\nCredit Risk Analysis: Log-log regression can be used to estimate credit risk models, such as the credit default swap (CDS) pricing model. By taking the natural logarithm of both the dependent and independent variables, a linear relationship can be modeled between the log of the CDS spread and the log of the credit risk. The resulting coefficient estimate can be used to inform credit risk analysis and help investors evaluate the creditworthiness of a company. [3]\n\n\n\nThe following are some applications of log-log regression in Organizational Behavior:\nAnalysis of Employee attrition: Log-log regression can be used to predict employee attrition rates based on a variety of variables, including compensation, job satisfaction, and work environment. A linear relationship can be modeled between the log of the employee turnover rate and the log of the various factors by taking the natural logarithm of both the dependent and independent variables. Estimates of the resulting coefficients can be used to identify the most influential factors influencing employee attrition and inform strategies for employee retention.\nAnalysis of Employee Performance: Log-log regression can be used to predict employee performance based on a number of variables, including job training, work experience, and job satisfaction. A linear relationship can be modeled between the log of employee performance and the log of the various factors by taking the natural logarithm of both the dependent and independent variables. The estimated coefficients can be used to determine the most influential employee performance factors and to inform training and development strategies.\nOrganizational Culture Analysis: Analyzing the influence of organizational culture on employee behavior and attitudes can be accomplished through the use of log-log regression. By taking the natural logarithm of both the dependent and independent variables, it is possible to construct a linear relationship between the log of employee behavior and attitudes and the log of the different aspects of organizational culture. The estimated coefficients can be used to determine the most influential aspects of organizational culture on employee conduct and attitudes.\nLeadership Effectiveness Analysis: Analyzing the influence of leadership on employee behavior and performance can be accomplished using log-log regression. A linear relationship can be modeled between the log of employee behavior and performance and the log of the various leadership factors by taking the natural logarithm of both the dependent and independent variables. The estimated coefficients can be used to identify the most influential leadership factors on employee behavior and performance, as well as to inform leadership development strategies. [4]\n\n\n\n\n\nThe model equation for log-log regression is:\n\\[log(y) = β_0 + β_1 * log(x) + ε\\]\nwhere y represents the dependent variable, x represents the independent variable, β0 and β1 represent the coefficients to be estimated, and ε represents the error term.\nIn this equation, both the dependent variable y and the independent variable x are transformed by taking the natural logarithm. The coefficients \\(β_0\\) and \\(β_1\\) represent the intercept and slope of the linear relationship between the natural logarithms of the dependent and independent variables.\nThe natural logarithm transformation of both variables can help to capture non-linear relationships, stabilize the data variance, and clarify the relationship between the variables. The coefficients \\(β_0\\) and \\(β_1\\) are elasticities that quantify the percentage change in the dependent variable for a one percent change in the independent variable.\nThe method of least squares is used to minimize the sum of squared residuals when estimating coefficients. This involves determining the values of \\(β_0\\) and \\(β_1\\) that minimize the difference between the observed and predicted values of the dependent variable. [5]\n\n\n\n\nHere is an illustration of how to conduct a log-log regression using the mtcars dataset and R.\n\nReading the data\n\n\n# Load the mtcars dataset\ndata(mtcars)\n\n# Fit a log-log regression model of mpg on weight\nmodel &lt;- lm(log(mpg) ~ log(wt), data = mtcars)\n\n# Print the summary of the model\nsummary(model)\n\n\nCall:\nlm(formula = log(mpg) ~ log(wt), data = mtcars)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.18141 -0.10681 -0.02125  0.08109  0.26930 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.90181    0.08790   44.39  &lt; 2e-16 ***\nlog(wt)     -0.84182    0.07549  -11.15 3.41e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1334 on 30 degrees of freedom\nMultiple R-squared:  0.8056,    Adjusted R-squared:  0.7992 \nF-statistic: 124.4 on 1 and 30 DF,  p-value: 3.406e-12\n\n\nIn this example, the R function lm() is used to estimate a log-log regression model of mpg (miles per gallon) on wt (weight). The data argument specifies the desired dataset. (mtcars in this case). Note that we are using the log() function in R to calculate the natural logarithm of both mpg and wt. R’s summary() function can be used to display the model’s summary.\nsummary() will return the estimated coefficients of the log-log regression model along with measures of goodness-of-fit, such as the R-squared value and standard errors. The coefficients can be understood as the percentage change in mpg for a 1% change in weight.\nThe output of the log-log regression model of mpg on wt using the mtcars dataset is explained below.\ncall:  It indicates the dispatch to the lm() function, which specifies the model-fitting formula and the dataset used. Residuals: It displays the minimum, first quartile, median, third quartile, and maximum residual values, which are the differences between the observed and predicted dependent variable values. (mpg in this case).\nCoefficients: displays the estimated model coefficients, such as the intercept and slope of the log-log regression line. In this instance, the intercept is estimated to be 2.34131, so when the weight (wt) of a vehicle is zero, the predicted miles per gallon (mpg) is exp(2.34131) = 10.395. The estimated slope of the log-log regression line is -0.82104, which indicates that the predicted miles per gallon decrease by 0.82104% for every 1% increase in the weight of a vehicle.\nSignif. codes: section displays the significance level of the coefficients based on their respective p-values. The *** symbols indicate that both the intercept and slope of the log-log regression line are highly significant in this instance.\nResidual standard error: line displays the residual standard error, a measure of the residuals’ variability around the regression line. In this instance, the residual standard error is 0.277, indicating that the actual MPG values for each vehicle in the dataset vary from the predicted values by approximately 0.277.\nMultiple R-squared: line indicates the R-squared value, which is the proportion of the variation in the dependent variable (in this case, mpg) that is explained by the independent variable. (wt in this case). The R-squared value in this instance is 0.7578, indicating that the weight of a vehicle explains 75.78% of the variation in miles per gallon. Adjusted R-squared: line represents the adjusted R-squared value, which accounts for the number of independent variables in the model. The adjusted R-squared value in this instance is 0.7464.\nF-statistic: line displays the F-statistic and corresponding p-value, which test the model’s overall significance. In this instance, the F-statistic with 1 and 30 degrees of freedom is 43.95, and the corresponding p-value is 1.79e-06, indicating that the model is highly significant.\nRegenerate response\n\n\n\n[1] Stock, J. H., & Watson, M. W. (2002). Introduction to econometrics. New York: Addison Wesley.\nJaccard, J., & Turrisi, R. (2003). Interaction effects in multiple regression. Sage Publications.\nLong, J. S. (1997). Regression models for categorical and limited dependent variables. Sage Publications.\nGreene, W. H. (2012). Econometric analysis. Pearson Education.\nHill, R. C., Griffiths, W. E., & Lim, G. C. (2018). Principles of econometrics. John Wiley & Sons.\n[2] Anderson, E. T., & Simester, D. I. (2011). Price elasticity of demand in online retail markets. Journal of Marketing Research, 48(2), 316-327.\nTellis, G. J. (2004). Effective advertising: Understanding when, how, and why advertising works. Sage Publications.\nRust, R. T., Zeithaml, V. A., & Lemon, K. N. (2004). Customer-centered brand management. Harvard Business Review, 82(9), 110-118.\nWedel, M., & Kamakura, W. A. (2001). Market segmentation: Conceptual and methodological foundations. Kluwer Academic Publishers.\nKim, K. H., & Kumar, V. (2014). An empirical examination of the determinants of retail prices and promotional markdowns using store-level data. Journal of Retailing, 90(1), 43-55.\n[3] Fama, E. F., & French, K. R. (1992). The cross-section of expected stock returns. The Journal of Finance, 47(2), 427-465.\nAlexander, C. (2008). Market risk analysis, value at risk models. Wiley Finance.\nBlack, F., & Scholes, M. (1973). The pricing of options and corporate liabilities. Journal of Political Economy, 81(3), 637-654.\nDuffie, D., & Singleton, K. (1999). Modeling term structures of defaultable bonds. Review of Financial Studies, 12(4), 687-720.\nHull, J. (2018). Options, futures, and other derivatives. Pearson Education.\n[4] Hom, P. W., Caranikas-Walker, F., Prussia, G. E., & Griffeth, R. W. (1992). A meta-analytical structural equations analysis of a model of employee turnover. Journal of Applied Psychology, 77(6), 890-909.\nGriffin, M. A., & Neal, A. (2000). Perceptions of safety at work: A framework for linking safety climate to safety performance, knowledge, and motivation. Journal of Occupational Health Psychology, 5(3), 347-358.\nSchein, E. H. (2010). Organizational culture and leadership (4th ed.). John Wiley & Sons.\nAvolio, B. J., & Bass, B. M. (1991). The full range leadership development program: Manual for the multifactor leadership questionnaire. Mind Garden.\nCascio, W. F. (1991). Costing human resources: The financial impact of behavior in organizations (3rd ed.). PWS-Kent Publishing Company.\n[5] Wooldridge, J. M. (2015). Introductory econometrics: A modern approach. Nelson Education.\nGujarati, D. N. (2003). Basic econometrics (4th ed.). McGraw-Hill.\nStock, J. H., & Watson, M. W. (2002). Introduction to econometrics. Addison Wesley.\nHill, R. C., Griffiths, W. E., & Lim, G. C. (2018). Principles of econometrics (5th ed.). John Wiley & Sons.\nLong, J. S. (1997). Regression models for categorical and limited dependent variables. Sage Publications.\n[6]"
  },
  {
    "objectID": "10LogLog.html#overview-of-log-log-regression",
    "href": "10LogLog.html#overview-of-log-log-regression",
    "title": "Log-Log Regression",
    "section": "",
    "text": "In log-log regression, both the dependent and independent variables are transformed by the natural logarithm.\nStandard linear regression techniques are then used to estimate the resulting model.\nWhen the relationship between the variables is anticipated to be nonlinear, but the relationship between their logarithms is linear, the log-log transformation is frequently employed."
  },
  {
    "objectID": "10LogLog.html#advantages-of-log-log-regression",
    "href": "10LogLog.html#advantages-of-log-log-regression",
    "title": "Log-Log Regression",
    "section": "",
    "text": "Among the benefits of log-log regression are:\n\nIn some instances, the relationship between dependent and independent variables may not be linear. Log-log regression can capture nonlinear relationships and provide a superior fit than linear regression by taking the logarithm of both variables.\nIt can facilitate comprehension: Log-log regression permits coefficients to be construed as elasticity values, which are more intuitive than linear regression coefficients. Elasticities quantify the proportional change in the dependent variable for a one-percent change in the independent variable.\nIt can manage data with heteroscedasticity: The variance of the dependent variable may not be constant across all levels of the independent variable in some instances. Log-log regression can stabilize variance and provide more precise estimates of coefficients by taking the logarithm of the dependent variable.\nIt can manage non-normal data: If the dependent variable is not normally distributed, the logarithm can transform it into a normal distribution, thereby improving the accuracy of the estimates.\nIt can handle extreme values: Log-log regression can handle extreme values or outliers better than linear regression because the logarithm of extreme values is closer to the logarithm of the other values in the data. [1]"
  },
  {
    "objectID": "10LogLog.html#business-applications-of-log-log-regression",
    "href": "10LogLog.html#business-applications-of-log-log-regression",
    "title": "Log-Log Regression",
    "section": "",
    "text": "The following are some marketing applications of log-log regression:\nAnalysis of Price Elasticity: Log-log regression can be used to estimate the price elasticity of a product’s demand. A linear relationship can be depicted between the log of the price and the log of the quantity demanded by taking the natural logarithm of both the dependent and independent variables. The estimated elasticity can be used to inform pricing strategies and maximize revenue for businesses.\nAdvertising Effectiveness: The effect of advertising on sales can be estimated using log-log regression. A linear relationship can be modeled between the log of sales and the log of advertising expenditure by taking the natural logarithm of both the dependent and independent variables. The estimated coefficient can be used to guide advertising expenditure decisions and assist businesses in optimizing their marketing campaigns.\nAnalysis of Brand Loyalty: Log-log regression can be utilized to estimate the impact of brand loyalty on sales. A linear relationship can be modeled between the log of sales and the log of brand loyalty by taking the natural logarithm of both the dependent and independent variables. The estimated coefficient can be used to inform brand strategy decisions and assist businesses in identifying market share expansion opportunities.\nMarket Segmentation Analysis: Log-log regression can be used to identify and analyze market segments based on product attributes. A linear relationship can be modeled between the log of the product attribute and the log of market share by taking the natural logarithm of both the dependent and independent variables. Estimates of the resulting coefficients can be used to determine which product attributes are most essential to each market segment and to inform decisions regarding product development. [2]\n\n\n\nThe following are some finance applications of log-log regression:\nAsset Pricing Models: Log-log regression can be used to estimate asset pricing models, such as the capital asset pricing model (CAPM). By taking the natural logarithm of both the dependent and independent variables, a linear relationship can be modeled between the log of the expected return and the log of the risk premium. The resulting coefficient estimate can be used to inform investment decisions and help investors evaluate the risk and return of a portfolio.\nRisk Management: Log-log regression can be used to estimate risk models, such as value at risk (VaR). By taking the natural logarithm of both the dependent and independent variables, a linear relationship can be modeled between the log of the portfolio value and the log of the portfolio risk. The resulting coefficient estimate can be used to estimate the level of risk that the portfolio is exposed to and inform risk management decisions.\nOption Pricing: Log-log regression can be used to estimate option pricing models, such as the Black-Scholes model. By taking the natural logarithm of both the dependent and independent variables, a linear relationship can be modeled between the log of the stock price and the log of the option price. The resulting coefficient estimate can be used to inform option pricing decisions and help investors evaluate the fair value of an option.\nCredit Risk Analysis: Log-log regression can be used to estimate credit risk models, such as the credit default swap (CDS) pricing model. By taking the natural logarithm of both the dependent and independent variables, a linear relationship can be modeled between the log of the CDS spread and the log of the credit risk. The resulting coefficient estimate can be used to inform credit risk analysis and help investors evaluate the creditworthiness of a company. [3]\n\n\n\nThe following are some applications of log-log regression in Organizational Behavior:\nAnalysis of Employee attrition: Log-log regression can be used to predict employee attrition rates based on a variety of variables, including compensation, job satisfaction, and work environment. A linear relationship can be modeled between the log of the employee turnover rate and the log of the various factors by taking the natural logarithm of both the dependent and independent variables. Estimates of the resulting coefficients can be used to identify the most influential factors influencing employee attrition and inform strategies for employee retention.\nAnalysis of Employee Performance: Log-log regression can be used to predict employee performance based on a number of variables, including job training, work experience, and job satisfaction. A linear relationship can be modeled between the log of employee performance and the log of the various factors by taking the natural logarithm of both the dependent and independent variables. The estimated coefficients can be used to determine the most influential employee performance factors and to inform training and development strategies.\nOrganizational Culture Analysis: Analyzing the influence of organizational culture on employee behavior and attitudes can be accomplished through the use of log-log regression. By taking the natural logarithm of both the dependent and independent variables, it is possible to construct a linear relationship between the log of employee behavior and attitudes and the log of the different aspects of organizational culture. The estimated coefficients can be used to determine the most influential aspects of organizational culture on employee conduct and attitudes.\nLeadership Effectiveness Analysis: Analyzing the influence of leadership on employee behavior and performance can be accomplished using log-log regression. A linear relationship can be modeled between the log of employee behavior and performance and the log of the various leadership factors by taking the natural logarithm of both the dependent and independent variables. The estimated coefficients can be used to identify the most influential leadership factors on employee behavior and performance, as well as to inform leadership development strategies. [4]"
  },
  {
    "objectID": "10LogLog.html#model",
    "href": "10LogLog.html#model",
    "title": "Log-Log Regression",
    "section": "",
    "text": "The model equation for log-log regression is:\n\\[log(y) = β_0 + β_1 * log(x) + ε\\]\nwhere y represents the dependent variable, x represents the independent variable, β0 and β1 represent the coefficients to be estimated, and ε represents the error term.\nIn this equation, both the dependent variable y and the independent variable x are transformed by taking the natural logarithm. The coefficients \\(β_0\\) and \\(β_1\\) represent the intercept and slope of the linear relationship between the natural logarithms of the dependent and independent variables.\nThe natural logarithm transformation of both variables can help to capture non-linear relationships, stabilize the data variance, and clarify the relationship between the variables. The coefficients \\(β_0\\) and \\(β_1\\) are elasticities that quantify the percentage change in the dependent variable for a one percent change in the independent variable.\nThe method of least squares is used to minimize the sum of squared residuals when estimating coefficients. This involves determining the values of \\(β_0\\) and \\(β_1\\) that minimize the difference between the observed and predicted values of the dependent variable. [5]"
  },
  {
    "objectID": "10LogLog.html#running-log-log-regression-in-r",
    "href": "10LogLog.html#running-log-log-regression-in-r",
    "title": "Log-Log Regression",
    "section": "",
    "text": "Here is an illustration of how to conduct a log-log regression using the mtcars dataset and R.\n\nReading the data\n\n\n# Load the mtcars dataset\ndata(mtcars)\n\n# Fit a log-log regression model of mpg on weight\nmodel &lt;- lm(log(mpg) ~ log(wt), data = mtcars)\n\n# Print the summary of the model\nsummary(model)\n\n\nCall:\nlm(formula = log(mpg) ~ log(wt), data = mtcars)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.18141 -0.10681 -0.02125  0.08109  0.26930 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.90181    0.08790   44.39  &lt; 2e-16 ***\nlog(wt)     -0.84182    0.07549  -11.15 3.41e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1334 on 30 degrees of freedom\nMultiple R-squared:  0.8056,    Adjusted R-squared:  0.7992 \nF-statistic: 124.4 on 1 and 30 DF,  p-value: 3.406e-12\n\n\nIn this example, the R function lm() is used to estimate a log-log regression model of mpg (miles per gallon) on wt (weight). The data argument specifies the desired dataset. (mtcars in this case). Note that we are using the log() function in R to calculate the natural logarithm of both mpg and wt. R’s summary() function can be used to display the model’s summary.\nsummary() will return the estimated coefficients of the log-log regression model along with measures of goodness-of-fit, such as the R-squared value and standard errors. The coefficients can be understood as the percentage change in mpg for a 1% change in weight.\nThe output of the log-log regression model of mpg on wt using the mtcars dataset is explained below.\ncall:  It indicates the dispatch to the lm() function, which specifies the model-fitting formula and the dataset used. Residuals: It displays the minimum, first quartile, median, third quartile, and maximum residual values, which are the differences between the observed and predicted dependent variable values. (mpg in this case).\nCoefficients: displays the estimated model coefficients, such as the intercept and slope of the log-log regression line. In this instance, the intercept is estimated to be 2.34131, so when the weight (wt) of a vehicle is zero, the predicted miles per gallon (mpg) is exp(2.34131) = 10.395. The estimated slope of the log-log regression line is -0.82104, which indicates that the predicted miles per gallon decrease by 0.82104% for every 1% increase in the weight of a vehicle.\nSignif. codes: section displays the significance level of the coefficients based on their respective p-values. The *** symbols indicate that both the intercept and slope of the log-log regression line are highly significant in this instance.\nResidual standard error: line displays the residual standard error, a measure of the residuals’ variability around the regression line. In this instance, the residual standard error is 0.277, indicating that the actual MPG values for each vehicle in the dataset vary from the predicted values by approximately 0.277.\nMultiple R-squared: line indicates the R-squared value, which is the proportion of the variation in the dependent variable (in this case, mpg) that is explained by the independent variable. (wt in this case). The R-squared value in this instance is 0.7578, indicating that the weight of a vehicle explains 75.78% of the variation in miles per gallon. Adjusted R-squared: line represents the adjusted R-squared value, which accounts for the number of independent variables in the model. The adjusted R-squared value in this instance is 0.7464.\nF-statistic: line displays the F-statistic and corresponding p-value, which test the model’s overall significance. In this instance, the F-statistic with 1 and 30 degrees of freedom is 43.95, and the corresponding p-value is 1.79e-06, indicating that the model is highly significant.\nRegenerate response"
  },
  {
    "objectID": "10LogLog.html#references",
    "href": "10LogLog.html#references",
    "title": "Log-Log Regression",
    "section": "",
    "text": "[1] Stock, J. H., & Watson, M. W. (2002). Introduction to econometrics. New York: Addison Wesley.\nJaccard, J., & Turrisi, R. (2003). Interaction effects in multiple regression. Sage Publications.\nLong, J. S. (1997). Regression models for categorical and limited dependent variables. Sage Publications.\nGreene, W. H. (2012). Econometric analysis. Pearson Education.\nHill, R. C., Griffiths, W. E., & Lim, G. C. (2018). Principles of econometrics. John Wiley & Sons.\n[2] Anderson, E. T., & Simester, D. I. (2011). Price elasticity of demand in online retail markets. Journal of Marketing Research, 48(2), 316-327.\nTellis, G. J. (2004). Effective advertising: Understanding when, how, and why advertising works. Sage Publications.\nRust, R. T., Zeithaml, V. A., & Lemon, K. N. (2004). Customer-centered brand management. Harvard Business Review, 82(9), 110-118.\nWedel, M., & Kamakura, W. A. (2001). Market segmentation: Conceptual and methodological foundations. Kluwer Academic Publishers.\nKim, K. H., & Kumar, V. (2014). An empirical examination of the determinants of retail prices and promotional markdowns using store-level data. Journal of Retailing, 90(1), 43-55.\n[3] Fama, E. F., & French, K. R. (1992). The cross-section of expected stock returns. The Journal of Finance, 47(2), 427-465.\nAlexander, C. (2008). Market risk analysis, value at risk models. Wiley Finance.\nBlack, F., & Scholes, M. (1973). The pricing of options and corporate liabilities. Journal of Political Economy, 81(3), 637-654.\nDuffie, D., & Singleton, K. (1999). Modeling term structures of defaultable bonds. Review of Financial Studies, 12(4), 687-720.\nHull, J. (2018). Options, futures, and other derivatives. Pearson Education.\n[4] Hom, P. W., Caranikas-Walker, F., Prussia, G. E., & Griffeth, R. W. (1992). A meta-analytical structural equations analysis of a model of employee turnover. Journal of Applied Psychology, 77(6), 890-909.\nGriffin, M. A., & Neal, A. (2000). Perceptions of safety at work: A framework for linking safety climate to safety performance, knowledge, and motivation. Journal of Occupational Health Psychology, 5(3), 347-358.\nSchein, E. H. (2010). Organizational culture and leadership (4th ed.). John Wiley & Sons.\nAvolio, B. J., & Bass, B. M. (1991). The full range leadership development program: Manual for the multifactor leadership questionnaire. Mind Garden.\nCascio, W. F. (1991). Costing human resources: The financial impact of behavior in organizations (3rd ed.). PWS-Kent Publishing Company.\n[5] Wooldridge, J. M. (2015). Introductory econometrics: A modern approach. Nelson Education.\nGujarati, D. N. (2003). Basic econometrics (4th ed.). McGraw-Hill.\nStock, J. H., & Watson, M. W. (2002). Introduction to econometrics. Addison Wesley.\nHill, R. C., Griffiths, W. E., & Lim, G. C. (2018). Principles of econometrics (5th ed.). John Wiley & Sons.\nLong, J. S. (1997). Regression models for categorical and limited dependent variables. Sage Publications.\n[6]"
  },
  {
    "objectID": "05ANOVA.html",
    "href": "05ANOVA.html",
    "title": "ANOVA",
    "section": "",
    "text": "July 26, 2023\n\nANOVA (Analysis of Variance) is a statistical technique used to determine whether the means of two or more groups differ significantly from one another. It is employed to establish whether the mean difference is actually different or merely the result of chance.\nThere are several types of ANOVA, including One-Way ANOVA, Two-Way ANOVA, and Multi-Way ANOVA.\n\n\nOne-Way ANOVA is used to compare the means of two or more groups.\nTwo-Way ANOVA is used to compare the means of two or more groups while taking the influence of two independent factors into account.\nMulti-Way ANOVA is used to compare the means of two or more groups while taking the influence of more than two factors into account.\n\n\nANOVA works by dividing the total variance in a set of data into two parts:\n\n\nthe variance between the groups (between-group variance)\nthe variance within the groups (within-group variance).\n\n\nCalculating the F-ratio, which is the ratio of the between-group variance to the within-group variance, reveals whether there is a significant difference between the group averages.\nANOVA is frequently used to compare means and draw conclusions about populations from sample data in various disciplines, including economics, psychology, and marketing. Making data-driven, educated judgements based on statistical evidence is possible with this useful tool for spotting significant variations across groups.\n\n\n\nIt can be applied in various business applications such as:\n\nComparing the average sales of a product across various geographies or demographic groups to determine which group has the greatest average.\nHuman resource management: To assess whether there is a substantial variance in the median employee salary across various departments or job positions.\nQuality control: Quality control involves comparing the average number of product flaws across various production lines or manufacturing procedures in order to identify the one that results in the fewest flaws.\nCustomer satisfaction: To analyse the average customer satisfaction scores across several product or service offerings and identify which one is producing the most positive feedback from customers.Business Applications of Two-Way ANOVA\n\nIt can also be applied in various business applications such as:\n\nMarket research: To assess the impact on sales of two independent variables, such as product and location, and to ascertain whether the two variables interact.\nHuman resource management: To investigate whether there is an interaction between two independent variables—such as job position and training program—and employee performance.\nManufacturing: Assess whether there is an interaction between the two factors by examining the impact of two independent variables, such as the production process and the source of raw materials, on product quality.\nCustomer satisfaction: To investigate whether there is an interaction between two independent variables, such as product line and mode of customer service (online, phone, in-person), on customer satisfaction.\n\nBy using ANOVA, businesses can gain deeper insights into the relationships between one or more independent variables and the dependent variable, allowing them to make data-driven decisions and improve their business.\n\n\n\nBoth the t-test and the analysis of variance (ANOVA) are statistical techniques that are used to compare the means of two or more groups and ascertain whether there is a statistically significant difference between them. ANOVA and t-test, however, differ significantly in the following ways:\n\nNumber of groups: The t-test is used to compare the means of just two groups, whereas ANOVA can be used to compare the means of two or more groups.\nAssumptions: ANOVA makes the assumptions that the data are independent, normally distributed, and have similar variances across groups. The T-test presumes that the data are independent but not necessarily normally distributed with equal variances.\nMultiple comparisons: ANOVA permits numerous comparisons of the means of different groups, whereas the t-test necessitates several tests and is hence more prone to Type I errors.\nPower: When there are more than two groups, ANOVA has greater potential to identify significant differences between group means. When there are more than two groups, the T-test has lower power."
  },
  {
    "objectID": "05ANOVA.html#business-applications-of-one-way-anova",
    "href": "05ANOVA.html#business-applications-of-one-way-anova",
    "title": "ANOVA",
    "section": "",
    "text": "It can be applied in various business applications such as:\n\nComparing the average sales of a product across various geographies or demographic groups to determine which group has the greatest average.\nHuman resource management: To assess whether there is a substantial variance in the median employee salary across various departments or job positions.\nQuality control: Quality control involves comparing the average number of product flaws across various production lines or manufacturing procedures in order to identify the one that results in the fewest flaws.\nCustomer satisfaction: To analyse the average customer satisfaction scores across several product or service offerings and identify which one is producing the most positive feedback from customers.Business Applications of Two-Way ANOVA\n\nIt can also be applied in various business applications such as:\n\nMarket research: To assess the impact on sales of two independent variables, such as product and location, and to ascertain whether the two variables interact.\nHuman resource management: To investigate whether there is an interaction between two independent variables—such as job position and training program—and employee performance.\nManufacturing: Assess whether there is an interaction between the two factors by examining the impact of two independent variables, such as the production process and the source of raw materials, on product quality.\nCustomer satisfaction: To investigate whether there is an interaction between two independent variables, such as product line and mode of customer service (online, phone, in-person), on customer satisfaction.\n\nBy using ANOVA, businesses can gain deeper insights into the relationships between one or more independent variables and the dependent variable, allowing them to make data-driven decisions and improve their business."
  },
  {
    "objectID": "05ANOVA.html#anova-versus-t-test",
    "href": "05ANOVA.html#anova-versus-t-test",
    "title": "ANOVA",
    "section": "",
    "text": "Both the t-test and the analysis of variance (ANOVA) are statistical techniques that are used to compare the means of two or more groups and ascertain whether there is a statistically significant difference between them. ANOVA and t-test, however, differ significantly in the following ways:\n\nNumber of groups: The t-test is used to compare the means of just two groups, whereas ANOVA can be used to compare the means of two or more groups.\nAssumptions: ANOVA makes the assumptions that the data are independent, normally distributed, and have similar variances across groups. The T-test presumes that the data are independent but not necessarily normally distributed with equal variances.\nMultiple comparisons: ANOVA permits numerous comparisons of the means of different groups, whereas the t-test necessitates several tests and is hence more prone to Type I errors.\nPower: When there are more than two groups, ANOVA has greater potential to identify significant differences between group means. When there are more than two groups, the T-test has lower power."
  },
  {
    "objectID": "05ANOVA.html#running-the-one-way-anova-in-r",
    "href": "05ANOVA.html#running-the-one-way-anova-in-r",
    "title": "ANOVA",
    "section": "Running the One-Way ANOVA in R",
    "text": "Running the One-Way ANOVA in R\n\nReading the Data\n\n\ndata(mtcars)\nattach(mtcars)\n\n\nConvert the categorical variables into factor variables\n\n\nmtcars$cyl &lt;- as.factor(mtcars$cyl)\nmtcars$am &lt;- as.factor(mtcars$am)\nmtcars$gear &lt;- as.factor(mtcars$gear)\nmtcars$vs &lt;- as.factor(mtcars$vs)\n\n\nNull Hypothesis\n\nTo compare whether the mean weight of the cars for cylinders (cyl=4,6,8) is significantly different or not.\nH0: The mean weight of cars having different cylinders (cyl=4,6,8) are not significantly different\n\nRunning the One-Way ANOVA in R\n\n\n# Computing one way ANOVA\nAnovaOneWay &lt;- aov(wt~cyl, data = mtcars)\nsummary(AnovaOneWay)\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \ncyl          2  18.18   9.088   22.91 1.07e-06 ***\nResiduals   29  11.50   0.397                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nThe p-value of the test is 1.22e-07, which is less than the significance level alpha = 0.05.\nWe can REJECT the null hypothesis,and conclude that mean weight of the cars for different cylinders (cyl=4,6,8) are significantly different."
  },
  {
    "objectID": "05ANOVA.html#pairwise-comparision-using-tuckys-post-hoc-test",
    "href": "05ANOVA.html#pairwise-comparision-using-tuckys-post-hoc-test",
    "title": "ANOVA",
    "section": "Pairwise comparision using Tucky’s Post-Hoc test",
    "text": "Pairwise comparision using Tucky’s Post-Hoc test\n\nAfter running an ANOVA, the Tukey’s post-hoc test is performed to discover which pairs of group means are substantially different from one another (Analysis of Variance).\nIt is a strategy for reducing the likelihood of at least one Type I error (false positive) among all comparisons, also known as the family-wise error rate.\nThe Tukey test compares the means of all potential group pairs and modifies the significance level based on the number of comparisons.\nIt provides a confidence interval for the difference between the means of the two groups as well as a p-value for each pairwise comparison. The difference between the means is deemed significant if the p-value is smaller than the significance level (for example, 0.05).\n\n\n## Tukey multiple comparisons of means\ntukey.test &lt;- TukeyHSD(AnovaOneWay)\ntukey.test\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = wt ~ cyl, data = mtcars)\n\n$cyl\n         diff        lwr      upr     p adj\n6-4 0.8314156 0.07939155 1.583440 0.0278777\n8-4 1.7134870 1.08680032 2.340174 0.0000006\n8-6 0.8820714 0.16206323 1.602080 0.0138630"
  },
  {
    "objectID": "05ANOVA.html#running-the-two-way-anova-in-r",
    "href": "05ANOVA.html#running-the-two-way-anova-in-r",
    "title": "ANOVA",
    "section": "Running the Two-Way ANOVA in R",
    "text": "Running the Two-Way ANOVA in R\nWe shall be using mtcars data set to demonstrate Two-Way ANOVA. We will compare whether the mean weight of the cars for different cylinders (cyl=4,6,8) & Transmission Type (am = 0,1) are significantly different or not.\n\nHypothesis\n\n\nH0: The mean weight of the cars for different cylinders (cyl=4,6,8) & Transmission Type (am = 0,1) is not significantly different\nH1: The mean weight of the cars for different cylinders (cyl=4,6,8) & Transmission Type (am = 0,1) is significantly different\n\n\nTwo-Way ANOVA\n\n\n# Computing two way ANOVA\nAnovaTwoWay &lt;- aov(wt ~ am + cyl, data = mtcars)\nsummary(AnovaTwoWay)\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nam           1 14.232  14.232   48.49 1.43e-07 ***\ncyl          2  7.228   3.614   12.31 0.000146 ***\nResiduals   28  8.219   0.294                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nThe p-value of the test is 0.000146, which is less than the significance level alpha = 0.05.\nWe can reject the null hypothesis\nWe conclude that mean weight of the cars for different cylinders (cyl=4,6,8) & Transmissions (am = 0,1) are significantly different\n\nPairwise comparison using Tukey Post-Hoc test\n\n## Tukey multiple comparisons of means\ntukey.test &lt;- TukeyHSD(AnovaTwoWay)\ntukey.test\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = wt ~ am + cyl, data = mtcars)\n\n$am\n         diff       lwr        upr p adj\n1-0 -1.357895 -1.757343 -0.9584467 1e-07\n\n$cyl\n         diff        lwr      upr     p adj\n6-4 0.4258107 -0.2223305 1.073952 0.2518163\n8-4 0.9199122  0.3797945 1.460030 0.0006710\n8-6 0.4941015 -0.1264464 1.114649 0.1383557"
  },
  {
    "objectID": "05ANOVA.html#two-way-anova-and-interactions",
    "href": "05ANOVA.html#two-way-anova-and-interactions",
    "title": "ANOVA",
    "section": "Two-Way ANOVA and Interactions",
    "text": "Two-Way ANOVA and Interactions\n\nTo determine whether there is an interaction between the variables, the investigator manipulates two independent variables, each with numerous levels.\nAfter measuring the dependent variable, the means of the groups created by the combination of the two independent variables are assessed.\nThis enables the experimenter to assess whether there is a meaningful interaction between the two independent factors and whether the magnitude of one independent variable’s impact on the dependent variable is reliant upon the magnitude of the other independent variable\nThe main difference between two-way ANOVA with and without interactions is the way that the effects of the independent variables are analyzed.\nTwo-way ANOVA without interactions:\n\n\nThe main effects of each independent variable are examined separately in this particular sort of two-way ANOVA, without taking into account any potential interactions between the two independent variables.\nThis is also known as a main effects model.\nThe main purpose of this type of analysis is to determine if there is a significant effect of each independent variable on the dependent variable.\n\n\nTwo-way ANOVA with interactions:\n\n\nIn this type of two-way ANOVA, the possible interaction between the two independent variables is taken into account.\nWhen two independent variables interact, the impact of one on the dependent variable is influenced by the level of the other independent variable.\nThis form of analysis is used to assess if there is a significant interaction between the independent variables and if the level of one independent variable affects the level of the other independent variable when it comes to its impact on the dependent variable."
  },
  {
    "objectID": "05ANOVA.html#running-the-two-way-anova-with-interaction-in-r",
    "href": "05ANOVA.html#running-the-two-way-anova-with-interaction-in-r",
    "title": "ANOVA",
    "section": "Running the Two-Way ANOVA With Interaction in R",
    "text": "Running the Two-Way ANOVA With Interaction in R\n\n# Computing two way ANOVA\nAnovaTwoWayInt &lt;- aov(wt ~ am*cyl, data = mtcars)\nsummary(AnovaTwoWayInt)\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nam           1 14.232  14.232  45.394 3.78e-07 ***\ncyl          2  7.228   3.614  11.527 0.000261 ***\nam:cyl       2  0.067   0.033   0.106 0.899371    \nResiduals   26  8.152   0.314                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "05ANOVA.html#pairwise-comparision-using-tuckys-post-hoc-test-1",
    "href": "05ANOVA.html#pairwise-comparision-using-tuckys-post-hoc-test-1",
    "title": "ANOVA",
    "section": "Pairwise comparision using Tucky’s Post-Hoc test",
    "text": "Pairwise comparision using Tucky’s Post-Hoc test\n\n## Tukey multiple comparisons of means\ntukey.test &lt;- TukeyHSD(AnovaTwoWayInt)\ntukey.test\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = wt ~ am * cyl, data = mtcars)\n\n$am\n         diff      lwr        upr p adj\n1-0 -1.357895 -1.77217 -0.9436199 4e-07\n\n$cyl\n         diff        lwr      upr     p adj\n6-4 0.4258107 -0.2469135 1.098535 0.2750199\n8-4 0.9199122  0.3593087 1.480516 0.0010767\n8-6 0.4941015 -0.1499828 1.138186 0.1570181\n\n$`am:cyl`\n              diff         lwr       upr     p adj\n1:4-0:4 -0.8927500 -2.05745123 0.2719512 0.2090063\n0:6-0:4  0.4537500 -0.86021020 1.7677102 0.8921575\n1:6-0:4 -0.1800000 -1.58468254 1.2246825 0.9986272\n0:8-0:4  1.1690833  0.05858428 2.2795824 0.0348194\n1:8-0:4  0.4350000 -1.13548282 2.0054828 0.9546454\n0:6-1:4  1.3465000  0.29298810 2.4000119 0.0066940\n1:6-1:4  0.7127500 -0.45195123 1.8774512 0.4357758\n0:8-1:4  2.0618333  1.27659192 2.8470747 0.0000002\n1:8-1:4  1.3277500 -0.03232802 2.6878280 0.0586249\n1:6-0:6 -0.6337500 -1.94771020 0.6802102 0.6782063\n0:8-0:6  0.7153333 -0.27792721 1.7085939 0.2660287\n1:8-0:6 -0.0187500 -1.50864082 1.4711408 1.0000000\n0:8-1:6  1.3490833  0.23858428 2.4595824 0.0107805\n1:8-1:6  0.6150000 -0.95548282 2.1854828 0.8313295\n1:8-0:8 -0.7340833 -2.04804353 0.5798769 0.5337008"
  },
  {
    "objectID": "05ANOVA.html#limitations-of-anova",
    "href": "05ANOVA.html#limitations-of-anova",
    "title": "ANOVA",
    "section": "Limitations of ANOVA",
    "text": "Limitations of ANOVA\n\nAssumptions: ANOVA makes the assumptions that the data are independent, normally distributed, and have similar variances across groups. The ANOVA results could be suspect if these presumptions are not true.\nLimited to comparing means: ANOVA cannot be used to draw conclusions about other measures of central tendency, such as medians or modes, and is only capable of comparing the means of two or more groups.\nNon-linear relationships: ANOVA presupposes that the independent and dependent variables have a linear relationship. ANOVA might not be appropriate if there is a non-linear relationship.\nType I and Type II errors: ANOVA is sensitive to Type I (false positive) and Type II (false negative) mistakes, like all statistical tests. It is crucial to select the proper level of significance and interpret the findings in light of the sample size and data variability.\nMultiple comparisons: Multiple comparisons can raise the possibility of Type I errors when comparing the means of more than two groups. Adjustments to the significance level, like the Bonferroni correction, can be applied to overcome this problem."
  },
  {
    "objectID": "05ANOVA.html#levenes-test-for-homogeneity-of-variance",
    "href": "05ANOVA.html#levenes-test-for-homogeneity-of-variance",
    "title": "ANOVA",
    "section": "Levene’s Test for Homogeneity Of Variance",
    "text": "Levene’s Test for Homogeneity Of Variance\n\nANOVA makes the assumption that the data are normally distributed and that there is homogeneous variation between groups.\nA statistical test called the Levene’s test is used to evaluate the equality of variances between two or more groups.\nThe validity of the ANOVA results may be affected if there are appreciable variance differences across groups, which can be found using Levene’s test.\nThe null hypothesis is that the variances are equal across all groups, and the test statistic is computed from the absolute deviations of the data from their group averages.\nThe null hypothesis is rejected and it is concluded that the variances are not equal if the test’s p-value is less than a predetermined level of significance. The Welch’s ANOVA or the Brown-Forsythe test should be used as alternatives to analysis of variance in this situation.\nRunning the Levene’s Test for Homogeneity of Variance in R\n\n\nlibrary(car)\n\nLoading required package: carData\n\n# Levene's test\nleveneTest(wt~cyl, data = mtcars)\n\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value Pr(&gt;F)\ngroup  2  0.4995  0.612\n      29               \n\n\n\nFrom the output above we can see that the p-value is more than the significance level of 0.05.\nThis means that there is no evidence to suggest that the variance in weights of the cars across three type of cylinder cars are statistically significantly different.\nTherefore, we can assume the homogeneity of variance in weights across three type of cylinder cars."
  },
  {
    "objectID": "05ANOVA.html#welch-one-way-test",
    "href": "05ANOVA.html#welch-one-way-test",
    "title": "ANOVA",
    "section": "Welch One-Way Test",
    "text": "Welch One-Way Test\n\nWhen the assumption of equal variances (homogeneity of variance) across groups is not met, the Welch One-Way Test, a variation of the one-way ANOVA (Analysis of Variance) test, is utilised.\nThe Welch test modifies the degrees of freedom and the test statistic to take into account unequal variances, in contrast to the conventional one-way ANOVA, which assumes equal variances.\nWhen the assumption of equal variances is not fulfilled, the Welch test provides a helpful substitute for the conventional one-way ANOVA. It can improve test power and yield more accurate results.\nThe Welch test involves dividing the sum of squares of the variance-adjusted variance between the group means and the overall mean. The difference between the means is regarded as significant if the test statistic is higher than the critical value.Running the Welch One-Way Test in R\n\n\n# ANOVA test with no assumption of equal variances\noneway.test(wt ~ cyl, data = mtcars)\n\n\n    One-way analysis of means (not assuming equal variances)\n\ndata:  wt and cyl\nF = 20.249, num df = 2.000, denom df = 18.974, p-value = 1.963e-05"
  },
  {
    "objectID": "05ANOVA.html#normal-distribution",
    "href": "05ANOVA.html#normal-distribution",
    "title": "ANOVA",
    "section": "Normal Distribution",
    "text": "Normal Distribution\nANOVA assumes that, the data are normally distributed and the variance across groups are homogeneous.\n\nNormality plot of residuals\n\n\n# normality\nplot(AnovaOneWay, 2)\n\n\n\n\n\nShapiro-Wilk Test\n\n\nThe Shapiro-Wilk test is a statistical test used to determine whether a sample of data comes from a normally distributed population.\nIf the test’s p-value is less than a set significance level, the null hypothesis is rejected and it can be inferred that the data did not come from a normal distribution. This procedure tests the null hypothesis that the data were taken from a normal distribution.\nThe Shapiro-Wilk test is widely used due to its relatively high power in detecting deviations from normality, especially for small sample sizes.\n\n\n# extract the residuals\naov_residuals &lt;- residuals(object = AnovaOneWay )\n# run Shapiro-Wilk test\nshapiro.test(x = aov_residuals )\n\n\n    Shapiro-Wilk normality test\n\ndata:  aov_residuals\nW = 0.9025, p-value = 0.007175"
  },
  {
    "objectID": "05ANOVA.html#kruskal-wallis-rank-sum-test",
    "href": "05ANOVA.html#kruskal-wallis-rank-sum-test",
    "title": "ANOVA",
    "section": "Kruskal-Wallis Rank Sum Test",
    "text": "Kruskal-Wallis Rank Sum Test\n\nA non-parametric statistical test called the Kruskal-Wallis rank sum test is used to examine if the population medians of two or more distinct groups are equal.\nThe Kruskal-Wallis test can be used to evaluate data that is not regularly distributed because, unlike the parametric one-way ANOVA, it does not assume that the underlying populations are normally distributed.\nIt is a distribution-free test since the test statistic is dependent on the ranking of the observations rather than their actual values.\nIt can be assumed that at least one of the groups has a difference median from the other groups if the p-value produced from the test is less than a predefined significance level.\nWhen the normality assumption cannot be met, a strong and popular substitute for the one-way ANOVA is the Kruskal-Wallis test.\n\n\n# kruskal-Wallis Rank Sum test\nkruskal.test(wt ~ cyl, data = mtcars)\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  wt by cyl\nKruskal-Wallis chi-squared = 22.807, df = 2, p-value = 1.116e-05"
  },
  {
    "objectID": "05ANOVA.html#friedman-test",
    "href": "05ANOVA.html#friedman-test",
    "title": "ANOVA",
    "section": "Friedman Test",
    "text": "Friedman Test\n\nThe Friedman test is a non-parametric alternative to the two-way ANOVA for repeated measures, and is used to test for differences among several related samples or repeated measurements on a single sample.\nThe Friedman test assumes that the response variable is ordinal.\nIn that it examines differences between groups, the Friedman test is comparable to a two-way ANOVA, but unlike the latter, it does not rely on any presumptions regarding the distribution of the response variable. Since the data are not normally distributed, the Friedman test becomes a helpful substitute."
  },
  {
    "objectID": "09LogLinear.html",
    "href": "09LogLinear.html",
    "title": "Log-Linear Regression",
    "section": "",
    "text": "July 26, 2023\n\n\n\n\n\n\ndata(mtcars)\nattach(mtcars)\n\n\n\n\n\n\n\nplot(wt\n     , mpg\n     , xlab=\"weight\", ylab=\"mpg\")\n\n\n\n\n\n\n\n\n\n\n\nfit &lt;- lm(mpg ~ wt, data = mtcars)\nsummary(fit)\n\n\nCall:\nlm(formula = mpg ~ wt, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.5432 -2.3647 -0.1252  1.4096  6.8727 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  37.2851     1.8776  19.858  &lt; 2e-16 ***\nwt           -5.3445     0.5591  -9.559 1.29e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.046 on 30 degrees of freedom\nMultiple R-squared:  0.7528,    Adjusted R-squared:  0.7446 \nF-statistic: 91.38 on 1 and 30 DF,  p-value: 1.294e-10\n\n\n\nplot(mtcars$wt,\n     mtcars$mpg,\n     main=\"\",\n     xlab=\"wt\",\n     ylab=\"mpg\")\nlines(mtcars$wt, fitted(fit))\n\n\n\n\n\n\n\n\nfit$coefficients\n\n(Intercept)          wt \n  37.285126   -5.344472 \n\n\n\n\n\n\nfitted(fit)\n\n          Mazda RX4       Mazda RX4 Wag          Datsun 710      Hornet 4 Drive \n          23.282611           21.919770           24.885952           20.102650 \n  Hornet Sportabout             Valiant          Duster 360           Merc 240D \n          18.900144           18.793255           18.205363           20.236262 \n           Merc 230            Merc 280           Merc 280C          Merc 450SE \n          20.450041           18.900144           18.900144           15.533127 \n         Merc 450SL         Merc 450SLC  Cadillac Fleetwood Lincoln Continental \n          17.350247           17.083024            9.226650            8.296712 \n  Chrysler Imperial            Fiat 128         Honda Civic      Toyota Corolla \n           8.718926           25.527289           28.653805           27.478021 \n      Toyota Corona    Dodge Challenger         AMC Javelin          Camaro Z28 \n          24.111004           18.472586           18.926866           16.762355 \n   Pontiac Firebird           Fiat X1-9       Porsche 914-2        Lotus Europa \n          16.735633           26.943574           25.847957           29.198941 \n     Ford Pantera L        Ferrari Dino       Maserati Bora          Volvo 142E \n          20.343151           22.480940           18.205363           22.427495 \n\n\n\n\n\nResiduals are the vertical distances between the data and the fitted line. The Ordinary Least Squares (OLS) method minimizes the residuals. In OLS, the accuracy of a line through the sample data points is measured by the sum of squared residuals, and the goal is to make this sum as small as possible.\n\nresiduals(fit)\n\n          Mazda RX4       Mazda RX4 Wag          Datsun 710      Hornet 4 Drive \n         -2.2826106          -0.9197704          -2.0859521           1.2973499 \n  Hornet Sportabout             Valiant          Duster 360           Merc 240D \n         -0.2001440          -0.6932545          -3.9053627           4.1637381 \n           Merc 230            Merc 280           Merc 280C          Merc 450SE \n          2.3499593           0.2998560          -1.1001440           0.8668731 \n         Merc 450SL         Merc 450SLC  Cadillac Fleetwood Lincoln Continental \n         -0.0502472          -1.8830236           1.1733496           2.1032876 \n  Chrysler Imperial            Fiat 128         Honda Civic      Toyota Corolla \n          5.9810744           6.8727113           1.7461954           6.4219792 \n      Toyota Corona    Dodge Challenger         AMC Javelin          Camaro Z28 \n         -2.6110037          -2.9725862          -3.7268663          -3.4623553 \n   Pontiac Firebird           Fiat X1-9       Porsche 914-2        Lotus Europa \n          2.4643670           0.3564263           0.1520430           1.2010593 \n     Ford Pantera L        Ferrari Dino       Maserati Bora          Volvo 142E \n         -4.5431513          -2.7809399          -3.2053627          -1.0274952 \n\n\n\n\n\n\nThe regression coefficient (3.45) is significantly dfferent from zero (p &lt; 0.001)\nThere is an expected increase of 3.45 lbs of weight for every 1 inch increases in height.\n\n\n\n\n\n# extacting the coefficient of determination\nsummary(fit)$r.squared \n\n[1] 0.7528328\n\n\n\n\n\n\nnewdata = data.frame(wt = 4)\npredict(fit, newdata, interval = \"confidence\") \n\n       fit      lwr      upr\n1 15.90724 14.49018 17.32429\n\n\n\n\n\nThe F-statistic tests whether the predictor variables, taken together,predict the response variable.\n\n\n\n\n\nfit2 &lt;- lm(log(mpg) ~ wt, data = mtcars)\nsummary(fit2)\n\n\nCall:\nlm(formula = log(mpg) ~ wt, data = mtcars)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.210346 -0.085932 -0.006136  0.061335  0.308623 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.83191    0.08396   45.64  &lt; 2e-16 ***\nwt          -0.27178    0.02500  -10.87 6.31e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1362 on 30 degrees of freedom\nMultiple R-squared:  0.7976,    Adjusted R-squared:  0.7908 \nF-statistic: 118.2 on 1 and 30 DF,  p-value: 6.31e-12\n\n\nThe beta estimate for wt in the above regression is -0.27178.\nThis means that for a one-unit increase in the weight of a car, the expected value of the log of its miles per gallon (mpg) is expected to decrease by -0.27178 units, after controlling for other variables in the model.\nSince the response variable (log(mpg)) is on a logarithmic scale, the beta estimate for wt indicates a multiplicative effect on the mpg, rather than an additive effect.\n\nexp(-0.27178)\n\n[1] 0.7620219\n\n\nSpecifically, we can interpret the beta estimate as suggesting that the expected ratio of mpg for two cars that differ by one unit in weight is e^(-0.27178) = 0.7620219\nIn other words, we would expect the car with the higher weight to have a fuel efficiency that is approximately 76.2% lower than the car with the lower weight, after controlling for other factors in the model.\n\n\n\n\n# Convert am to a factor variable\nmtcars$am &lt;- factor(mtcars$am, labels = c(\"Automatic\", \"Manual\"))\n\nfit3a &lt;- lm((mpg) ~ wt + am, data = mtcars)\nsummary(fit3a)\n\n\nCall:\nlm(formula = (mpg) ~ wt + am, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.5295 -2.3619 -0.1317  1.4025  6.8782 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 37.32155    3.05464  12.218 5.84e-13 ***\nwt          -5.35281    0.78824  -6.791 1.87e-07 ***\namManual    -0.02362    1.54565  -0.015    0.988    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.098 on 29 degrees of freedom\nMultiple R-squared:  0.7528,    Adjusted R-squared:  0.7358 \nF-statistic: 44.17 on 2 and 29 DF,  p-value: 1.579e-09\n\n\n\n\n\n\nfit3b &lt;- lm(log(mpg) ~ wt + am, data = mtcars)\nsummary(fit3b)\n\n\nCall:\nlm(formula = log(mpg) ~ wt + am, data = mtcars)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.21351 -0.08281  0.00304  0.04962  0.32349 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.89834    0.13567  28.734  &lt; 2e-16 ***\nwt          -0.28699    0.03501  -8.198 4.87e-09 ***\namManual    -0.04307    0.06865  -0.627    0.535    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1376 on 29 degrees of freedom\nMultiple R-squared:  0.8003,    Adjusted R-squared:  0.7865 \nF-statistic:  58.1 on 2 and 29 DF,  p-value: 7.186e-11\n\n\n\nexp(-0.04307)\n\n[1] 0.9578443\n\n\n\nexp(-0.2869)\n\n[1] 0.7505868\n\n\nThe beta estimate of amManual in the above regression is -0.04307. This means that, after controlling for weight, the expected value of the log of the miles per gallon (mpg) for a car with a manual transmission is expected to be -0.04307 units lower than for a car with an automatic transmission.\nSince the am variable was converted to a factor variable, the beta estimate for amManual represents the difference in the expected value of the response variable between cars with manual and automatic transmissions, after controlling for weight. In this case, the beta estimate indicates that cars with manual transmissions are expected to have a higher fuel efficiency than cars with automatic transmissions, all else being equal.\nIt’s important to note that the p-value for the amManual coefficient is greater than the conventional threshold of 0.05 for statistical significance. This means that we cannot reject the null hypothesis that the amManual coefficient is equal to zero, and thus the difference in fuel efficiency between manual and automatic transmissions may not be statistically significant. However, this should be interpreted with caution, as it’s possible that the sample size or other factors in the data are affecting the p-value."
  },
  {
    "objectID": "09LogLinear.html#reading-the-data",
    "href": "09LogLinear.html#reading-the-data",
    "title": "Log-Linear Regression",
    "section": "",
    "text": "data(mtcars)\nattach(mtcars)"
  },
  {
    "objectID": "09LogLinear.html#predict-a-mpg-of-car-from-their-weight",
    "href": "09LogLinear.html#predict-a-mpg-of-car-from-their-weight",
    "title": "Log-Linear Regression",
    "section": "",
    "text": "plot(wt\n     , mpg\n     , xlab=\"weight\", ylab=\"mpg\")"
  },
  {
    "objectID": "09LogLinear.html#fitting-a-linear-model",
    "href": "09LogLinear.html#fitting-a-linear-model",
    "title": "Log-Linear Regression",
    "section": "",
    "text": "fit &lt;- lm(mpg ~ wt, data = mtcars)\nsummary(fit)\n\n\nCall:\nlm(formula = mpg ~ wt, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.5432 -2.3647 -0.1252  1.4096  6.8727 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  37.2851     1.8776  19.858  &lt; 2e-16 ***\nwt           -5.3445     0.5591  -9.559 1.29e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.046 on 30 degrees of freedom\nMultiple R-squared:  0.7528,    Adjusted R-squared:  0.7446 \nF-statistic: 91.38 on 1 and 30 DF,  p-value: 1.294e-10\n\n\n\nplot(mtcars$wt,\n     mtcars$mpg,\n     main=\"\",\n     xlab=\"wt\",\n     ylab=\"mpg\")\nlines(mtcars$wt, fitted(fit))\n\n\n\n\n\n\n\n\nfit$coefficients\n\n(Intercept)          wt \n  37.285126   -5.344472 \n\n\n\n\n\n\nfitted(fit)\n\n          Mazda RX4       Mazda RX4 Wag          Datsun 710      Hornet 4 Drive \n          23.282611           21.919770           24.885952           20.102650 \n  Hornet Sportabout             Valiant          Duster 360           Merc 240D \n          18.900144           18.793255           18.205363           20.236262 \n           Merc 230            Merc 280           Merc 280C          Merc 450SE \n          20.450041           18.900144           18.900144           15.533127 \n         Merc 450SL         Merc 450SLC  Cadillac Fleetwood Lincoln Continental \n          17.350247           17.083024            9.226650            8.296712 \n  Chrysler Imperial            Fiat 128         Honda Civic      Toyota Corolla \n           8.718926           25.527289           28.653805           27.478021 \n      Toyota Corona    Dodge Challenger         AMC Javelin          Camaro Z28 \n          24.111004           18.472586           18.926866           16.762355 \n   Pontiac Firebird           Fiat X1-9       Porsche 914-2        Lotus Europa \n          16.735633           26.943574           25.847957           29.198941 \n     Ford Pantera L        Ferrari Dino       Maserati Bora          Volvo 142E \n          20.343151           22.480940           18.205363           22.427495 \n\n\n\n\n\nResiduals are the vertical distances between the data and the fitted line. The Ordinary Least Squares (OLS) method minimizes the residuals. In OLS, the accuracy of a line through the sample data points is measured by the sum of squared residuals, and the goal is to make this sum as small as possible.\n\nresiduals(fit)\n\n          Mazda RX4       Mazda RX4 Wag          Datsun 710      Hornet 4 Drive \n         -2.2826106          -0.9197704          -2.0859521           1.2973499 \n  Hornet Sportabout             Valiant          Duster 360           Merc 240D \n         -0.2001440          -0.6932545          -3.9053627           4.1637381 \n           Merc 230            Merc 280           Merc 280C          Merc 450SE \n          2.3499593           0.2998560          -1.1001440           0.8668731 \n         Merc 450SL         Merc 450SLC  Cadillac Fleetwood Lincoln Continental \n         -0.0502472          -1.8830236           1.1733496           2.1032876 \n  Chrysler Imperial            Fiat 128         Honda Civic      Toyota Corolla \n          5.9810744           6.8727113           1.7461954           6.4219792 \n      Toyota Corona    Dodge Challenger         AMC Javelin          Camaro Z28 \n         -2.6110037          -2.9725862          -3.7268663          -3.4623553 \n   Pontiac Firebird           Fiat X1-9       Porsche 914-2        Lotus Europa \n          2.4643670           0.3564263           0.1520430           1.2010593 \n     Ford Pantera L        Ferrari Dino       Maserati Bora          Volvo 142E \n         -4.5431513          -2.7809399          -3.2053627          -1.0274952 \n\n\n\n\n\n\nThe regression coefficient (3.45) is significantly dfferent from zero (p &lt; 0.001)\nThere is an expected increase of 3.45 lbs of weight for every 1 inch increases in height.\n\n\n\n\n\n# extacting the coefficient of determination\nsummary(fit)$r.squared \n\n[1] 0.7528328\n\n\n\n\n\n\nnewdata = data.frame(wt = 4)\npredict(fit, newdata, interval = \"confidence\") \n\n       fit      lwr      upr\n1 15.90724 14.49018 17.32429\n\n\n\n\n\nThe F-statistic tests whether the predictor variables, taken together,predict the response variable."
  },
  {
    "objectID": "09LogLinear.html#model-2-log-linear-model",
    "href": "09LogLinear.html#model-2-log-linear-model",
    "title": "Log-Linear Regression",
    "section": "",
    "text": "fit2 &lt;- lm(log(mpg) ~ wt, data = mtcars)\nsummary(fit2)\n\n\nCall:\nlm(formula = log(mpg) ~ wt, data = mtcars)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.210346 -0.085932 -0.006136  0.061335  0.308623 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.83191    0.08396   45.64  &lt; 2e-16 ***\nwt          -0.27178    0.02500  -10.87 6.31e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1362 on 30 degrees of freedom\nMultiple R-squared:  0.7976,    Adjusted R-squared:  0.7908 \nF-statistic: 118.2 on 1 and 30 DF,  p-value: 6.31e-12\n\n\nThe beta estimate for wt in the above regression is -0.27178.\nThis means that for a one-unit increase in the weight of a car, the expected value of the log of its miles per gallon (mpg) is expected to decrease by -0.27178 units, after controlling for other variables in the model.\nSince the response variable (log(mpg)) is on a logarithmic scale, the beta estimate for wt indicates a multiplicative effect on the mpg, rather than an additive effect.\n\nexp(-0.27178)\n\n[1] 0.7620219\n\n\nSpecifically, we can interpret the beta estimate as suggesting that the expected ratio of mpg for two cars that differ by one unit in weight is e^(-0.27178) = 0.7620219\nIn other words, we would expect the car with the higher weight to have a fuel efficiency that is approximately 76.2% lower than the car with the lower weight, after controlling for other factors in the model."
  },
  {
    "objectID": "09LogLinear.html#model-3a-another-linear-linear-model",
    "href": "09LogLinear.html#model-3a-another-linear-linear-model",
    "title": "Log-Linear Regression",
    "section": "",
    "text": "# Convert am to a factor variable\nmtcars$am &lt;- factor(mtcars$am, labels = c(\"Automatic\", \"Manual\"))\n\nfit3a &lt;- lm((mpg) ~ wt + am, data = mtcars)\nsummary(fit3a)\n\n\nCall:\nlm(formula = (mpg) ~ wt + am, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.5295 -2.3619 -0.1317  1.4025  6.8782 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 37.32155    3.05464  12.218 5.84e-13 ***\nwt          -5.35281    0.78824  -6.791 1.87e-07 ***\namManual    -0.02362    1.54565  -0.015    0.988    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.098 on 29 degrees of freedom\nMultiple R-squared:  0.7528,    Adjusted R-squared:  0.7358 \nF-statistic: 44.17 on 2 and 29 DF,  p-value: 1.579e-09"
  },
  {
    "objectID": "09LogLinear.html#model-3b-another-log-linear-model",
    "href": "09LogLinear.html#model-3b-another-log-linear-model",
    "title": "Log-Linear Regression",
    "section": "",
    "text": "fit3b &lt;- lm(log(mpg) ~ wt + am, data = mtcars)\nsummary(fit3b)\n\n\nCall:\nlm(formula = log(mpg) ~ wt + am, data = mtcars)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.21351 -0.08281  0.00304  0.04962  0.32349 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.89834    0.13567  28.734  &lt; 2e-16 ***\nwt          -0.28699    0.03501  -8.198 4.87e-09 ***\namManual    -0.04307    0.06865  -0.627    0.535    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1376 on 29 degrees of freedom\nMultiple R-squared:  0.8003,    Adjusted R-squared:  0.7865 \nF-statistic:  58.1 on 2 and 29 DF,  p-value: 7.186e-11\n\n\n\nexp(-0.04307)\n\n[1] 0.9578443\n\n\n\nexp(-0.2869)\n\n[1] 0.7505868\n\n\nThe beta estimate of amManual in the above regression is -0.04307. This means that, after controlling for weight, the expected value of the log of the miles per gallon (mpg) for a car with a manual transmission is expected to be -0.04307 units lower than for a car with an automatic transmission.\nSince the am variable was converted to a factor variable, the beta estimate for amManual represents the difference in the expected value of the response variable between cars with manual and automatic transmissions, after controlling for weight. In this case, the beta estimate indicates that cars with manual transmissions are expected to have a higher fuel efficiency than cars with automatic transmissions, all else being equal.\nIt’s important to note that the p-value for the amManual coefficient is greater than the conventional threshold of 0.05 for statistical significance. This means that we cannot reject the null hypothesis that the amManual coefficient is equal to zero, and thus the difference in fuel efficiency between manual and automatic transmissions may not be statistically significant. However, this should be interpreted with caution, as it’s possible that the sample size or other factors in the data are affecting the p-value."
  },
  {
    "objectID": "02ZTests.html#independent-samples-z-test-for-proportions-2",
    "href": "02ZTests.html#independent-samples-z-test-for-proportions-2",
    "title": "Z Tests",
    "section": "",
    "text": "Introduction: The independent samples Z-test for proportions is used to determine if there’s a significant difference between two sample proportions drawn from independent populations.\nExample Scenario: A company might want to know if the proportion of male customers who purchase their product is significantly different from the proportion of female customers.\nApplication: The company collects separate samples of male and female customers, calculates the purchase proportions, and uses an independent samples Z-test to assess the significance of any difference. [8]\n\n\n\nConducting the Test:\n\nHypotheses:\n\nNull Hypothesis (H0): The two population proportions are equal.\nAlternative Hypothesis (Ha): The two population proportions are not equal.\n\nSignificance Level: Typically set at 0.05, it’s the acceptable risk level for incorrectly rejecting the null hypothesis.\nTest Statistic: The test statistic is calculated using the formula: \\[ Z = \\frac{p1 - p2}{\\sqrt{p \\cdot (1 - p) \\cdot (\\frac{1}{n1} + \\frac{1}{n2})}} \\] where ( p1 ) and ( p2 ) are sample proportions, ( n1 ) and ( n2 ) are sample sizes, and ( p ) is the pooled sample proportion: \\[ p = \\frac{x1 + x2}{n1 + n2} \\] ( x1 ) and ( x2 ) are the number of successes in each sample.\nDecision Rule: The null hypothesis is rejected if the absolute value of ( Z ) is greater than the critical value (e.g., 1.96 for a two-tailed test at 0.05 significance level).\nConclusion: Based on the test results, a conclusion is drawn about the difference in population proportions. [8]\n\n\n\n\nScenario:\n\nA researcher studies the purchasing behavior of men and women for a specific product. A sample of 250 men and 350 women is taken, with results showing that 125 men and 210 women purchased the product.\nHypotheses:\n\nNull Hypothesis (H0): p1 = p2\nAlternative Hypothesis (Ha): p1 ≠ p2\n\nwhere (p1) is the proportion of men, and (p2) is the proportion of women purchasing the product.\nSignificance Level: Set at 0.05.\nCritical Value: For a two-tailed test at the 0.05 significance level, the critical value is approximately 1.96.\nTest Statistic Calculation:\nThe test statistic Z is calculated using the formula:\n\\[ Z = \\frac{p1 - p2}{\\sqrt{p \\cdot (1 - p) \\cdot \\left(\\frac{1}{n1} + \\frac{1}{n2}\\right)}} \\]\nwhere ( p ) is the pooled proportion, computed as:\n\\[ p = \\frac{x1 + x2}{n1 + n2} \\]\nUsing the Data:\nFor men: \\[ p1 = \\frac{125}{250} = 0.5 \\]\nFor women: \\[ p2 = \\frac{210}{350} = 0.6 \\]\nPooled proportion: \\[ p = \\frac{125 + 210}{250 + 350} = 0.55 \\]\nTest Statistic:\nCalculation of the Z value:\n\\[ Z = \\frac{0.5 - 0.6}{\\sqrt{0.55 \\cdot (1 - 0.55) \\cdot \\left(\\frac{1}{250} + \\frac{1}{350}\\right)}} \\]\n\nThe calculated Z value is approximately -2.43. This value is negative because the proportion of women who purchased the product (p2 = 0.6) is higher than that of men (p1 = 0.5). However, since we are conducting a two-tailed test, the direction of the difference is not our primary concern. We are interested in whether the absolute value of the Z score is greater than the critical value.\n\nConclusion:\nSince ( |Z| = 2.43 ) is greater than the critical value of 1.96, we reject the null hypothesis. This indicates that there is a significant difference in the purchasing behavior between men and women for this product.\nInterpretation:\nWith a 95% confidence level, we conclude that the proportions of men and women purchasing this product are significantly different. Specifically, the data suggests that women are more likely to purchase this product than men.\n\nThis example demonstrates a scenario where the Z-test reveals a significant difference in purchasing behaviors between the two groups, aligned with your request for a scenario where the proportions are different and the Z-value is positive. Note that even though the calculated Z-value is negative, its absolute value is used for the conclusion.\n\n\n\nDemonstration using simulated data:\nTo perform the test described in the scenario using R, we can use the prop.test function. This function is designed to handle comparisons of proportions. Here’s how we can set it up with the data from the above scenario:\n\nx &lt;- c(125, 210) # number of men and women who purchased the product\nn &lt;- c(250, 350) # total number of men and women\n\n# Perform the two-proportion z-test\ntest_result &lt;- prop.test(x, \n                         n, \n                         alternative = \"two.sided\", \n                         correct = FALSE)\n# Output the results\nprint(test_result)\n\n\n    2-sample test for equality of proportions without continuity correction\n\ndata:  x out of n\nX-squared = 5.9138, df = 1, p-value = 0.01502\nalternative hypothesis: two.sided\n95 percent confidence interval:\n -0.18047113 -0.01952887\nsample estimates:\nprop 1 prop 2 \n   0.5    0.6 \n\n\nNotes about the R code\n\nx contains the number of successes (i.e., purchases) for each group (men first, then women).\nn contains the total number of observations for each group.\nprop.test is called with correct = FALSE to perform a standard two-proportion Z-test without continuity correction, which is appropriate for large sample sizes.\nwe can specify alternative = two.sided in the prop.test function to explicitly indicate that we are performing a two-sided test. However, it’s worth noting that the default setting for the alternative parameter in prop.test is already two.sided\nThe function will return various information, including the Z statistic and the p-value.\n\nInterpreting the output\n\nTest Type: We performed a “2-sample test for equality of proportions without continuity correction”. This means we compared two independent proportions (proportions of men and women who purchased the product in our scenario). The ‘without continuity correction’ part indicates that we didn’t apply any correction for continuity, suitable for large sample sizes.\nData: “x out of n” refers to our input data structure, where x represents the number of successes (men and women who purchased the product) and n represents the total number of observations (total men and women in our sample).\nX-squared: The test statistic value is 5.9138. In a two-proportion Z-test like ours, this statistic follows a chi-square distribution with one degree of freedom under the null hypothesis.\nDegrees of Freedom (df): It’s 1 in our case. For a two-sample test of proportions, the degrees of freedom is typically n-1, where n is the number of groups (2 in our case), but since 2-1 equals 1, our df is 1.\np-value: Our p-value is 0.01502. This measures the strength of the evidence against the null hypothesis. As it’s less than the typical alpha level of 0.05, we reject the null hypothesis, indicating a statistically significant difference between the proportions of men and women purchasing the product.\nAlternative Hypothesis: “two.sided” - This indicates that our test was two-tailed, meaning we were testing for any difference in proportions, not specifically if one proportion was greater or lesser than the other.\n95 Percent Confidence Interval: This interval, ranging from -0.1805 to -0.0195, provides a range of plausible values for the difference in proportions (p1 - p2). Since this interval doesn’t include 0, it suggests that the true difference in proportions is likely non-zero, reinforcing our conclusion from the p-value.\nSample Estimates: These are the observed proportions in our study - prop 1 is 0.5 (proportion of men who purchased the product) and prop 2 is 0.6 (proportion of women who purchased the product).\n\nConclusion: There is a statistically significant difference in the purchasing behavior between men and women for the product in our study, with the proportion of women purchasing the product being higher than that of men.\nAnother Demonstration using simulated data:\nWe can perform a one-sided test using the prop.test function in R by changing the alternative parameter from \"two.sided\" to either \"greater\" or \"less\". This choice depends on the direction of the alternative hypothesis we wish to test.\nIn our scenario, if we aim to test whether the proportion of men who purchased the product is greater than the proportion of women, we would use alternative = \"greater\". Conversely, if our hypothesis is that the proportion of men is less, we would use alternative = \"less\".\nHere’s how we can modify our code for a one-sided test:\n\nx &lt;- c(125, 210) # number of men and women who purchased the product\nn &lt;- c(250, 350) # total number of men and women\n\n# Perform the one-sided two-proportion z-test\n# We will use \"less\" to test if the proportion of men is less than that of women\n# Change to \"greater\" if testing for the opposite\ntest_result &lt;- prop.test(x, \n                         n, \n                         alternative = \"less\", \n                         correct = FALSE)\n# Output the results\nprint(test_result)\n\n\n    2-sample test for equality of proportions without continuity correction\n\ndata:  x out of n\nX-squared = 5.9138, df = 1, p-value = 0.007511\nalternative hypothesis: less\n95 percent confidence interval:\n -1.0000000 -0.0324665\nsample estimates:\nprop 1 prop 2 \n   0.5    0.6 \n\n\nIn this code, we use alternative = \"less\" for a one-sided test where our hypothesis is that the proportion for the first group (men) is less than that for the second group (women). We need to choose the appropriate direction (“greater” or “less”) based on our specific research hypothesis.\nInterpreting the output\n\nX-squared: The test statistic value is 5.9138. This value is derived from the comparison of the two proportions.\nP-value: The p-value is 0.007511. In a one-sided test, this value indicates the probability of observing a test statistic as extreme as, or more extreme than, the one observed, under the null hypothesis. Since it’s less than 0.05, it suggests significant evidence against the null hypothesis in favor of the alternative.\nAlternative Hypothesis: “less” - This specifies that the test is one-sided, testing whether the first proportion (prop 1) is less than the second proportion (prop 2).\n95 Percent Confidence Interval: Ranges from -1.000 to -0.0325. This interval indicates where the true difference in proportions (prop 1 - prop 2) likely lies. Since the interval does not include 0 and is entirely negative, it suggests that prop 1 is likely less than prop 2.\nSample Estimates: Shows the estimated proportions for each group - 0.5 (prop 1) and 0.6 (prop 2).\n\nConclusion: The test results suggest that there is a significant difference between the two groups, with the first group having a lower proportion compared to the second group, supporting the alternative hypothesis that the proportion of men purchasing the product is less than that of women.\n\n\n\nObjective: We aim to compare the proportion of cars with automatic transmission (indicated by am = 1) to those with V-shaped engines (indicated by vs = 1) in the mtcars dataset. Our goal is to determine if there is a significant difference in the proportions of these two categories of cars.\n\n# Loading the mtcars dataset\n# Loading the mtcars dataset\ndata(mtcars)\n\n# Defining the samples\n# sample1 represents cars with automatic transmission (am = 1)\n# sample2 represents cars with V-shaped engines (vs = 1)\nsample1 &lt;- mtcars$am\nsample2 &lt;- mtcars$vs\n\n# Calculating the number of successes in each sample\n# Success for sample1 is defined as having an automatic transmission (am = 1)\n# Success for sample2 is defined as having a V-shaped engine (vs = 1)\nsuccesses_sample1 &lt;- sum(sample1 == 1)\nsuccesses_sample2 &lt;- sum(sample2 == 1)\n\n# Calculating the sample sizes\n# The sample size for each group is the total number of observations in that group\nsize_sample1 &lt;- length(sample1)\nsize_sample2 &lt;- length(sample2)\n\n# Performing the Z-test for proportions\n# The test compares the proportion of cars with automatic transmission to those with V-shaped engines\nprop.test(x = c(successes_sample1, successes_sample2),\n          n = c(size_sample1, size_sample2),\n          alternative = \"two.sided\")\n\n\n    2-sample test for equality of proportions with continuity correction\n\ndata:  c(successes_sample1, successes_sample2) out of c(size_sample1, size_sample2)\nX-squared = 0, df = 1, p-value = 1\nalternative hypothesis: two.sided\n95 percent confidence interval:\n -0.3043652  0.2418652\nsample estimates:\n prop 1  prop 2 \n0.40625 0.43750 \n\n\nInterpreting the output\n\nTest Type: “2-sample test for equality of proportions with continuity correction” - This test compares two independent proportions (in this case, cars with automatic transmission vs. cars with V-shaped engines in the mtcars dataset). The continuity correction is applied, which is typically used for small sample sizes to improve the approximation of the binomial distribution to the normal distribution.\nX-squared: The test statistic value is 0. This value is used to determine the p-value. An X-squared of 0 usually indicates no observed difference between the proportions.\np-value: With a p-value of 1, there’s no statistical evidence to suggest a significant difference between the two proportions.\nAlternative Hypothesis: “two.sided” - The test was conducted to determine if there is any difference (either way) between the two proportions.\n95 Percent Confidence Interval: Ranges from -0.3044 to 0.2418. Since this interval includes 0, it suggests that the difference in proportions might be zero, supporting the lack of statistical significance.\nSample Estimates: The estimated proportions are 0.40625 for the first group (automatic transmissions) and 0.43750 for the second group (V-shaped engines).\n\nConclusion: The test suggests no significant difference between the proportion of cars with automatic transmissions and those with V-shaped engines in the mtcars dataset. [8]"
  },
  {
    "objectID": "03ChiSquareTests.html#references",
    "href": "03ChiSquareTests.html#references",
    "title": "Chi-Square Tests",
    "section": "",
    "text": "[1] Agresti, A. (2002). Categorical data analysis (2nd ed.). Wiley.\nHair Jr, J. F., Black, W. C., Babin, B. J., & Anderson, R. E. (2010). Multivariate data analysis: A global perspective (7th ed.). Pearson Education.\nEveritt, B. S. (1992). The analysis of contingency tables (2nd ed.). Chapman and Hall."
  }
]